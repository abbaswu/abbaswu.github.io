{"meta":{"title":"Jifeng Wu's Personal Website","subtitle":"Jifeng Wu's Personal Website","description":"Jifeng Wu's Personal Website","author":"Jifeng Wu","url":"https://abbaswu.github.io","root":"/"},"pages":[{"title":"About Me","date":"2022-10-20T07:00:00.000Z","updated":"2023-12-30T15:24:17.667Z","comments":false,"path":"index.html","permalink":"https://abbaswu.github.io/index.html","excerpt":"","text":"Hello everyone! My name is Jifeng Wu, and I am a Computer Science Masters' student at UBC working with Caroline Lemieux in the Software Practices Lab with an interest in Programming Languages and Software Engineering. You can see my UBC Grades Summary here. Before that, I obtained my Bachelor of Engineering from School of Computer Science, Wuhan University. My bachelor's thesis, \"Effective Stack Wear Leveling for NVM\" has been advised by Qingan Li, and I have also done research in Graph Data Mining, advised by Yuanyuan Zhu. You can see my Wuhan University Student's Transcript here. I enjoy investigating and coming up with personal interpretations of various concepts in the natural and social sciences, computer programming (master C++, Python, Shell, and learning Rust and OCaml; also an avid Unix fan), photography, hiking, cycling, cooking, and language learning in my free time. Linear Regression, Ridge Regression, Lasso Regression, and Kernel Ridge Regression Giving Talks on Research: Whats and Hows Understanding the Name, Structure, and Loss Function of the Variational Autoencoder From the Fourier Series to the Fourier Transform to the Discrete-time Fourier Transform: Demystifying the Formulas Type-Theoretic Constructs in C++ Python in a Functional Style: Closures, Generators, and Coroutines Understanding the Formulation of Information Entropy You can find my blog and diary here, my CV here, and feel free to contact me through my social media links! Work in Progress Attributed-Based Type Inference for Python We infer types for variables in Python projects without type annotations through a novel attribute-centric type inference approach that collects attributes variables should provide. Mentor: Prof. Caroline Lemieux Time: July 2023 - Present Poster and lightning talk presented at the PNW PLSE 2023 Workshop Papers Effective Stack Wear Leveling for NVM We increase the lifespan of non-volatile memory with limited write durability by implementing an LLVM pass that can convert wear-heavy loops in programs into recursive functions. Published in: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (2023) Authors: Jifeng Wu, Wei Li, Libing Wu, Mengting Yuan, Chun Jason Xue, Jingling Xue, Qingan Li Mentor: Prof. Qingan Li Time: August 2021 - August 2022 Paper GitHub Repository Essays EECE 571F Deep Learning with Structure Project \"The Impact of Synthetic Data on Image Captioning Models\" Significant improvements in the quality and availability of generative models have led to widespread synthetic data across the Internet. Motivated by previous work on synthetic data from an \"in-the-wild\" Stable Diffusion model improving accuracy and robustness of image classification, this paper investigates the influence of synthetic data on image captioning, a much more challenging task bridging computer vision and natural language processing, through experiments conducted with the classic \"Show and Tell\" model. Time: October 2023 - December 2023 Essay GitHub Repository CPSC 548: Directed Studies Report on Program Synthesis This directed studies report on program synthesis explores fundamental concepts and principles, implements and compares classic program synthesis algorithms, and provides insights into their strengths, weaknesses, challenges, and future research directions. The experience has equipped me with knowledge and skills applicable to future research, such as working with SMT solvers (constraint solvers) and automating the data extraction and visualization processes. Mentor: Prof. Caroline Lemieux Time: January 2023 - April 2023 Essay GitHub Repository Community Detection Using Social Relations and Trajectories Community detection is an essential task in social network analysis, but many friends on social networks are not close to one another in the real world. We introduce a novel approach that utilizes user trajectories to identify cohesive groups of users who frequently hang out together and presents algorithms for efficiently calculating spatiotemporal similarity between trajectories and community detection. Mentor: Prof. Yuanyuan Zhu Time: September 2019 - June 2021 Essay GitHub Repository Conference Experiences Pre-MICCAI Workshop@UBC Time: October 7, 2023 Observations and Gained Insights ICFP 2023 Role: Mentee, Programming Languages Mentoring Workshop Time: September 4, 2023 The Cornell, Maryland, Max Planck Pre-doctoral Research School 2023 Role: Attendee Time: August 6 - 13, 2023 Certificate Observations and Gained Insights ISSTA/ECOOP 2023 Role: Student Volunteer Time: July 17 - 21, 2023 Observations and Gained Insights PNW PLSE 2023 Workshop Role: Presenter Time: May 9, 2023 Presented poster and lightning talk on \"Automated Type Inference in Python.\" Observations and Gained Insights Pointers Machine Learning The Bitter Lesson Functional Programming Caml trading - experiences with functional programming on Wall Street OCaml Programming: Correct + Efficient + Beautiful My introduction to Z3 and solving satisfiability problems LLVM Mapping High Level Constructs to LLVM IR Trajectory Data Mining A survey of trajectory distance measures and performance evaluation PDF https://doi.org/10.1007/s00778-019-00574-9 Community Detection The Core Decomposition of Networks: Theory, Algorithms and Applications PDF https://doi.org/10.1007/s00778-019-00587-4 Links to My Friends Zhaowei Zhang @ Peking University Ziqi Rong @ University of Michigan"},{"title":"","date":"2023-08-22T15:47:02.623Z","updated":"2023-08-22T15:47:02.623Z","comments":false,"path":"categories/index.html","permalink":"https://abbaswu.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2023-08-22T15:47:02.623Z","updated":"2023-08-22T15:47:02.623Z","comments":false,"path":"projects/index.html","permalink":"https://abbaswu.github.io/projects/index.html","excerpt":"","text":""},{"title":"","date":"2023-08-22T15:47:02.623Z","updated":"2023-08-22T15:47:02.623Z","comments":false,"path":"tags/index.html","permalink":"https://abbaswu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Linear Regression, Ridge Regression, Lasso Regression, and Kernel Ridge Regression","slug":"Linear-Regression-Ridge-Regression-Lasso-Regression-and-Kernel-Ridge-Regression","date":"2023-12-24T08:00:00.000Z","updated":"2023-12-25T05:01:47.903Z","comments":true,"path":"2023/12/24/Linear-Regression-Ridge-Regression-Lasso-Regression-and-Kernel-Ridge-Regression/","link":"","permalink":"https://abbaswu.github.io/2023/12/24/Linear-Regression-Ridge-Regression-Lasso-Regression-and-Kernel-Ridge-Regression/","excerpt":"","text":"Linear Regression Linear Regression Linear regression is a fundamental statistical model used in statistics and supervised machine learning. It establishes a linear relationship between a scalar response and one or more explanatory variables. The simplicity and well-established properties of linear regression make it a cornerstone algorithm in machine learning. Historically, linear regression was developed by Legendre (1805) and Gauss (1809) for astronomical predictions and later popularized in the social sciences by Quetelet. Linear regression is widely used for two primary purposes: For predictive modeling, it fits a model to observed data sets, allowing for future predictions when new explanatory variables are available without their corresponding response values. For analysis, it helps quantify the relationship between response and explanatory variables, assessing the strength of this relationship and identifying variables with no linear relationship or redundant information. In its most general case, a linear regression model can be written in matrix notation as \\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\] where \\(\\mathbf{y} = {\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{bmatrix}}\\) is a vector of \\(n\\) observed values of the response variable. \\(\\mathbf{X} = {\\begin{bmatrix} \\mathbf{x}_{1}^{\\mathsf{T}} \\\\ \\mathbf{x}_{2}^{\\mathsf{T}} \\\\ \\vdots \\\\ \\mathbf{x}_{n}^{\\mathsf{T}} \\end{bmatrix}} = {\\begin{bmatrix} 1 &amp; x_{1, 1} &amp; \\cdots &amp; x_{1, p} \\\\ 1 &amp; x_{2, 1} &amp; \\cdots &amp; x_{2, p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n, 1} &amp; \\cdots &amp; x_{n, p} \\end{bmatrix}}\\) is a matrix of \\(n\\) observed \\((p + 1)\\)-dimensional row-vectors of the explanatory variables. \\(\\boldsymbol{\\beta} = {\\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\beta_{2} \\\\ \\vdots \\\\ \\beta_{p} \\end{bmatrix}}\\) is a \\((p + 1)\\)-dimensional parameter vector, whose elements, multiplied with each dimension of the explanatory variables, are known as effects or regression coefficients. \\(\\boldsymbol{\\varepsilon}={\\begin{bmatrix}\\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon _{n} \\end{bmatrix}}\\) is a vector of \\(n\\) error terms. It captures all other factors that influence \\(\\mathbf{y}\\) other than \\(\\mathbf{X}\\). Note that the first dimension of the explanatory variables is the constant 1. This is designed such that the corresponding first element of \\(\\boldsymbol{\\beta}\\), \\(\\beta_{0}\\), would be the intercept after matrix multiplication. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero. Fitting a linear model to a given data set usually requires estimating \\(\\boldsymbol{\\beta}\\) such that \\(\\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\) is minimized. For example, it is common to use the sum of squared errors (known as ordinary least squares) \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) as a loss function for minimization. This minimization problem has a unique solution, \\({\\hat{\\boldsymbol{\\beta}}} = (\\mathbf{X}^{\\operatorname{T}} \\mathbf{X})^{-1}\\mathbf{X}^{\\operatorname{T}} \\mathbf{y}\\). References: https://en.wikipedia.org/wiki/Linear_regression https://en.wikipedia.org/wiki/Ordinary_least_squares References: https://en.wikipedia.org/wiki/Linear_regression https://en.wikipedia.org/wiki/Ordinary_least_squares Ridge Regression However, when linear regression models have some multicollinear (highly correlated) dimensions of the explanatory variables, which commonly occurs in models with high-dimensional explanatory variables, \\(\\mathbf{X}^{\\operatorname{T}} \\mathbf{X}\\) approaches a singular matrix and calculating \\(\\left(\\mathbf{X}^{\\operatorname{T}} \\mathbf{X} \\right)^{-1}\\) becomes numerically unstable (note how the magnitude of np.linalg.inv(X.T @ X) changes as the columns of X become more and more correlated below): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; for x_21 in [2.1, 2.01, 2.001, 2.0001, 2.00001]:... X = np.array([... [1., 2.],... [1., x_21]... ])... ... print(&#x27;X:&#x27;)... print(X)... ... print(&#x27;X.T @ X:&#x27;)... print(X.T @ X)... ... print(&#x27;np.linalg.inv(X.T @ X):&#x27;)... print(np.linalg.inv(X.T @ X))... X:[[1. 2. ] [1. 2.1]]X.T @ X:[[2. 4.1 ] [4.1 8.41]]np.linalg.inv(X.T @ X):[[ 841. -410.] [-410. 200.]]X:[[1. 2. ] [1. 2.01]]X.T @ X:[[2. 4.01 ] [4.01 8.0401]]np.linalg.inv(X.T @ X):[[ 80401.00000048 -40100.00000024] [-40100.00000024 20000.00000012]]X:[[1. 2. ] [1. 2.001]]X.T @ X:[[2. 4.001 ] [4.001 8.004001]]np.linalg.inv(X.T @ X):[[ 8004000.98507102 -4000999.99253738] [-4000999.99253738 1999999.99626962]]X:[[1. 2. ] [1. 2.0001]]X.T @ X:[[2. 4.0001 ] [4.0001 8.00040001]]np.linalg.inv(X.T @ X):[[ 8.00039556e+08 -4.00009777e+08] [-4.00009777e+08 1.99999889e+08]]X:[[1. 2. ] [1. 2.00001]]X.T @ X:[[2. 4.00001] [4.00001 8.00004]]np.linalg.inv(X.T @ X):[[ 7.99973381e+10 -3.99985690e+10] [-3.99985690e+10 1.99992345e+10]] This problem can be alleviated by adding positive elements to the diagonals. 123456789101112131415161718192021222324&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.array([... [1., 2.],... [1., 2.00001]... ])&gt;&gt;&gt; for _lambda in [1, 0.1, 0.01, 0.001, 0.0001]:... print(f&#x27;np.linalg.inv(X.T @ X + &#123;_lambda&#125; * np.eye(len(X))):&#x27;)... print(np.linalg.inv(X.T @ X + _lambda * np.eye(len(X))))... np.linalg.inv(X.T @ X + 1 * np.eye(len(X))):[[ 0.81818248 -0.36363595] [-0.36363595 0.27272628]]np.linalg.inv(X.T @ X + 0.1 * np.eye(len(X))):[[ 8.01980982 -3.96039026] [-3.96039026 2.07919969]]np.linalg.inv(X.T @ X + 0.01 * np.eye(len(X))):[[ 80.02005978 -39.95998014] [-39.95998014 20.07983982]]np.linalg.inv(X.T @ X + 0.001 * np.eye(len(X))):[[ 800.02078984 -399.95940022] [-399.95940022 200.07918976]]np.linalg.inv(X.T @ X + 0.0001 * np.eye(len(X))):[[ 8000.02719959 -3999.95360059] [-3999.95360059 2000.07179896]] By replacing \\((\\mathbf{X}^{\\operatorname{T}} \\mathbf{X})^{-1}\\) with \\((\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\) in \\({\\hat{\\boldsymbol{\\beta}}} = (\\mathbf{X}^{\\operatorname{T}} \\mathbf{X})^{-1}\\mathbf{X}^{\\operatorname{T}} \\mathbf{y}\\), we derive the solution to ridge regression, \\({\\hat {\\beta }}_{R}=(\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\). Ridge regression (linear regression with L2 regularization), is linear regression using \\({\\mathcal{L}}(\\boldsymbol{\\beta}, \\lambda) = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2} + \\lambda (\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} - C)\\) as the loss function to minimize. This is a Lagrangian function expressing the original ordinary least squares loss function \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) subject to the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} \\le C\\) for some \\(C &gt; 0\\). Note that by calculating \\({\\hat {\\beta }}_{R}=(\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\) with a given \\(\\lambda\\) value, instead of simultaneously solving for \\(\\boldsymbol{\\beta}\\) and lambda through \\(\\nabla{\\mathcal{L}}(\\boldsymbol{\\beta}, \\lambda) = 0\\) (the usual practice of using Lagrangian functions for constrained optimization), we do not necessary obtain a \\(\\boldsymbol{\\beta}\\) that satisfies for a given value of C. However, increasing the given \\(\\lambda\\) value monotonically decreases the value of \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2}\\), thus making the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} \\le C\\) be satisfied for smaller values of \\(C\\). Image from https://math.stackexchange.com/questions/1723201/solution-for-arg-min-xt-x-1-xt-a-x-ct-x-quadratic We can also demonstrate this with an example: 1234567891011121314151617181920212223242526&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; &gt;&gt;&gt; def beta_squared(l, X, y):... beta = np.linalg.inv(X.T @ X + l * np.eye(len(X))) @ X.T @ y... return beta.T @ beta... &gt;&gt;&gt; np.random.seed(0)&gt;&gt;&gt; &gt;&gt;&gt; X = np.random.rand(2, 2)&gt;&gt;&gt; &gt;&gt;&gt; y = np.random.rand(2, 1)&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(0.01, X, y)array([[1.33717503]])&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(0.1, X, y)array([[0.37141735]])&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(1.0, X, y)array([[0.13504294]])&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(10.0, X, y)array([[0.0062103]])&gt;&gt;&gt; &gt;&gt;&gt; beta_squared(100.0, X, y)array([[7.92298438e-05]]) Furthermore, as \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} = \\beta_{0}^{2} + \\beta_{1}^{2} + \\cdots + \\beta_{p}^{2}\\), increasing the given \\(\\lambda\\) value helps to constrain the magnitude of the effects or regression coefficients corresponding to dimensions which are redundant in high-dimensional explanatory variables. This is visualized in the right diagram, where the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{2}^{2} \\le C\\) in the Lagrangian function (the green circle) tangentially touches a contour of the original ordinary least squares loss function \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) at a point where one of the effects (or regression coefficients) is close to 0. Modified from the plot used in \"The Elements of Statistical Learning\" by Saptashwa Bhattacharyya To further strengthen this effect and completely \"zero out\" certain effects or regression coefficients, lasso regression (linear regression with L1 regularization) can be used in lieu of ridge recursion. In this case, the original ordinary least squares loss function \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) subject to the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{1} = |\\beta_{0}| + |\\beta_{1}| + \\cdots + |\\beta_{p}| \\le C\\) for some \\(C &gt; 0\\), as depicted in the left diagram, where the constraint \\(\\|{\\boldsymbol{\\beta}}\\|_{1} \\le C\\) in the Lagrangian function (the cyan square) tangentially touches a contour of the original ordinary least squares loss function \\(\\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2} = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2}\\) at a point where one of the effects (or regression coefficients) is 0. However, we cannot derive an analytical solution for \\(\\boldsymbol{\\beta}\\) given the Lagrangian function for lasso regression (a.k.a. the loss function to minimize), \\({\\mathcal{L}}(\\boldsymbol{\\beta}, \\lambda) = \\|\\mathbf{y} -\\mathbf{X}{\\boldsymbol{\\beta}}\\|_{2}^{2} + \\lambda (\\|{\\boldsymbol{\\beta}}\\|_{1} - C)\\). We can only iteratively solve for \\(\\boldsymbol{\\beta}\\) in this case. References: https://en.wikipedia.org/wiki/Lagrange_multiplier https://stats.stackexchange.com/questions/401212/showing-the-equivalence-between-the-l-2-norm-regularized-regression-and https://math.stackexchange.com/questions/1723201/solution-for-arg-min-xt-x-1-xt-a-x-ct-x-quadratic https://arxiv.org/pdf/1509.09169.pdf https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b https://en.wikipedia.org/wiki/Ridge_regression https://online.stat.psu.edu/stat857/node/155/ https://allmodelsarewrong.github.io/ridge.html Kernel Ridge Regression Given the solution to ridge recursion above, \\({\\hat {\\beta }}_{R}=(\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\), we can predict the value of the response variable \\(y_{n + 1}(\\mathbf{x}_{n + 1})\\), given an out-of-dataset vector of explanatory variables \\(\\mathbf{x}_{n + 1} = {\\begin{bmatrix} 1 \\\\ x_{n + 1, 1} \\\\ \\vdots \\\\ x_{n + 1, p} \\end{bmatrix}}\\): \\[y_{n + 1}(\\mathbf{x}_{n + 1}) = \\mathbf{x}_{n + 1}^{\\mathsf{T}} {\\hat {\\beta }}_{R} = \\mathbf{x}_{n + 1}^{\\mathsf{T}} (\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\] We can make some changes to \\(\\mathbf{x}_{n + 1}^{\\mathsf{T}} (\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} +\\lambda \\mathbf{I} )^{-1}\\mathbf{X} ^{\\mathsf{T}}\\mathbf{y}\\). Push-Through Identity Given two matrices \\(\\mathbf{P}, \\mathbf{Q}\\), based on \\(\\mathbf{P} (I + \\mathbf{Q} \\mathbf{P}) = (I + \\mathbf{P} \\mathbf{Q}) \\mathbf{P}\\), we can derive \\({(I + \\mathbf{P} \\mathbf{Q})}^{-1} \\mathbf{P} = \\mathbf{P} {(I + \\mathbf{Q} \\mathbf{P})}^{-1}\\). This is known as the push-through identity, one of the matrix inversion identities used to derive the Woodbury matrix identity, which allows cheap computation of inverses and solutions to linear equations. References: http://www0.cs.ucl.ac.uk/staff/g.ridgway/mil/mil.pdf https://en.wikipedia.org/wiki/Woodbury_bmatrix_identity Based on the push through identity, \\(\\mathbf{x}_{n + 1}^{\\mathsf{T}} (\\mathbf{X} ^{\\mathsf{T}} \\mathbf{X} + \\lambda \\mathbf{I} )^{-1} \\mathbf{X}^{\\mathsf{T}} \\mathbf{y} = \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{X}^{\\mathsf{T}} {(\\mathbf{X} \\mathbf{X}^{\\mathsf{T}} + \\lambda \\mathbf{I})}^{-1} \\mathbf{y}\\). As \\(\\mathbf{X} = {\\begin{bmatrix} \\mathbf{x}_{1}^{\\mathsf{T}} \\\\ \\vdots \\\\ \\mathbf{x}_{n}^{\\mathsf{T}} \\end{bmatrix}}\\), \\(\\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n} \\end{bmatrix}}\\), we have: \\(\\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{x}_{n} \\end{bmatrix}}\\) \\(\\mathbf{X} \\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} \\mathbf{x}_{1}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{1}^{\\mathsf{T}} \\mathbf{x}_{n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{x}_{n}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n}^{\\mathsf{T}} \\mathbf{x}_{n} \\end{bmatrix}}\\) Thus: \\[y_{n + 1}(\\mathbf{x}_{n + 1}) = {\\begin{bmatrix} \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{x}_{n} \\end{bmatrix}} {({\\begin{bmatrix} \\mathbf{x}_{1}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{1}^{\\mathsf{T}} \\mathbf{x}_{n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{x}_{n}^{\\mathsf{T}} \\mathbf{x}_{1} &amp; \\cdots &amp; \\mathbf{x}_{n}^{\\mathsf{T}} \\mathbf{x}_{n} \\end{bmatrix}} + \\lambda \\mathbf{I})}^{-1} \\mathbf{y}\\] This means that we can calculate \\(y_{n + 1}(\\mathbf{x}_{n + 1})\\) directly from the dot products among \\(\\mathbf{x}_{1}, \\cdots, \\mathbf{x}_{n}\\) and the dot products between \\(\\mathbf{x}_{n + 1}\\) and \\(\\mathbf{x}_{1}, \\cdots, \\mathbf{x}_{n}\\), without having to explicitly know the values of \\(\\mathbf{x}_{1}, \\cdots, \\mathbf{x}_{n}\\) and \\(\\mathbf{x}_{n + 1}\\). Moreover, the dot product between two vectors of explanatory variables here can be generalized to any symmetric similarity function between two vectors of explanatory variables known as kernel functions. Using \\(k(\\mathbf{x}_{i}, \\mathbf{x}_{j})\\) to denote the similarity between \\(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\) under the kernel function \\(k\\), let: \\(\\mathbf{K} = \\mathbf{X} \\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} k(\\mathbf{x}_{1}, \\mathbf{x}_{1}) &amp; \\cdots &amp; k(\\mathbf{x}_{1}, \\mathbf{x}_{n}) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ k(\\mathbf{x}_{n}, \\mathbf{x}_{1}) &amp; \\cdots &amp; k(\\mathbf{x}_{n}, \\mathbf{x}_{n}) \\end{bmatrix}}\\) \\(\\mathbf{k}(\\mathbf{x}_{n + 1}) = \\mathbf{x}_{n + 1}^{\\mathsf{T}} \\mathbf{X}^{\\mathsf{T}} = {\\begin{bmatrix} k(\\mathbf{x}_{n + 1}, \\mathbf{x}_{1}) &amp; \\cdots &amp; k(\\mathbf{x}_{n + 1}, \\mathbf{x}_{n}) \\end{bmatrix}}\\) We have: \\[y_{n + 1}(\\mathbf{x}_{n + 1}) = \\mathbf{k}(\\mathbf{x}_{n + 1}) {(\\mathbf{K} + \\lambda \\mathbf{I})}^{-1} \\mathbf{y}\\] There are two benefits of kernel ridge regression. It allows implicitly performing nonlinear transformations on the vector representations of explanatory variables within similarity calculation, allowing nonlinearity to be introduced. A prominent example is the widespread radial basis function kernel, first used in mining engineering (\"kriging\"). It allows regressions on explanatory variables that do not have explicit vector representations but have similarity functions. There are \"string kernels,\" \"image kernels,\" \"graph kernels,\" and so on. Kriging (from UBC CPSC 340 slides) Kriging Kernel Trick for Non-Vector Data (from UBC CPSC 340 slides) Kernel Trick for Non-Vector Data (from UBC CPSC 340 slides)","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://abbaswu.github.io/categories/Mathematics/"}],"tags":[]},{"title":"Sarah Chasins' Works on PL and HCI","slug":"Sarah-Chasins-Works-on-PL-and-HCI","date":"2023-11-05T07:00:00.000Z","updated":"2023-11-08T20:50:51.900Z","comments":true,"path":"2023/11/05/Sarah-Chasins-Works-on-PL-and-HCI/","link":"","permalink":"https://abbaswu.github.io/2023/11/05/Sarah-Chasins-Works-on-PL-and-HCI/","excerpt":"","text":"Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain Investigative journalists and public defenders are crucial in scrutinizing and litigating significant matters concerning police violence and misconduct. However, they often need help navigating through vast, unordered heaps of data, which adds strain to their resource-constrained teams. In partnership with U.S. public defenders and investigative journalists, we developed an AI-enhanced tool through a joint design effort to aid in working with such data. This process offered us valuable insights into the requirements of resource-constrained teams dealing with large data sets, including how some experts became self-taught programmers to streamline their workflows. We pinpointed three primary data needs throughout our collaborative design journey and established five design objectives. Three Primary Data Needs Data Cleaning, particularly the process of de-duplication. That involves identifying identical (images of pages are pixel-for-pixel copies of each other) or nearly identical data (images are not pixel-for-pixel identical but capture the same physical document) within a dataset. Data Extraction. The professionals also struggled in extracting relevant information such as names, dates, locations, and case numbers from case files due to their disparate formats and layouts, necessitating extensive, hands-on work. Data Organization. There was a need to systematically organize PDF documents by specific cases, complicated by the fact that cases may be spread across numerous documents and folders, or conversely, several cases might be compiled into one extensive PDF. Five Fundamental Design Principles Human Control and Intervention. The design must prioritize aiding users over complete automation of the process. Non-Interference with Existing Practices. The design should integrate seamlessly with existing workflows and practices. Adaptability to Data Diversity. High-level Abstractions. General-purpose languages like Python or R demand extensive technical expertise. Pre-built software, on the other hand, offers limited flexibility. Cost-Sensitive Solutions. Results Participants in our sessions became adept in all three programming paradigms (visual, PBE, and text-based interfaces). This contradicts the common misconception that non-technical experts need formal coding training to handle text-based programming; if the tools are appropriately supportive, they can. Rather than creating new code, participants preferred to modify what was already there. Particularly with text-based coding, almost all chose to adapt sample code instead of originating their own, aligning with previous research on the blank-page syndrome. A Need-Finding Study with Users of Geospatial Data Current geospatial analysis and visualization tools present significant learning curves and usability challenges. Finding and transforming geospatial data to specific spatiotemporal constraints. Grasping the behavior of geospatial operators. Tracking the provenance of geospatial data, including cross-system provenance. Exploring the cartographic design space. Grasping the behavior of geospatial operators Users had to run operators and manually check outputs to understand operator semantics. Live programming, which offers users immediate visual feedback on program behavior using concrete inputs, could align with users' existing debugging patterns of using small collections of geographic features or pixels as test cases to infer operator behavior. Tracking the provenance of geospatial data, including cross-system provenance The GIS tools used by participants did not track the steps leading to final outputs, complicating the replication of previous analyses. Modifying maps or adapting them to new datasets often meant laboriously reverse engineering the initial analysis steps. Creating repeatable and communicable geospatial workflows was a struggle for GIS users. Limitations in current history features made it difficult to recover information on the current analysis state or revisit past analysis decisions. The problem of tracking provenance across different systems was also prominent. Users often kept informal records of the steps taken in data acquisition, cleaning, analysis, and visualization, which spanned several applications. For instance, one user used macOS Notes to detail a process involving data transfer between Sentinel Hub, QGIS, Illustrator, and Photoshop, documenting everything from selecting a Sentinel-2 image to reassembling raster segments in Illustrator. This kind of multi-tool orchestration was typical among our subjects, yet none had automated systems to log data lineage across these platforms. Exploring the cartographic design space Many participants used direct manipulation tools for geospatial data visualization, which discarded all geographical metadata, posing challenges to revising the analysis after starting the visualization. This uncovers a potential for development in tools that (1) unify geospatial analysis with cartographic design and (2) preserve the geospatial data aspects of visual elements while supporting direct manipulation. Existing research suggests that combining scripting with direct manipulation for visually oriented tasks is feasible. The Sketch-n-sketch application is a testament to the successful merger of these methods for SVG graphics. Such a combined approach could also remedy the fundamental issue participants faced when using direct manipulation tools for cartography: the need to recreate map designs in code after finishing a design. How Statically-Typed Functional Programmers Write Code A deeper comprehension of the coding methods of statically-typed functional programmers could lead to the creation of more practical tools, more user-friendly programming languages, and better gateways into programming communities. These programmers utilize their compilers for more than just producing an executable; they also use compilers as corrective and directive aids. Compilers as corrective tools. Compiler error messages were useful not just to fix their programs but also to correct their mental models of the problem domain. Compilers as directive tools. Many developers treat compiler errors as to-do lists, guiding their subsequent coding actions. A typical process includes beginning a program change with a minor alteration and compiling to receive error-driven sub-tasks - essentially turning error messages into a step-by-step guide for coding. It's not uncommon for programmers to compile their code with the expectation of errors, using the compiler to validate the direction of their development. Statically-typed functional programmers often seek feedback from automated tools even when their code isn't yet operational, suggesting that such tools should strive to extract as much information as possible from non-compilable code. When comparing pattern matching with combinators, statically-typed functional programmers report less cognitive and time pressure with the former. This could be due to pattern matching's explicit textual representation of tasks, explicit handling of recursion, or consistent interface across various data structures. Nonetheless, some programmers prefer to rewrite their code using combinators eventually. Ideally, a tool would assist in this process, starting with a data type and guiding the programmer through case completion, subsequently offering a series of combinators as a refined alternative. It is beneficial to recognize which language constructs allow for low-workload or opportunistic construction and how these constructs are valued within the programming community. There's a demand for tools that minimize the difficulty of altering types during development. Furthermore, these tools should facilitate the natural cyclic changes of a developer's focus between modifying types and modifying expressions, possibly by employing program repair techniques to predict how changes in one will affect the other. Program sketches provide a wealth of information about undefined functions, like inferred types and potential uses. Since statically-typed functional programmers regularly employ this method of drafting and refining code, there's a clear opportunity for tools that could enhance or even automate parts of this practice.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Research Programming","slug":"Paper-Reading/Research-Programming","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Research-Programming/"}],"tags":[]},{"title":"Giving Talks on Research: Whats and Hows","slug":"Giving-Talks-on-Research-Whats-and-Hows","date":"2023-10-27T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/10/27/Giving-Talks-on-Research-Whats-and-Hows/","link":"","permalink":"https://abbaswu.github.io/2023/10/27/Giving-Talks-on-Research-Whats-and-Hows/","excerpt":"","text":"As researchers, we often need to give talks on our research, either as part of presenting our results, or as a means of pitching our aspirations for the future. In this talk, inspired by ideas presented by Derek Dreyer at CMMRS 2023, Simon Peyton Jones at ICFP PLSE 2023, Finn Hacket and Robert Xiao at \"Workshop on Presentation Skills\", and my personal experiences, I will present the high-level \"whats\" an ideal talk should be like, and the low-level \"hows\" along the path of actually preparing and giving such talks. Source code and compiled PDF of the presentation for \"Giving Talks on Research: Whats and Hows\" presented at the Systopia Reading Group Oct 27, 2023 is available in this GitHub repository.","categories":[{"name":"Talks","slug":"Talks","permalink":"https://abbaswu.github.io/categories/Talks/"}],"tags":[]},{"title":"Conversation with Prof. Robert Xiao","slug":"Conversation-with-Prof-Robert-Xiao","date":"2023-10-23T07:00:00.000Z","updated":"2023-11-06T07:31:36.161Z","comments":true,"path":"2023/10/23/Conversation-with-Prof-Robert-Xiao/","link":"","permalink":"https://abbaswu.github.io/2023/10/23/Conversation-with-Prof-Robert-Xiao/","excerpt":"","text":"Abstract The following is a polished version of a conversation with Prof. Robert Xiao on the confluence of Programming Languages, Software Engineering, and Human-Computer Interaction (HCI) for research programming. The main points mentioned by Prof. Robert Xiao are as follows. Determining which aspects of research programming are amenable to systematization is a challenge. Advancements in code tracing, like PyTorch 2's bytecode analysis, which, while not addressing verifiability directly, allows for in-depth program behavior analysis. Understanding the influence of input data on outputs is central to the challenge of explainable AI (XAI). However, pinpointing specific model features or layers leading to an output is perhaps more accessible and useful for debugging. The concept of 'radioactively' tagging data to trace its influence through a model is an intriguing one, akin to tracking the uptake of a tagged substance in a biological system. There are many scenarios with extensive object interactions in programming. Game programming provides a more structured context to study these complexities, with ample open-source resources for research. The difficulty in game programming arises from the myriad interactions between diverse object systems - like physics, collision, and interaction logic. Coding these interactions is a lot of work. It would be enlightening to study how game developers handle this complexity and whether there are ways to simplify it. Game studios, being the behemoths they are, would undoubtedly embrace methods to alleviate the strenuous nature of their programming efforts. Polished Transcript Robert Xiao: [00:00] Could you share the focus of your research and how it's pertinent to this project? Also, what do you aim to achieve with it? There seem to be several components you've touched upon, such as visualization, pipeline development, and programming processes. These represent different approaches you could potentially adopt or consider integrating into a comprehensive pipeline. I believe the ultimate goal here is to aid research programmers in accelerating system development while minimizing errors, correct? Jifeng Wu: [00:43] Yes, precisely. Robert Xiao: [00:45] Let's delve into your research focus. How does your current work align with this? Jifeng Wu: [00:52] My ongoing research isn't directly related, as this is a path I'm contemplating for a future Ph.D. project, which I still need to commit to. I'm currently working on my master's thesis titled 'Type Inference for Python.' It aims to infer types in Python code, which often lacks annotations. This lack can lead to IDEs providing less accurate suggestions. With type information, predictions become more reliable, enhancing the coding and code interaction experience. Robert Xiao: [01:54] So, to clarify, your project is about developing a system for automatic type inference that assists IDEs, not just creating a type annotation database. Existing tools do offer preliminary type extraction, but I'm interested in the novel contribution your research makes. Jifeng Wu: [02:31] Exactly. I'm not just extracting types; I'm inferring them in unannotated code bases to enhance IDE functionality. Robert Xiao: [02:45] Understood. There are incremental typing tools available, but we can explore that later. For now, it's great that you're well-versed in Python, especially since it's prevalent in LLM research. An interesting aspect of your direction could be mitigating bugs, which often derail projects. Implementing automated checks could be invaluable. However, the challenge lies in determining which aspects of research programming are amenable to systematization. Jifeng Wu: [06:17] My vision is to support researchers engaged in data analysis or custom model design in an environment akin to Jupyter notebooks. And touching on debugging, I see a potential to harness functional programming due to its purity and ease of debugging. Robert Xiao: [07:22] The question, however, is the application of functional programming to research code, which often depends on pre-existing libraries. While functional design has its merits, the practicality of integrating it into the current ecosystem is worth discussing. Jifeng Wu: [07:55] I concede the point; many codebases are indeed messy. I'm contemplating a clean slate design, potentially developing a new language or library to demonstrate the concept. Robert Xiao: [08:21] It's noteworthy that there have been advancements in code tracing, like PyTorch 2's bytecode analysis, which, while not addressing verifiability directly, allows for in-depth program behavior analysis. Jifeng Wu: [11:09] Certainly. There are facets of current notebook technologies that pique my interest, primarily due to their inadequate support, with debugging being a prime example. Debugging encompasses two key aspects: the logic of the program, as previously mentioned, and data provenance. Sometimes, despite the sound logic, I need to delve into the origins of an unexpected output data point by tracing the implicit calculations that led to it. [11:58] This necessity for data provenance tracking is something I find critically important in my daily research, and I understand it's known as the data provenance problem. Robert Xiao: [12:09] Indeed, if you've ever discussed this with Margo, you're likely well-versed in the topic, given her research focuses precisely on provenance. Many of her colleagues are exploring this area, which is complex, particularly in the context of outputs from extensive machine learning models. While it would be beneficial to trace data points back to their origins, integrating such a mechanism into a model is a formidable challenge. [12:55] As a developer, I'm keen on understanding the influence of input data on outputs, which is central to the challenge of explainable AI (XAI). Resolving this would mark a significant milestone. However, pinpointing specific model features or layers leading to an output is perhaps more accessible and useful for debugging. [13:53] For instance, identifying a misconfigured layer responsible for input-related issues would be invaluable. Although considering the interconnected nature of model layers, this remains a complex task. [14:57] The concept of 'radioactively' tagging data to trace its influence through a model is an intriguing one, akin to tracking the uptake of a tagged substance in a biological system. Yet, translating this to a machine learning environment presents a unique set of challenges. [16:57] While I'm not deeply familiar with the latest advancements in this field, it's clear that XAI could significantly benefit HCI applications. The goal is to incrementally address these challenges by developing models that acknowledge tagged inputs throughout the data processing pipeline. [17:58] These are some thoughts on the subject. I'd like to know which aspects you find most relevant or valuable for your future endeavors. Jifeng Wu: [18:15] The examples and pointers you've provided are insightful. As someone with a software engineering background, I believe that adapting certain constructs, like functional programming and traditional program analysis, could offer potential solutions. These are directions I'm considering for my Ph.D. research. Robert Xiao: [18:58] Exploring program tracing for optimization could prove fruitful, given the untapped potential in that area. The philosophy I subscribe to favors solutions that minimize user effort, exemplified by the tracing compiler feature in PyTorch 2.0. Unlike TensorFlow model, which requires upfront operation declarations, PyTorch's immediate mode operation presents a more straightforward approach for users, facilitating a clearer understanding of variable flow during execution. Jifeng Wu: [ 24:12 ] Incidentally, as a researcher in HCI, have you ever engaged in work that marries aspects of software engineering, specifically functional programming, with HCI? Robert Xiao: [ 24:28 ] My experience with functional programming in a research capacity is virtually nonexistent. My computer science education covered the basics of functional programming - I dabbled in Scheme and Racket - but in terms of research, functional programming hasn't been part of my repertoire. Our work typically involves Python, C#, and various visual programming tools, none of which adhere to a functional programming paradigm. [ 24:56 ] Game programming, which encompasses many of our VR/AR programming, is about as far removed from functional programming as possible. It's heavily state-driven, with an ever-changing state environment. A functional approach could be applied to VR/AR development. It could offer advantages over current methods. I'm aware of actor model programming being used for VR/AR experiences, though it's not functional programming per se, and I have yet to adopt it in my work personally. Conversely, when it comes to software engineering, we do integrate its methodologies into our software creation process. This includes best practices like code structuring for reusability, modularization, refactoring, and especially source control, which I find is grossly underutilized in research programming, among other things. Jifeng Wu: [ 26:33 ] Understood, yes. My vision, when I speak of integrating functional programming, isn't confined to conventional languages like Scheme or Racket that you mentioned earlier. My thoughts were more aligned with programming paradigms like the actor model you described, as well as visual programming. I'm interested in approaches that are more formalized, easier to reason about, and offer a clearer path to verifying properties and facilitating debugging. Robert Xiao: [ 27:18 ] With general-purpose imperative coding, the debugging process is, frankly, a nightmare. Although my personal experience in developing large-scale games is limited, our research typically involves creating specific VR/AR experiences. These are smaller in scale, utilizing existing libraries to build a finite number of interactive elements within a controlled environment. This contrasts with the vast complexity of full-scale game development, where the difficulty arises from the myriad interactions between diverse object systems - like physics, collision, and interaction logic. [ 28:55 ] Take collision logic as an instance; the multitude of possible outcomes from a single collision event can be incredibly intricate to code. If we consider bullet dynamics in games, the behavior of these projectiles upon impact with walls, enemies, or objects varies dramatically, leading to a cascade of different effects. Coding these interactions is a lot of work. It would be enlightening to study how game developers handle this complexity and whether there are ways to simplify the process. The Entity Component System (ECS) attempts to mitigate this by adopting an actor-like model, but even then, complexity escalates rapidly as interactions increase. [ 30:56 ] Despite efforts to manage these interactions, there comes a point where local decisions require some form of higher-level orchestration. When a bullet is fired, for instance, numerous actions must be coordinated, from ammo count adjustments to triggering animations. All this complexity makes game programming something I would not want to revisit except in the context of my research. However, there lies a vast potential for impactful research in understanding and easing the complexities of game development. Game studios, being the behemoths they are, would undoubtedly embrace methods to alleviate the strenuous nature of their programming efforts. Yes, indeed. Jifeng Wu: [32:00] Indeed, that aligns with my central interests. My master's thesis on Python type inference encountered similar challenges to those you've highlighted. In analyzing types, one must grapple with substantial propagation throughout the program. This is precisely the issue at hand, and it serves as a prime motivator in my quest to develop formalizations that simplify the process for programmers, particularly in areas like game development, where complex interactions are commonplace. Robert Xiao: [32:45] You've sparked a thought here - this may be a digression - but I'm curious. Has there been research into the amount of typing necessary for untyped code to converge? You made an excellent point about the recursive search required in untyped code to determine types, which resembles an intricate graph search. However, it's more than that because the connections in the 'type graph' are not always apparent. This raises an intriguing theoretical question: At what point in a code base's typing does the cost shift from an exponential to a linear time complexity? It's a highly theoretical question, indeed, and one that surely must have been examined in terms of computational complexity. [34:58] It's fascinating because most research focuses on strong typing systems and type inference within defined parameters. But the dynamics change with untyped constructs. Apologies for the tangent - it's just a curious question. Jifeng Wu: [35:32] Your point is very interesting, and it is pertinent to my future research endeavors. Robert Xiao: [35:39] There ought to be studies on this - how computationally complex is a type system? The PL community has likely delved into this. However, the issue becomes significantly more compelling when considering untyped languages. Sorry for the tangent, but it's a topic worth exploring. Jifeng Wu: [36:15] Returning to our discussion, my work in type inference for Python resonates with your mention of interaction-heavy domains like game programming. Robert Xiao: [36:27] Certainly. Game programming exemplifies a scenario with extensive object interactions, which is atypical in most systems where such interactions are minimized. For instance, hiring a student triggers a cascade of bureaucratic actions, illustrating how a single decision can activate multiple layers of complexity. Managing these interactions presents a formidable challenge in software engineering - taming the complexity is a substantial part of the job. However, game programming provides a more structured context to study these complexities, with ample open-source resources for research. Jifeng Wu: [38:55] Your insights on game programming are quite valuable. [38:59] I hadn't realized the prominence of such problems in that field. Your points offer an excellent foundation for my research into these issues. Robert Xiao: [39:16] There are indeed intriguing research opportunities at the intersection of AI, software, programming languages, and HCI. Although my HCI pursuits are broad, one key HCI interest is explainable AI. As language models advance, the ability to explain AI operations lags, posing a risk of increasing reliance on inscrutable systems. Advancing explainable AI methodologies will be critical in HCI, particularly in the coming years. Jifeng Wu: [42:10] Yes, indeed. What captivates my interest more than the empirical approach to software engineering - like automated bug detection and performance optimization - is the human-computer interaction aspect, particularly making developers' lives easier. This is crucial, especially for rapid prototyping. Robert Xiao: [42:42] Right, your IDE example aligns perfectly with that. It's a tool that enhances usability for people - expanding how they interact with typing. It could very well be something for publication. So, you're drawing connections here. When do you expect to graduate? Jifeng Wu: [43:05] If all goes according to my advisor's plan, I should graduate in May 2024. Robert Xiao: [43:13] Okay. And your advisor is? Jifeng Wu: [43:16] Caroline Lemieux from the Software Practices Lab. Robert Xiao: [43:20] Oh, yes, she's a recent addition. So, she's guiding you toward a May graduation. And regarding your Ph.D., are you considering continuing in the Software Practices Lab or exploring other areas? Jifeng Wu: [43:38] Well, our lab's focus is split between user studies and the technical side, like bug detection or software fuzzing, and, on the other hand, theoretical topics like formal semantics. There isn't much overlap with HCI and usability, so I'm uncertain about where I'll pursue a Ph.D., if at all, but likely not within our Software Practices Lab. Robert Xiao: [44:21] That's something to ponder, especially as you approach graduation in May. Are you already applying to Ph.D. programs? Jifeng Wu: [44:34] I am still deciding. I'm still considering my options. Robert Xiao: [44:38] Sure, it's a big decision. My focus is on VR/AR-related projects, where my funding comes from. It's trickier to shift to developing support tools for developers without an established portfolio. However, I'm open to discussing co-advisory opportunities or committee collaborations if you pursue a Ph.D. here at UBC. Jifeng Wu: [45:44] That's encouraging to hear. Robert Xiao: [45:45] To clarify, I'm not sure I could supervise a Ph.D., but I'm open to discussing future possibilities. Jifeng Wu: [45:59] That aligns with my thoughts as well. Robert Xiao: [46:01] Engaging in research discussions can help refine your interests, which is beneficial for crafting a strong research statement for Ph.D. applications. These conversations can guide your initial research direction, even though your focus may evolve. Jifeng Wu: [47:10] Thank you for the insights and advice today. They've been very helpful, and I'll reflect on them further. Robert Xiao: [47:24] I'm glad to assist. You should have my email address. Jifeng Wu: [47:30] Is it listed on your website? Robert Xiao: [47:32] It should be - unless my website isn't updated with my current email, which would be a blunder. Let me check. Jifeng Wu: [47:36] I'll look it up. Robert Xiao: [47:37] If it's on there, then it's correct. Jifeng Wu: [47:45] Yes, it's on your site. Robert Xiao: [47:47] Great, feel free to reach out anytime. Robert Xiao: [47:56] I'm impressed by your work and would be happy to attend your thesis presentation when the time comes. Jifeng Wu: [48:15] I appreciate that. Our conversation today has been very enjoyable. Robert Xiao: [48:21] It was an enlightening chat, indeed. If you have further questions or topics, don't hesitate to email me. Jifeng Wu: [48:34] For now, that's all I have. If something else comes up, I'll send you an email. Thank you.","categories":[{"name":"Meeting Minutes","slug":"Meeting-Minutes","permalink":"https://abbaswu.github.io/categories/Meeting-Minutes/"}],"tags":[]},{"title":"Nico Ritschel's Ph.D. Defense Summary","slug":"Nico-Ritschel-s-Ph-D-Defense-Summary","date":"2023-10-13T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/10/13/Nico-Ritschel-s-Ph-D-Defense-Summary/","link":"","permalink":"https://abbaswu.github.io/2023/10/13/Nico-Ritschel-s-Ph-D-Defense-Summary/","excerpt":"","text":"Nico Ritschel's research focuses on refining block-based programming by integrating elements from visual programming to make it more accessible and effective for end-users, especially in the robotics domain. Problem Statement Block-based programming is mainly used for computer science education. Can they target other tasks, such as end-user programming? The challenge: end-users often need to write larger, real-world programs, contrasting with the simple toy examples students typically handle. Traditional block-based programming struggles with scalability, especially in terms of readability. While visual end-user programming tools like Excel and Simulink support bigger programs through domain-specific visual abstractions, creating new visual languages is difficult and costly. Solution Approach: Merge design features from visual programming into block-based programming languages. Target Domain: Robotics Current Scenario: Professional tools exist, but they're challenging to use. There needs to be more effective block-based tools in the domain. Robot Arms for Factory Floors: Task: Coordinate and synchronize two robot arms. Issues: Current block-based languages require complex solutions like nontrivial mutexes. Solution &amp; Studies: Proposed two design ideas: Represent programs for each arm vertically and side-by-side. Synchronized actions appear as shared nodes between the arms. A left-to-right flow resembling video editing. The 'side-by-side' design was selected. A study found that end-users using this design outperformed those using a commercial, text-based tool. Mobile Robots for Warehouses &amp; Labs: Task: Handle large tasks across multiple workstations. Issues: Difficulty decomposing long programs and locating where to make changes. Solutions &amp; Features: Introduced block-based language that supports functional decomposition. Provided two separate canvases: one for task composition/movement and the other for low-level task definitions. Included triggers as dataflow graphs to improve the visibility of nested expressions and enhance user freedom in structuring programs. Questions Addressed During the Practice Session Why focus on the two robotics scenarios? They are important and relevant in the robotics domain. These scenarios present challenges for end-users learning to program. They represent a complex form of programming that's worth refining. Would functional programming principles enhance end-user visual programming, given the imperative nature of block-based programming? The inherent complexity in robotics means many elements can't be simplified. Introducing functional programming might not necessarily boost user productivity. What was the environment for user studies? Engaged actual end-users for genuine feedback. Also recruited students from non-computer science departments for a broader perspective. Questions Asked During the Ph.D. Defense How were the visions and observations formulated? Separate users into traditional versus new environments and then compare. Gain knowledge of their needs and patterns. Test on a small pool of users to refine the design. How do you account for the spectrum of end-users regarding programming experience, domain-specific task time, and tool experience? Which results were the most and least robust? What factors made the tool easy to learn? The \"blocks\" concept is already well-known. The tool matches the users' previous domain-specific knowledge (e.g., separate columns for two arms). How realistic is the decomposition at scale? Any evidence from related work? More of a \"lower bound,\" limited by the time of the user study. Why was the comparison made between block-based methods and graph-based methods? Graph-based methods are already used in end-user programming, such as game programming. What is the importance and implication of the determined p-value? We have a null hypothesis - there is no difference between the performance of the two groups. What improvements (e.g., 5%) are worthwhile? What are the advantages of block-based approaches over dataflow, and how can this be further investigated? Different aspects, e.g., reading vs writing Different domains, e.g., robotics vs game Different styles of programs Different representations of graphs How are potential accessibility challenges addressed? Already addressed to a degree in the normal block-based domain. Domain-specific challenges are directions for future work. How does the new tool compare with LLMs? Can work together. Have advantages in evolution and understanding vs. writing something that would work the first time. Debugging. Reliability. No training required. What follow-up studies are anticipated for real-world usage? How do you anticipate the tool's usability in practical scenarios? Follow-up studies based on real-world usage in the wild may encounter unanticipated, really specific problems. Is your tool something someone wants to use in practice? Would featuring a table of reactive values a la Excel be beneficial? How do different domains within computer science influence the tool's design and analysis? What interdisciplinary expertise would be beneficial? Information visualization. Designing design drafts with an expert in visualization would be beneficial. What about your tool's applicability to expert programmers instead of end users? Different design goals.","categories":[{"name":"Meeting Minutes","slug":"Meeting-Minutes","permalink":"https://abbaswu.github.io/categories/Meeting-Minutes/"}],"tags":[]},{"title":"Conversation with Prof. Margo Seltzer","slug":"Conversation-with-Prof-Margo-Seltzer","date":"2023-10-10T07:00:00.000Z","updated":"2023-11-06T07:31:36.161Z","comments":true,"path":"2023/10/10/Conversation-with-Prof-Margo-Seltzer/","link":"","permalink":"https://abbaswu.github.io/2023/10/10/Conversation-with-Prof-Margo-Seltzer/","excerpt":"","text":"Introduction to Prof. Margo Seltzer (quoted from Wikipedia) \"Margo Ilene Seltzer is a professor and researcher in computer systems. She is currently the Canada 150 Research Chair in Computer Systems and the Cheriton Family Chair in Computer Science at the University of British Columbia. Previously, Seltzer was the Herchel Smith Professor of Computer Science at Harvard University's John A. Paulson School of Engineering and Applied Sciences and director at the Center for Research on Computation and Society.\" Question: How did you conduct research across a variety of domains, from operating systems to machine learning systems? Prof. Seltzer: I've always been intellectually curious and I find almost all research problems fascinating. Engaging in discussions with diverse people has also fueled my passion. When I was a junior faculty member, I focused on tenure and focused on core systems research, but that was miserable. However, I still explored different areas. My deep interest lies in software architecture, even though my Ph.D. was in storage. I was fortunate when another lab decided to support my research. This shift allowed me to progress from storage to core systems. I also got interested into data provenance, especially realizing that we could do a lot more at the systems level. Transitioning to machine learning was a natural progression, driven mainly by collaborations with graduate students and other partners. Question: Why did you pursue a Ph.D. in storage if you were more interested in core systems? Prof. Seltzer: Before pursuing my Ph.D., I was primarily involved with databases. However, as I delved deeper, my curiosity veered towards system issues. Question: How do you manage evolving interests during a Ph.D.? Prof. Seltzer: It's uncommon for Ph.D. students to plot a lifetime research agenda. Instead, it's about producing one miracle per paper and developing the skills to do research for your whole life. The key is to focus on accomplishing your first piece of independent research during your Ph.D. Choosing a supervisor you get along well with is most important. It's essential to be involved in an interesting area and join a lab that aligns with your interests. However, a perfect match isn't always necessary. Looking at co-supervised students can give insights into potential co-supervision opportunities. As a Ph.D. student, your primary goal should be to define your research problem. Although you shouldn't jump between entirely different areas, it's crucial to select a project that genuinely interests you in the first year. Other interests can be pursued as side projects. To maintain engagement, pick a broad domain that offers a plethora of projects you find captivating. Before starting a Ph.D., actively seek out research papers that intrigue you and identify the labs behind them. Question: What's your vision for the future of Computer Systems? Prof. Seltzer: A pressing concern is that people are not very good at writing software that works. We need to develop strategies to create software with minimal bugs from the ground up. Embracing modularity can be a solution, and the solution is about software architecture. Researchers focus on verifying existing software products because the publication cycle is way too short. Moreover, we lack good metrics for evaluating software architecture, and there is no equivalent of a debugger for software architecture. Software architecture, in its current state, remains an art more than a well-defined discipline. Often, professionals in the field rely heavily on mentors, and they don't see the growth of a new generation of software architects. Personal Comments and Recommendations: I'm pleased to note your inspiration derived from challenges in the 'Type Inference for Python' project, including the importance of formalizations and specifications both for the design goal and for implementation, the tedious and fault-prone task of setting up an evaluation pipeline, etc. Deep learning thrives in domains with a clear ground truth. In other scenarios, basic probabilistic methods might offer better results. Your task of combining AST traversal with introspection of live objects reminds me of the work I did in \"StarFlow: A Script-Centric Data Analysis Environment\" and Arpan Gujarati's tracing infrastructure efforts in Python. Should you wish to delve deeper into software architecture for data science and machine learning, I recommend focusing on constructing intricate software like operating systems instead of shorter data wrangling scripts. Or you can explore the challenges in experimental frameworks. For insights on this, consider discussing with Joe Wonsil. Additionally, Philip Guo at UCSD has an intriguing Ph.D. thesis about tools for research programmers that might be of interest to you.","categories":[{"name":"Meeting Minutes","slug":"Meeting-Minutes","permalink":"https://abbaswu.github.io/categories/Meeting-Minutes/"}],"tags":[]},{"title":"Pre-MICCAI Workshop@UBC Observations and Gained Insights","slug":"Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights","date":"2023-10-08T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/10/08/Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights/","link":"","permalink":"https://abbaswu.github.io/2023/10/08/Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights/","excerpt":"","text":"From the Pre-MICCAI Workshop@UBC website: The Pre-MICCAI Workshop is a dynamic and innovative platform that unites machine learning and medical computer vision. As a prelude to the prestigious MICCAI (Medical Image Computing and Computer-Assisted Intervention) conference, this workshop serves as a vital nexus where experts, researchers, and enthusiasts converge to explore cutting-edge advancements, exchange knowledge, and foster collaborative partnerships in the field of medical image analysis. Selfie Shaoting Zhang (Shanghai AI Lab) - Keynote Talk 2 - Foundation Models in Medicine: Generalist vs Specialist Advantages of Large Models: Emergent abilities. Long-tail problems (only a small amount of fine-tuning is required for downstream tasks and does not require a tremendous amount of data collection and labeling). Model sharing strengthens data security. Shanghai AI Lab presents OpenMEDLab (open-source medical image and language foundation models). Utilizing a single model with varied prompts for diverse tasks. Large language model training encompasses: Self-supervised pre-training. Instruction tuning. RLHF. Plugins for accessing updated information without retraining. Computer vision researchers lean towards generalist models due to the technical challenges. Clinicians prefer specialist models to solve day-to-day work. Question: Will medical foundation models support more modalities in the future besides vision and language? Answer: People will still focus on one modality for one model with high accuracy to address practical business demands. Multiple models can be used on demand to handle multimodal data. Briefings Sana Ayromlou - Continual Class-Specific Impression for Data-free Class Incremental Learning Focuses on training models over newly introduced classes, termed incremental learning. Challenges include the loss of old data, resulting in catastrophic forgetting. Proposed Solution: Generate synthetic medical data from prior classes using model inversion (extracting training data from the model) and employing cosine-normalized cross-entropy loss. Hooman Vaseli - ProtoASNet Emphasizes the importance of interpretability in AI solutions, especially in healthcare. Core Technology: Prototypical neural networks, which \"learn a metric space in which classification can be performed by computing distances to prototype representations of each class.\" Ruogu Fang (University of Florida) - Keynote Talk 4 - A Tale of Two Frontiers: When Brain Meets AI Research Vision: Integrate domain knowledge over mere data-driven approaches. Harness neuroscience principles for next-gen AI designs. Leverage AI in testing neural science hypotheses and promoting brain health. Deep Evolutionary Networks with Expedited Genetic Algorithms for Medical Image Denoising Auto feature extraction and hyperparameter search are major pain points in deep learning research (compared with traditional machine learning research) faced by deep learning researchers. Fine gene transfer learning to optimize on a larger dataset - c.f. portfolio balance in finance Question: Is it possible to combine the genetic algorithm that maintains a gene pool of neural networks with ensemble learning? Answer: Different objective. Emergence of Emotion Selectivity in A Deep Neural Network Trained to Recognize Visual Objects Simple, interpretable neural network architecture based on biology. Representation similarity between the DNN model and brain amygdala. $1M NSF funding. Modular machine learning for Alzheimer's disease classification from retinal vasculature Retina data is easy to collect. A lot of information (gender, body mass index) can be seen from the retina. The results are interpretable. Herv Lombaert (ETS Montreal) - Keynote Talk 3 - Geometric Deep Learning - Examples on Brain Surfaces Research directions: Geometry and Machine Learning. Correspondences and variability existent in the brain. Motivation: Traditional algorithms frequently rely on an image grid (pixels). However, in neuroimaging, data is often on 3D surfaces. Two neighboring points may be neighbors but may lie very far away on such a surface. How to learn on such surfaces? How do we transfer convolution and pooling on images to such surfaces? Solution: Represent surfaces as graphs. Project problem into spectral space (spectral shape analysis). An object's vibration pattern is governed by shape - spectral space captures a unique intrinsic shape signature. Extract spectral signature via spectral decomposition and exploit to find correspondences. Enables transforming convolutions on surfaces to convolutions on spectral embeddings, enabling classical architectures on brain surfaces. Ongoing work: Active learning to reduce annotation effort - focus on sample-level uncertainty and find the most uncertain images. Goals: Informative and diverse samples. Works: TAAL: Test-time augmentation for active learning in medical image segmentation Active learning for medical image segmentation with stochastic batches Ali Bashashati/Ruogu Fang/Shaoting Zhang/Herv Lombaer/Jun Ma - Panel Discussion The influence of Large Language Models is growing significantly. What changes do you think LLMs will bring about in medical imaging (from both positive and negative sides)? Language contributes to improved performance. Still need a diversity of models to investigate different modalities and tasks. Large language models help in day-to-day routine tasks. They are a copilot which facilitates the processing of huge amounts of information in pathology and brain research. Reduces cost and boosts accessibility for patients. Multimodal data integration. LLMs face data privacy and trustworthiness. When to use LLMs and when to use human abilities requires careful thinking. What other recent medical image analysis advancements excite you the most? Classic problems like segmentations and how to capture geometry remain unsolved. More comprehensive and dynamic brain-inspired, biologically-inspired AI. Understanding the biology behind the data will help you design more applicable models. Those models can better make a difference Prior knowledge is important in addition to big data. Foundational models will explore all non-synthetic data in the next few years; no new data will exist. Montreal is a major hub for neuroscience and AI. For the many students here, what technical skills and knowledge should the next generation of medical image analysis researchers prepare for? Know the neglected basics, e.g., solid mathematical background and proficiency in programming Understand the data Ability to explain the results and ask the question of why and how Visualization is very important for both exploratory data analysis and publishing Learning from mistakes - find out why a model doesn't work instead of throwing in different models Ask yourself: Who will care about an increase in accuracy? Is it significant? Will it have tradeoffs in robustness, explainability, etc.? Quickly take up new skills (mathematics, programming, etc.) Research paradigms have changed in the foundation model era - how to leverage foundation models for your field to stand on the shoulders of giants? Low-level implementation details such as preprocessing, multiprocessing in coding for large-scale data, model development, multi-node distributed training, efficient fine-tuning, and model deployment on constrained environments are also critical skills. Work and have fun at the same time. Perseverance in the face of failure is one of the most essential qualities for Ph.D. students. Question: The future of models for specific tasks (e.g., segmentation) vs end-to-end models. New models for specific tasks make lovely reads. Methodology will change, but specific tasks will stay there. However, improving specific tasks will gradually shift towards industry. Universities will focus on publishing the first paper in a domain, while industry will focus on publishing the last paper in a domain. In the end, we care about helping patients.","categories":[{"name":"Conferences","slug":"Conferences","permalink":"https://abbaswu.github.io/categories/Conferences/"}],"tags":[]},{"title":"Understanding the Name, Structure, and Loss Function of the Variational Autoencoder","slug":"Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder","date":"2023-09-30T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/09/30/Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder/","link":"","permalink":"https://abbaswu.github.io/2023/09/30/Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder/","excerpt":"","text":"Despite the intuitive appeal of variational autoencoders (VAEs), their underlying principles can be elusive. After extensive research across papers and online resources, I will summarize the core insights behind the VAE's name, structure, and loss function and try to explain how the mathematical formulas used to describe the VAE came into being from first principles, as opposed to simply providing interpretations for them. Basics of VAEs VAEs are probabilistic generative models, when trained on a dataset \\(X\\), allow us to sample from a latent variable \\(Z\\) and generate output resembling samples in \\(X\\) through a trained neural network \\(f: Z \\rightarrow X\\). This can be formulated as making the probability of generating \\(X = x\\) as close as possible to the actual \\(P(X = x)\\) (known quality) under the entire generative process. Ideal Training Goal In the ideal situation, based on the marginal distribution formula we have \\(P(X = x) = \\int{P(X = x | Z = z) P(Z = z) dz}\\). Thus, the training goal of variational autoencoders is to make the actual \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\) as close to \\(P(X = x)\\) as possible. Latent Variable Distribution VAEs select a multivariate normal distribution for the latent variable \\(Z\\) based on the principle that any distribution in \\(d\\) dimensions can be generated by mapping normally distributed variables through a sufficiently complicated function, which could be approximated using the neural network \\(f: Z \\rightarrow X\\) we train. Approximation Challenge Having reasonably decided \\(Z \\sim N(0, I)\\), we may calculate the actual \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\). This is straightforward to approximate: we can randomly sample a large number of \\(Z\\) values \\(\\{z_1, \\dots, z_n\\}\\), and approximate \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\) as \\(\\sum_{j}^{n}{P(X = x | Z = z_j)}\\). However, for most \\(Z\\) values, \\(P(X = x | Z)\\) will be nearly zero, contributing almost nothing to our calculation. This is especially the case in high dimensional spaces, for which an extremely large number of samples of \\(Z\\) may be required. To address the problem, we can attempt to sample values of \\(Z\\) that are likely to have produced \\(X = x\\) and compute \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\) just from those. The \"Variational\" Aspect: To do so, we can fit another parametrized function \\(Q(Z | X = x)\\), which can give us a distribution over \\(Z\\) values that are likely to produce \\(X = x\\) through \\(f: Z \\rightarrow X\\) given \\(X = x\\). This is an example of a variational Bayesian method, which involves finding an \"optimal\" function (a task known as variational calculus) and is the source of the word \"variational\" in variational autoencoders. Minimizing Divergence Theoretically, the values of \\(Z\\) that are likely to have produced \\(X = x\\) follow the conditional distribution \\(P(Z | X = x)\\). Thus, our original goal of making the actual \\(\\int{P(X = x | Z = z) P(Z = z) dz}\\) as close to \\(P(X = x)\\) as possible can be transformed to minimizing the Kullback-Leibler divergence between \\(P(Z | X = x)\\) and \\(Q(Z | X = x)\\): \\[KL(Q(Z | X = x) || P(Z | X = x)) = \\int{Q(Z = z | X = x) \\log{\\frac{Q(Z = z | X = x)}{P(Z = z | X = x)}} dz}\\] According to Bayes' Law, \\[P(Z = z | X = x) = \\frac{P(X = x | Z = z) P(Z = z)}{P(X = x)}\\] Thus, we have: \\[\\int{Q(Z = z | X = x) \\log{\\frac{Q(Z = z | X = x) P(X = x)}{P(X = x | Z = z) P(Z = z)}} dz}\\] \\[= \\int{Q(Z = z | X = x) (\\log{\\frac{Q(Z = z | X = x)}{P(Z = z)}} + \\log{P(X = x)} - \\log{P(X = x | Z = z)}) dz}\\] \\[= \\int{Q(Z = z | X = x) \\log{\\frac{Q(Z = z | X = x)}{P(Z = z)}} dz} + \\int{Q(Z = z | X = x) \\log{P(X = x)} dz} - \\int{Q(Z = z | X = x) \\log{P(X = x | Z = z)} dz}\\] Note that: \\[\\int{Q(Z = z | X = x) \\log{\\frac{Q(Z = z | X = x)}{P(Z = z)}} dz} = KL(Q(Z | X = x) || P(Z))\\] \\[\\int{Q(Z = z | X = x) \\log{P(X = x)} dz} = \\log{P(X = x)} \\int{Q(Z = z | X = x)} dz = \\log{P(X = x)}\\] Thus, we have: \\[KL(Q(Z | X = x) || P(Z | X = x)) = KL(Q(Z | X = x) || P(Z)) + \\log{P(X = x)} - \\int{Q(Z = z | X = x) \\log{P(X = x | Z = z)} dz}\\] As \\(\\log{P(X = x)}\\) is constant, if we were to minimize \\(KL(Q(Z | X = x) || P(Z | X = x))\\), we should minimize: \\[KL(Q(Z | X = x) || P(Z)) - \\int{Q(Z = z | X = x) \\log{P(X = x | Z = z)} dz}\\] To further transfer that into a calculatable function, we need to be more specific about the form that \\(Q(Z | X)\\) will take. The usual choice is to say that \\(Q(Z | X = x) = N(Z | \\mu(X = x), \\Sigma(X = x))\\), i.e., \\(Q(Z | X = x)\\) follows a Gaussian distribution where the mean and covariance matrix are calculated by parameterized functions (trained neural networks) given \\(X = x\\). In this case, fitting \\(Q(Z | X = x)\\) involves training these neural networks. The advantages of this choice are computational, as \\(KL(Q(Z | X = x) || P(Z)) + \\log{P(X = x)}\\) is now a KL-divergence between two multivariate Gaussian distributions, which can be computed in closed form. As for \\(\\int{Q(Z = z | X = x) \\log{P(X = x | Z = z)} dz}\\), it depicts the expected log-likelihood of generating \\(X = x\\) as the VAE's output through \\(f(Z)\\) when sampling from \\(Q(Z = z | X = x)\\) given \\(X = x\\). Thus, it can be treated as the \"reconstruction loss\" of the VAE, and different closed-form indices, such as mean square error, may be used as proxies of it depending on the project domain. Why \"Autoencoders\"? Despite the mathematical basis of VAEs being quite different from classical autoencoders, they are named \"autoencoders\" due to their final training objective involving an encoder (the neural networks \\(\\mu\\) and \\(\\Sigma\\) determining mean and covariance) and a decoder (the neural network \\(f\\)), which resembles a traditional autoencoder in structure. References https://arxiv.org/abs/1606.05908 https://agustinus.kristia.de/techblog/2016/12/10/variational-autoencoder/ https://arxiv.org/abs/1312.6114 https://arxiv.org/abs/1907.08956 https://stats.stackexchange.com/questions/485488/should-reconstruction-loss-be-computed-as-sum-or-average-over-input-for-variatio https://stats.stackexchange.com/questions/540092/how-do-we-get-to-the-mse-in-the-loss-function-for-a-variational-autoencoder https://stats.stackexchange.com/questions/464875/mean-square-error-as-reconstruction-loss-in-vae https://stats.stackexchange.com/questions/323568/help-understanding-reconstruction-loss-in-variational-autoencoder","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://abbaswu.github.io/categories/Mathematics/"}],"tags":[]},{"title":"My Software Engineering Philosophy","slug":"My-Software-Engineering-Philosophy","date":"2023-09-24T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/09/24/My-Software-Engineering-Philosophy/","link":"","permalink":"https://abbaswu.github.io/2023/09/24/My-Software-Engineering-Philosophy/","excerpt":"","text":"If you deprive yourself of outsourcing and your competitors do not, you're putting yourself out of business. Lee Kuan Yew Do the high-level, high-value requirements, analysis, and design work in an incremental fashion while not sacrificing rigor. Maximize the utilization of tools that make coding, testing, and operations as cheap, trivial, straightforward, and error-free as possible, minimizing technical debt, including: Generative AI tools like ChatGPT. Functional Programming. A critique on both the waterfall model and the agile model.","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://abbaswu.github.io/categories/Reflections/"}],"tags":[]},{"title":"The Cornell, Maryland, Max Planck Pre-doctoral Research School 2023 Observations and Gained Insights","slug":"The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023-Observations-and-Gained-Insights","date":"2023-09-08T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/09/08/The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023-Observations-and-Gained-Insights/","link":"","permalink":"https://abbaswu.github.io/2023/09/08/The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023-Observations-and-Gained-Insights/","excerpt":"","text":"Group Photo Panel Session 2: \"Research in industry vs. academia\" Problem Focus &amp; Recognition Industry tends to focus on concrete problems. In academia, broader issues are often addressed. Authorship and credit in academia is complex. It's not a zero-sum game. It's not just about who is first or second author; giving credit to students doesn't mean professors won't get any. Publication &amp; Quality The emphasis is on publishing fewer papers but ensuring they are of high quality. It's not about the quantity but the impact and quality of the papers. The first, last, or best paper on a topic are the most influential. Career Path Before securing a tenure professor position, many go through multiple postdocs and even stints as industrial research scientists. Only about 10-20% of PhDs eventually become faculty. Some research scientists find academic-like environments within the right industry groups. Factors Differentiating Academia and Industry: Industrial research must eventually have some commercial value. In academia, there are constraints like obtaining funding, student recruitment, and equipment acquisition. Academics have better job security and can rebound from mistakes. Industry doesn't need to chase grants or funding in the same way academia does. Skills &amp; Transitions Transferring skills between departments or companies is straightforward. Transitioning between academia and industry is often a one-way street. It's challenging to return to academia from industry unless one maintains a consistent publishing record and works on research-valued projects. Geographical and Topic Mobility Researchers are encouraged to be flexible, moving across countries and topics. Work-Life Balance Systems vary across locations. Enforce personal boundaries and learn to say no. A balance doesn't mean absence of stress. In the industry, even if the work-life balance is okay, stress may arise from working on undesired projects or facing peer pressure. Find people who become friends with you. Two-Body Problem It's more of an issue in academia than in industry since it's easier to change companies than academic institutions. Personality and Approach Industry caters to hackers and those interested in tooling. Academics focus on research and higher purposes and see coding as a tool. Effective communication, including selling your idea in proposals and talks, is vital. Startups vs. PhD Journey Both require a significant commitment, typically around 6-8 years to IPO. Startups demand full devotion, often with little to no work-life balance. Funding &amp; Tenure If a grant from a company fails, there will be no direct legal consequences, but the likelihood of getting another might be reduced. Tenure provides a basic salary and job security, but researchers still need to raise funds for their research. Doing a job aligned research can be beneficial for dissertation and future career opportunities. Laxman Dhulipala (2nd Lecture) Graphs are ubiquitous structures. Implementing high-performance graph algorithms speeds up scientific discovery. I don't work on dense graphs. Real-world graphs are sparse, and I haven't seen a dense graph in practice in 10 years. I focus on shared-memory algorithms and don't recommend programming supercomputers until you have to. Recommended reading: Scalability! But at what COST? Should batch updates to dynamic graphs More parallelism Reduces the cost of each update Representing adjacency information using purely functional trees are safe for concurrency. Guest Lecture: Yiting Xia There are different available connections at different time slices. Precomputing routes and handling link failure is still work in progress. Group-Mentoring Session Peter Druschel and Bobby Bhattacharjee Key Skills and Knowledge Emphasized the importance of academic aptitude and the ability to work in unstructured environments. Problem-solving Approach Seek problems that are significant, solvable, and align with your skill set. Recognize that one may not always approach the right problem from the best angle. Handling setbacks is crucial. Time spent on tackling a problem is never lost. Resilience, dedication, and discipline are essential traits for success. Read many things that are loosely related to solve a problem, as they might offer insights. Application Strategy Apply to a minimum of 5-10 institutions. Do the homework for providing a strong application, especially given low acceptance rates, like 10%. Interests and Graduate Programs Have a broad range of interests when considering a graduate program. Opt for programs that offer a wide variety of choices. Expressing diverse interests in applications can improve acceptance chances. It's advisable not to close one's doors apriori. Monitoring Progress in Grad Programs A competent group advisor is crucial, as they will guide and look out for students challenges like selecting an excessively challenging problem, lacking motivation, or poor time management. Set achievable milestones that lead to publications, helping to build a solid publication record. ### Mariya Toneva Changing Discipline during Ph.D. Evaluate if the institution has the necessary resources to support this transition. Traits of an Ideal Ph.D. Student Effective communication skills. Strong critical thinking abilities. A robust computational background. Prior research experience. Linguistics Noted a resurgence in the domain of linguistics as opposed to pure data-driven techniques. MPI-SWS MPI-SWS is highly recommended for programming languages, especially when collaborating with diverse groups of people. Diving into NLP (Natural Language Processing) - Hop On Now? When considering venturing into NLP, focus on: Experts who have a distinct vision in a less-saturated niche. Those with substantial experience in related fields, such as the intersection of NLP and robotics. Distinguishing Yourself in Applications To stand out: Foster qualities like initiative, drive, and ambition. Accumulate experiences that align with and support your academic and research interests. Obtain references that can vouch for your character and work ethic. It's also essential to explore and consider multiple options or paths. Lorenzo Alvisi Cultivating an Academic Sense To nurture an academic mindset, one should assess how an individual performs when faced with a problem. He mentioned the \"Dijkstra club\" at UT Austin as an example. Emphasized the significance of \"beautiful work\" and that it's crucial for individuals to produce work of beauty and quality. Observing and learning from the endeavors of others is beneficial. Life's Blueprint Life does not come with a set map but rather a compass for direction. Professor Alvisi never limited his imagination about his capabilities. Guiding principles in life: Seeking personal happiness. Maintaining healthy relationships. Pursuing a fulfilling job that combines happiness with challenges. Acceptance of uncertain outcomes: One might not always know if they will succeed or fail. The importance of personal growth: Find joy in self-improvement. Shared personal experience of pursuing two Ph.D. degrees, the first of which was at an institution he didn't particularly favor. Highlighted that struggles are often hidden from view. Career Perspectives One's career doesn't necessarily peak at a fixed point; there's always potential for growth, including entering academia. Career choices are not always black and white; it depends on personal preferences and aspirations, such as seeking excellent opportunities close to home. Consider the duration of your investments in particular career choices. Not every commitment needs to be long-term. Balancing Hobbies and Work Prof. Alvisi shared advice from his mentor's mentor about integrating hobbies into professional life. While he had diverse interests, he made sacrifices to focus on computer science due to his intellectual capacities. Some hobbies were too time-consuming. Emphasized the importance of hobbies as they provide a necessary balance and maintain mental well-being. Addressing the Two-Body Problem Universities recognize the challenge when both partners in a relationship are professionals. If partners excel in different domains, there's potential for both to be hired with attractive incentives. Solutions include proactive planning, alternating priorities between partners over the years, and considering remote work opportunities. Other Insights Mentioned the Sloan Fellowship as a notable achievement before tenure. Advised young professionals to delay specialization as long as possible. Explore various options. Encouraged students to seek advice from multiple professors to gain a diverse range of opinions and insights. Tapomayukh Bhattacharjee (2nd Lecture) There are six activities of daily living (ADLs) defined in literature: personal hygiene or grooming, dressing, toileting, transferring or ambulating, and eating Anomaly detection is used in processing sensor data. A* is widely used in motion planning due to its efficiency and optimality (it never overestimates the cost). Motion planning time = search time + collision checking time (~90%). Therefore, the author proposed lazy A* (which finds an optimal path in an unconstrained situation, goes over collision checking while on the path, and re-searches a path if a collision is encountered). Collect a dataset before embarking on research. To understand how to manipulate different kinds of foods, the author created a food manipulation taxonomy. Choose hardware components for real-world deployability. Use deformation of points on a gel coupled with computer vision algorithms to measure shear force Add structure to machine learning algorithms to overcome a lack of data. If integrating multimodal data sources, think of where to integrate as the size or magnitude of different data may be inconsistent. A \"bandit\" algorithm is an RL algorithm where we utilize partial feedback of one step in the decision-making process, unlike conventional RL algorithms with \"episodes\" spanning multiple steps. Audience question: How to stay up-to-date with the state-of-the-art (especially in the fast-changing landscape of machine learning)? One of the main tasks of faculty life Look at titles and abstracts of publications in all well-known conferences. Organize reading groups and reading sessions. Interact with known other research groups. Derek Dreyer: How to write papers and give talks that people can follow Many papers suffer from the TMI (too much information) problem. Aim at giving constructive principles that are easy to check and fix. A paper is different from a textbook - people aren't as committed to reading a paper as they are to reading a textbook. A good but not interesting paper tends to get a \"B\" or a \"weak accept.\" Putting the Related Work section at the front (as opposed to in the back before the Conclusion) may hinder unfamiliar authors from understanding your work. Most people don't listen to talks to determine whether they should read a paper. Instead, they listen to talks to discuss with others. The main goal of a talk is to give people positive feelings about your work. A talk should only cover the intro and key ideas sections of the corresponding paper. The key ideas should be the high point in your talk before presenting the takeaway messages. Add visual elements to emphasize one point per slide. Use smooth animations to help the listener follow.","categories":[{"name":"Conferences","slug":"Conferences","permalink":"https://abbaswu.github.io/categories/Conferences/"}],"tags":[]},{"title":"From the Fourier Series to the Fourier Transform to the Discrete-time Fourier Transform: Demystifying the Formulas","slug":"From-the-Fourier-Series-to-the-Fourier-Transform-to-the-Discrete-time-Fourier-Transform-Demystifying-the-Formulas","date":"2023-09-04T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/09/04/From-the-Fourier-Series-to-the-Fourier-Transform-to-the-Discrete-time-Fourier-Transform-Demystifying-the-Formulas/","link":"","permalink":"https://abbaswu.github.io/2023/09/04/From-the-Fourier-Series-to-the-Fourier-Transform-to-the-Discrete-time-Fourier-Transform-Demystifying-the-Formulas/","excerpt":"","text":"In realms as broad as electrical engineering, acoustics, optics, signal processing, quantum mechanics, and econometrics, the Fourier Series, Fourier Transform, and Discrete-time Fourier Transform play a pivotal role in analyzing signals by allowing us to decompose them into simpler components. Many articles present their formulas or dive into their intuition and applications. However, what seems to be missing is a blog post that explains the derivation of their formulas in a way that is both clear and accessible, requiring no more than a rudimentary understanding of calculus. Fourier Series Standard Form of the Fourier Series Our journey begins with the Fourier Series - a method to represent periodic functions as a sum of sine and cosine waves. Let \\(x(t)\\) be a periodic function with period \\(T\\). The standard form of the Fourier series for \\(x(t)\\) is given by: \\[x(t) = \\frac{a_0}{2} + a_1 \\cos{\\frac{2\\pi}{T} t} + b_1 \\sin{\\frac{2\\pi}{T} t} + a_2 \\cos{\\frac{4\\pi}{T} t} + b_2 \\sin{\\frac{4\\pi}{T} t} + \\dots \\] To solve for \\(a_0, a_1, b_1, \\dots\\), we first observe the . Thus, we can multiply both sides of the equation by \\(cos{\\frac{2k\\pi}{T} t}\\) or \\(\\sin{\\frac{2k\\pi}{T} t}\\) \\((k \\in \\{0, 1, 2, \\dots, n\\})\\), and then integrate over one period \\([-\\frac{T}{2}, \\frac{T}{2})\\) to obtain: \\[a_k = \\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) \\cos{\\frac{2k\\pi}{T} t} dt}\\] \\[b_k = \\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) \\sin{\\frac{2k\\pi}{T} t} dt}\\] Exponential Form of the Fourier Series Using Euler's formula \\(e^{ix} = \\cos{x} + i \\sin{x}\\), we can derive: \\[\\cos{x} = \\frac{e^{ix} + e^{-ix}}{2}\\] \\[\\sin{x} = -i \\frac{e^{ix} - e^{-ix}}{2}\\] Substituting these representations of \\(\\cos{x}\\) and \\(\\sin{x}\\) into \\(a_k\\) and \\(b_k\\), we get: \\[a_k = \\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) \\frac{e^{i \\frac{2k\\pi}{T} t} + e^{-i \\frac{2k\\pi}{T} t}}{2} dt}\\] \\[b_k = \\frac{2}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{-i x(t) \\frac{e^{i \\frac{2k\\pi}{T} t} - e^{-i \\frac{2k\\pi}{T} t}}{2} dt}\\] And: \\[a_k \\cos{\\frac{2k\\pi}{T} t} + b_k \\sin{\\frac{2k\\pi}{T} t} = a_k \\frac{e^{i \\frac{2k\\pi}{T} t} + e^{-i \\frac{2k\\pi}{T} t}}{2} - i b_k \\frac{e^{i \\frac{2k\\pi}{T} t} - e^{-i \\frac{2k\\pi}{T} t}}{2} = \\frac{a_k - i b_k}{2} e^{i \\frac{2k\\pi}{T} t} + \\frac{a_k + i b_k}{2} e^{-i \\frac{2k\\pi}{T} t}\\] And: \\[\\frac{a_k - i b_k}{2} = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i \\frac{2k\\pi}{T} t} dt}\\] \\[\\frac{a_k + i b_k}{2} = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{i \\frac{2k\\pi}{T} t} dt}\\] Furthermore, if we let: \\[c_k = \\frac{a_k - i b_k}{2} = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i \\frac{2k\\pi}{T} t} dt}\\] Substituting \\(k \\leftarrow -k\\) into the expression for \\(c_k\\), we will obtain: \\[c_{-k} = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{i \\frac{2k\\pi}{T} t} dt} = \\frac{a_k + i b_k}{2}\\] Thus: \\[a_k \\cos{\\frac{2k\\pi}{T} t} + b_k \\sin{\\frac{2k\\pi}{T} t} = \\frac{a_k - i b_k}{2} e^{i \\frac{2k\\pi}{T} t} + \\frac{a_k + i b_k}{2} e^{-i \\frac{2k\\pi}{T} t} = c_k e^{i \\frac{2k\\pi}{T} t} + c_{-k} e^{-i \\frac{2k\\pi}{T} t}\\] And by substituting \\(k \\leftarrow 0\\) into the expression for \\(c_k\\), we get: \\[c_0 = \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) dt} = \\frac{a_0}{2}\\] Therefore: \\[x(t) = \\frac{a_0}{2} + \\sum_{k=1}^{n}{(a_k \\cos{\\frac{2k\\pi}{T} t} + b_k \\sin{\\frac{2k\\pi}{T} t})} = c_0 + \\sum_{k=1}^{n}{(c_k e^{i \\frac{2k\\pi}{T} t} + c_{-k} e^{-i \\frac{2k\\pi}{T} t})} = \\sum_{k=-n}^{n}{c_k e^{i \\frac{2k\\pi}{T} t}}\\] Where: \\[c_k= \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i \\frac{2k\\pi}{T} t} dt}\\] This is the exponential form of the Fourier series. It is more concise than the standard form of the Fourier series and is used more often in practice. Fourier Transform The Fourier transform is a generalization of the Fourier series, which can analyze the effect of a frequency in any function (which may not necessarily be a periodic function). In this section, we will present how it can be derived from the exponential form of the Fourier series. Given a periodic function \\(x(t)\\) with period \\(T\\), the exponential form of the Fourier series of \\(x(t)\\) is as follows: \\[x(t) = \\sum_{k=-n}^{n}{c_k e^{i \\frac{2k\\pi}{T} t}}\\] Where: \\[c_k= \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i \\frac{2k\\pi}{T} t} dt}\\] Let's say that the period \\(T\\) is associated with a frequency known as the fundamental frequency \\(f_0 = \\frac{1}{T}\\). Given \\(f_0\\), we can rewrite the previous Fourier series as: \\[x(t) = \\sum_{k=-n}^{n}{c_k e^{i 2\\pi k f_0 t}}\\] Where: \\[c_k= \\frac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{x(t) e^{-i 2\\pi k f_0 t} dt}\\] For a non-periodic function, we can consider it as a periodic function with \\(T \\rightarrow +\\infty\\). In this case, the fundamental frequency \\(f_0\\) is an infinitesimal quantity; therefore, we can consider that any frequency \\(f\\) can be expressed as an integer multiple of the fundamental frequency, and the difference between two neighboring frequencies is the fundamental frequency \\(f_0\\). In this case, the fundamental frequency \\(f_0\\) can be expressed as a differential of the frequency \\(f\\), i.e., \\(df\\). In this case, for a possibly non-periodic function \\(x(t)\\): \\[x(t) = \\sum_{k=-\\infty}^{\\infty}{c_k e^{i 2\\pi k (df)t}}\\] \\[c_k= (df) \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi k (df) t} dt}\\] Thus, \\(x(t)\\) can be represented as: \\[x(t) = \\sum_{k=-\\infty}^{\\infty}{[(df) \\cdot \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi k (df) t} dt} \\cdot e^{i 2\\pi k (df) t}]}\\] By considering \\(f \\leftarrow k (df)\\), we can transform the summation into a definite integral: \\[x(t)= \\int_{-\\infty}^{\\infty}{ [(\\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi f t} dt}) e^{i 2\\pi f t}] df}\\] Let: \\[X(f) = \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi f t} dt}\\] Then \\(x(t)\\) can be represented as: \\[x(t) = \\int_{-\\infty}^{\\infty}{ X(f) e^{i 2\\pi f t} df}\\] These two equations are very important. If we know \\(x(t)\\) (i.e., the value of \\(x(t)\\) at any time \\(t\\)), through \\(X(f) = \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi f t} dt}\\), we can compute the relative magnitude of any frequency \\(f\\) over the whole time period. At the same time, if we know \\(X(f)\\) (i.e., the relative magnitude of any frequency \\(f\\) over the whole time period), by means of \\(x(t) = \\int_{-\\infty}^{\\infty}{ X(f) e^{i 2\\pi f t} df}\\), we can calculate the value of \\(x(t)\\) at any time \\(t\\). We refer to \\(X(f)\\) as the Fourier transform of \\(x(t)\\), also known as the spectrum of \\(x(t)\\), and to \\(x(t)\\) as the inverse Fourier transform of \\(X(f)\\). Discrete-time Fourier Transform When we process signals with computers, as computers cannot store a continuous infinite function, we usually take \\(N\\) samples of the original signal \\(x(t)\\) at a certain time interval \\(\\Delta t\\), obtaining an array \\(x[0:N-1]\\). Using \\(x[0:N-1]\\) to estimate the Fourier transform \\(X(f)\\) of the sampled function \\(x(t)\\), we get: \\[X(f) = \\int_{-\\infty}^{\\infty}{x(t) e^{-i 2\\pi f t} dt} \\approx \\int_{0}^{N \\Delta t}{x(t) e^{-i 2\\pi f t} dt} \\approx \\sum_{m=0}^{N - 1}{x(m \\Delta t) e^{-i 2\\pi f m \\Delta t}}\\] If we assume these samples have spanned a period of the original signal, e.g. \\(T = N \\Delta t\\), and that we only consider frequencies satisfying \\(f = k \\frac{1}{N \\Delta t} (k \\in \\{0, 1, \\dots, N - 1\\})\\), we get: \\[X(k \\frac{1}{N \\Delta t}) \\approx \\sum_{n=0}^{N - 1}{x(n \\Delta t) e^{-i 2\\pi k \\frac{1}{N \\Delta t} n \\Delta t}} = \\sum_{n=0}^{N - 1}{x(n \\Delta t) e^{-i 2\\pi \\frac{k}{N} n}} = \\sum_{n=0}^{N - 1}{x[n] e^{-i 2\\pi \\frac{k}{N} n}}\\] Let: \\[X[k] = \\sum_{n=0}^{N - 1}{x[n] e^{-i 2\\pi \\frac{k}{N} n}} (k \\in \\{0, 1, \\dots, N - 1\\})\\] We call such an array of \\(N\\) discrete numbers \\(X[0:N-1]\\) the discrete-time Fourier transform of \\(x[0:N-1]\\), which is a discrete frequency domain representation of \\(x[0:N-1]\\). Using \\(X[0:N-1]\\), we can restore \\(x[0:N-1]\\): \\[x[n] = \\frac{1}{N} \\sum_{k=0}^{N - 1}{X[k] e^{i 2\\pi \\frac{k}{N} n}} (n \\in \\{0, 1, \\dots, N - 1\\})\\] We call \\(x[0:N-1]\\) the inverse discrete-time Fourier transform of \\(X[0:N-1]\\). This is analogous to \\(X(f)\\) being the Fourier transform of \\(x(t)\\) and \\(x(t)\\) being the inverse Fourier transform of \\(X(f)\\) in the continuous case.","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://abbaswu.github.io/categories/Mathematics/"}],"tags":[]},{"title":"On Convolutional Neural Networks and Photographic Lenses","slug":"On-Convolutional-Neural-Networks-and-Photographic-Lenses","date":"2023-08-24T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/08/24/On-Convolutional-Neural-Networks-and-Photographic-Lenses/","link":"","permalink":"https://abbaswu.github.io/2023/08/24/On-Convolutional-Neural-Networks-and-Photographic-Lenses/","excerpt":"","text":"Convolutional neural networks are camera lenses to a computer. A convolutional neural network A camera lens The analogy does not stop at the point that both compress visual information: The evolution of convolutional neural network architectures resembles the evolution of camera lenses. The P-R curve showing the performance of a convolutional neural network is strikingly similar to the MTF curve evaluating lens performance. A P-R curve An MTF curve","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://abbaswu.github.io/categories/Reflections/"}],"tags":[]},{"title":"Our Motivation for Maintaining Our Blog","slug":"Our-Motivation-for-Maintaining-Our-Blog","date":"2023-08-16T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/08/16/Our-Motivation-for-Maintaining-Our-Blog/","link":"","permalink":"https://abbaswu.github.io/2023/08/16/Our-Motivation-for-Maintaining-Our-Blog/","excerpt":"","text":" This blog is a replica of our previous \"problem books\" used for junior and senior high school. In the past, we would record background knowledge, problem solving ideas, and efficient algorithms. Today, we would note down understandings and insights. These are all the result of the hard work of searching and exploring, and we record them down in order to gradually accumulate our knowledge and understanding. Only the direct purpose has been changed from \"preparing for a test\" to \"doing scientific research\".","categories":[{"name":"Reflections","slug":"Reflections","permalink":"https://abbaswu.github.io/categories/Reflections/"}],"tags":[]},{"title":"Timetable of Well-known Conferences in Different Subdomains of Computer Science","slug":"Timetable-of-Well-known-Conferences-in-Different-Subdomains-of-Computer-Science","date":"2023-08-16T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/08/16/Timetable-of-Well-known-Conferences-in-Different-Subdomains-of-Computer-Science/","link":"","permalink":"https://abbaswu.github.io/2023/08/16/Timetable-of-Well-known-Conferences-in-Different-Subdomains-of-Computer-Science/","excerpt":"","text":"We have compiled a timetable of well-known conferences in different subdomains of computer science based on the Class A and Class B conferences in \"Directory of International Academic Conferences and Journals Recommended by the Chinese Computer Society\". Although the precise start dates of each conference vary year by year, the provided start dates provide a general guideline on the relative order of the conferences throughout each year. Name Start Date Subdomain CIDR 01/08/23 Databases/Data Mining/Information Retrieval GROUP 01/08/23 Human Computer Interaction and Ubiquitous Computing POPL 01/15/23 Software Engineering/Systems Software/Programming Languages VMCAI 01/15/23 Software Engineering/Systems Software/Programming Languages HiPEAC 01/16/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems SODA 01/22/23 Theoretical Computer Science PPoPP 02/05/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems AAAI 02/07/23 Artificial Intelligence FPGA 02/12/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems FAST 02/21/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems HPCA 02/25/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CGO 02/25/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems NDSS 02/27/23 Security WSDM 02/27/23 Databases/Data Mining/Information Retrieval FM 03/07/23 Software Engineering/Systems Software/Programming Languages PERCOM 03/13/23 Human Computer Interaction and Ubiquitous Computing FSE 03/20/23 Security SANER 03/21/23 Software Engineering/Systems Software/Programming Languages DCC 03/21/23 Computer Graphics/Multimedia ASPLOS 03/25/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems VR 03/25/23 Computer Graphics/Multimedia IUI 03/27/23 Human Computer Interaction and Ubiquitous Computing ICDT 03/28/23 Databases/Data Mining/Information Retrieval EDBT 03/28/23 Databases/Data Mining/Information Retrieval ICDE 04/03/23 Databases/Data Mining/Information Retrieval RECOMB 04/16/23 Interdisciplinary/Emerging DATE 04/17/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems NSDI 04/17/23 Computer Networks DASFAA 04/17/23 Databases/Data Mining/Information Retrieval ETAPS 04/22/23 Software Engineering/Systems Software/Programming Languages EUROCRYPT 04/23/23 Security CHI 04/23/23 Human Computer Interaction and Ubiquitous Computing SDM 04/27/23 Databases/Data Mining/Information Retrieval WWW 04/30/23 Interdisciplinary/Emerging I3D 05/03/23 Computer Graphics/Multimedia PKC 05/07/23 Security EG 05/08/23 Computer Graphics/Multimedia RTAS 05/09/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems IPSN 05/09/23 Computer Networks HSCC 05/09/23 Theoretical Computer Science EuroSys 05/09/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ICSE 05/14/23 Software Engineering/Systems Software/Programming Languages IPDPS 05/15/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ICPC 05/15/23 Software Engineering/Systems Software/Programming Languages INFOCOM 05/17/23 Computer Networks MSST 05/22/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems S&amp;P 05/22/23 Security ICRA 05/29/23 Artificial Intelligence AAMAS 05/29/23 Artificial Intelligence ICASSP 06/04/23 Computer Graphics/Multimedia ECSCW 06/05/23 Human Computer Interaction and Ubiquitous Computing NOSSDAV 06/10/23 Computer Networks CAiSE 06/12/23 Software Engineering/Systems Software/Programming Languages SoCG 06/12/23 Theoretical Computer Science ICMR 06/12/23 Computer Graphics/Multimedia EuroVis 06/12/23 Computer Graphics/Multimedia SPAA 06/16/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ISCA 06/17/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems PLDI 06/17/23 Software Engineering/Systems Software/Programming Languages LCTES 06/17/23 Software Engineering/Systems Software/Programming Languages MobiSys 06/18/23 Computer Networks SIGMOD 06/18/23 Databases/Data Mining/Information Retrieval PODS 06/18/23 Databases/Data Mining/Information Retrieval CVPR 06/18/23 Artificial Intelligence SIGMETRICS 06/19/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems IWQoS 06/19/23 Computer Networks PODC 06/19/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems HPDC 06/20/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems STOC 06/20/23 Theoretical Computer Science ICS 06/21/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems HotOS 06/22/23 Software Engineering/Systems Software/Programming Languages LICS 06/26/23 Theoretical Computer Science DSN 06/27/23 Security EGSR 06/28/23 Computer Graphics/Multimedia CADE/IJCAR 07/01/23 Theoretical Computer Science ICWS 07/02/23 Software Engineering/Systems Software/Programming Languages SGP 07/03/23 Computer Graphics/Multimedia SAT 07/04/23 Theoretical Computer Science SPM 07/05/23 Computer Graphics/Multimedia ICAPS 07/08/23 Artificial Intelligence DAC 07/09/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CSFW 07/09/23 Security ACL 07/09/23 Artificial Intelligence USENIX ATC 07/10/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems OSDI 07/10/23 Software Engineering/Systems Software/Programming Languages ICALP 07/10/23 Theoretical Computer Science ICME 07/10/23 Computer Graphics/Multimedia COLT 07/12/23 Artificial Intelligence ISSTA 07/17/23 Software Engineering/Systems Software/Programming Languages ECOOP 07/17/23 Software Engineering/Systems Software/Programming Languages CAV 07/17/23 Theoretical Computer Science CCC 07/17/23 Theoretical Computer Science ICCBR 07/17/23 Artificial Intelligence ICDCS 07/18/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems SIGIR 07/23/23 Databases/Data Mining/Information Retrieval ICML 07/23/23 Artificial Intelligence ISMB 07/23/23 Interdisciplinary/Emerging CogSci 07/26/23 Interdisciplinary/Emerging UAI 07/31/23 Artificial Intelligence SCA 08/04/23 Computer Graphics/Multimedia SIGKDD 08/06/23 Databases/Data Mining/Information Retrieval SIGGRAPH 08/06/23 Computer Graphics/Multimedia ICPP 08/07/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems USENIX Security 08/09/23 Security CRYPTO 08/19/23 Security IJCAI 08/19/23 Artificial Intelligence HOT CHIPS 08/27/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CP 08/27/23 Software Engineering/Systems Software/Programming Languages VLDB 08/28/23 Databases/Data Mining/Information Retrieval KR 09/02/23 Artificial Intelligence RE 09/04/23 Software Engineering/Systems Software/Programming Languages ICFP 09/04/23 Software Engineering/Systems Software/Programming Languages ESA 09/04/23 Theoretical Computer Science SIGCOMM 09/10/23 Computer Networks CHES 09/10/23 Security SECON 09/11/23 Computer Networks ASE 09/11/23 Software Engineering/Systems Software/Programming Languages CODES+ISSS 09/17/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems EMSOFT 09/17/23 Interdisciplinary/Emerging ECML-PKDD 09/18/23 Databases/Data Mining/Information Retrieval CONCUR 09/19/23 Theoretical Computer Science ESORICS 09/25/23 Security SRDS 09/25/23 Security MobileHCI 09/26/23 Human Computer Interaction and Ubiquitous Computing ECAI 09/30/23 Artificial Intelligence MoDELS 10/01/23 Software Engineering/Systems Software/Programming Languages ICSME 10/01/23 Software Engineering/Systems Software/Programming Languages MobiCom 10/02/23 Computer Networks ICCV 10/02/23 Artificial Intelligence ECCV 10/02/23 Artificial Intelligence ITC 10/08/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems UbiComp 10/08/23 Human Computer Interaction and Ubiquitous Computing ESEM 10/09/23 Software Engineering/Systems Software/Programming Languages ISSRE 10/09/23 Software Engineering/Systems Software/Programming Languages ICNP 10/10/23 Computer Networks PG 10/10/23 Computer Graphics/Multimedia CSCW 10/14/23 Human Computer Interaction and Ubiquitous Computing RAID 10/16/23 Security ISMAR 10/16/23 Computer Graphics/Multimedia PACT 10/21/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CIKM 10/21/23 Databases/Data Mining/Information Retrieval OOPSLA 10/22/23 Software Engineering/Systems Software/Programming Languages SAS 10/22/23 Software Engineering/Systems Software/Programming Languages IEEE VIS 10/22/23 Computer Graphics/Multimedia MobiHoc 10/23/23 Computer Networks SOSP 10/23/23 Software Engineering/Systems Software/Programming Languages IMC 10/24/23 Computer Networks MICRO 10/28/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ICCAD 10/29/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ACM MM 10/29/23 Computer Graphics/Multimedia UIST 10/29/23 Human Computer Interaction and Ubiquitous Computing SoCC 10/30/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CLUSTER 10/31/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ISS 11/05/23 Human Computer Interaction and Ubiquitous Computing ICCD 11/06/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems ISWC 11/06/23 Databases/Data Mining/Information Retrieval FOCS 11/06/23 Theoretical Computer Science SC 11/12/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems SenSys 11/12/23 Computer Networks Performance 11/14/23 Computer Architecture/Parallel and Distributed Computing/Storage Systems CCS 11/26/23 Security ICSOC 11/28/23 Software Engineering/Systems Software/Programming Languages TCC 11/29/23 Security ICDM 12/01/23 Databases/Data Mining/Information Retrieval FSE/ESEC 12/03/23 Software Engineering/Systems Software/Programming Languages ACSAC 12/04/23 Security ASIACRYPT 12/04/23 Security CoNEXT 12/05/23 Computer Networks RTSS 12/05/23 Interdisciplinary/Emerging BIBM 12/05/23 Interdisciplinary/Emerging EMNLP 12/06/23 Artificial Intelligence NeurIPS 12/10/23 Artificial Intelligence Middleware 12/11/23 Software Engineering/Systems Software/Programming Languages","categories":[{"name":"Reference","slug":"Reference","permalink":"https://abbaswu.github.io/categories/Reference/"}],"tags":[]},{"title":"ISSTA/ECOOP 2023 Observations and Gained Insights","slug":"ISSTA-ECOOP-2023-Observations-and-Gained-Insights","date":"2023-07-22T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/07/22/ISSTA-ECOOP-2023-Observations-and-Gained-Insights/","link":"","permalink":"https://abbaswu.github.io/2023/07/22/ISSTA-ECOOP-2023-Observations-and-Gained-Insights/","excerpt":"","text":"Mon 17 Jul Session 1 FUZZING at Amazon Auditorium (Gates G20) Welcome and Introductions The following reviewing criteria for workshop papers can serve as a guide for us in writing papers: Is the problem that is addressed significant for research or practice? Are the contributions (technique, hypothesis, or evaluation) over existing work sufficient? Is the methodology (experimental setup or protocol) specified to validate the claims or hypotheses reasonable? Can an independent research group reproduce the results, given the proposed methodology (experimental setup) Establish significance, novelty, and soundness, even if results do not show a large performance gain. Inspect unexpected results, such as why results are negative. Three Colours of Fuzzing: Reflections and Open Challenges - Cristian Cadar Why does fuzzing keep finding bugs in production software? LOTS of code is added or modified without being tested. (Covrig: A framework for the analysis of code, test, and coverage evolution in real software) Fuzzing is not automated enough. Fuzz targets (test drivers) need to be manually specified. There is much work on improving fuzzing heuristics, but more work is required for test driver generation. An ideal test case should benefit quality assurance, debugging aid, and documentation. They should target human users, and be small, fast, readable, and well-documented. However, automatically generated test suites, such as those generated by fuzzers, need to be improved in these aspects. They achieve high code coverage, excel at finding generic/crash bugs in general software that may not be very realistic (assertion faults, crashes, undefined behavior) but do not achieve high feature coverage and are poor at detecting logical bugs in software for specific domains. On the other hand, such fuzzing makes it appropriate for use cases outside of security and software testing that require a novel search to find diverse failing inputs, corner cases, and loopholes, such as ML models and even investigating legal documents (Rohan). Developers tend to be afraid of using fuzzers as they don't understand them or think of them as security tools, in contrast to a standard testing tool. Allowing fuzzing to operate at a higher declarative level and combining fuzzing with domain-specific specification languages would be beneficial. Sound fuzzer evaluation is challenging. Well-designed experiment methodology. Huge variance due to randomness, demanding substantial computation resources (e.g., repeat 20x, 24 hours, X fuzzers, Y programs) Thu 20 Jul Keynotes at Amazon Auditorium (Gates G20) Paper Readinging Statistics 44/97 papers accepted Round 1: 40 submitted, 17 accepted, 9 rejected, 14 resubmit Round 2: 57 submitted (11 resubmissions), 27 accepted, 18 rejected, 12 resubmit Dahl-Nygaard Senior Prize: Safe Journeys into the Unknown - Object Capabilities - Sophia Drossopoulou Think of an exciting question, such as various language features, and look into it as a research question (An Abstract Model of Java Dynamic Linking and Loading, A Flexible Model for Dynamic Linking in Java and C#). The key for program verification is to develop formal models for a (subset) of a language, make it small and simple, and gradually expand (Java is type safe -- probably). Actively start collaborations (Ownership, encapsulation and the disjointness of type and effect). ISSTA 10: Test OptimizationsISSTA Technical Papers at Smith Classroom (Gates G10) June: A Type Testability Transformation for Improved ATG Performance Automatically generating unit tests is a powerful approach to exercising complex software. However, existing methods frequently fail to deliver appropriate input values, like strings, capable of bypassing domain-specific sanity checks. For instance, Randoop commonly uses \"hi!\" as a value. (Saying 'Hi!' is not enough: Mining inputs for effective test generation) Pattern-Based Peephole Optimizations with Java JIT Tests To demonstrate the advantage of JOG over hand-written peephole optimizations in terms of ease of writing, existing hand-written peephole optimizations are compared, and number of characters and number of lines are used as metrics. GPUHarbor: Testing GPU Memory Consistency at Large (Experience Paper) The tool has been implemented as a Web app using WebGPU to access the GPU, allowing the audience to try it out during the talk. Keynote - ISSTA'24 Preview - ClosingKeynotes at Amazon Auditorium (Gates G20) Machine Learning for Software Engineering What underlies the success of machine learning for software engineering? The naturalness of code. i++ is predictable given for (i = 0; i &lt; 10;, and backward() is predictable given loss. The bimodality of code, or code contains natural language. Q. How do I get a platform-dependent new line character? A. public static String getPlatformLineSeparator() { return System.getProperty(\"line.separator\"); } Code has predictable properties. Given ... = x.weight * x.height, what is the ??? in ... = y.weight * ???? Large amount of data (GitHub repos with code, version history, and commit logs, StackOverflow questions and answers, internal corpora in companies, etc.)","categories":[{"name":"Conferences","slug":"Conferences","permalink":"https://abbaswu.github.io/categories/Conferences/"}],"tags":[]},{"title":"Syncing a Local Directory With a Remote Directory via rsync","slug":"Syncing-a-Local-Directory-With-a-Remote-Directory-via-rsync","date":"2023-07-11T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/07/11/Syncing-a-Local-Directory-With-a-Remote-Directory-via-rsync/","link":"","permalink":"https://abbaswu.github.io/2023/07/11/Syncing-a-Local-Directory-With-a-Remote-Directory-via-rsync/","excerpt":"","text":"Syncing a Local Directory With a Remote Directory via rsync rsync is a powerful utility for efficiently synchronizing directories between a local computer and a remote server. It achieves this synchronization by comparing file modification times and sizes, utilizing delta encoding, and optionally employing data compression to minimize network usage. To successfully synchronize directories between two systems using rsync, you need to have rsync installed on both the local and remote machines. Additionally, the remote machine should be accessible from the local machine via SSH, enabling the local machine to invoke the remote machine's rsync and determine which parts of local files need to be transferred. The rsync command-line syntax is similar to that of cp and scp. The following command-line options are commonly used: -e: Specifies the command to establish an SSH connection before the username@domain section. This option allows you to customize SSH behavior, such as using specific ports or providing a PEM file (e.g., ssh -i SSH-key.pem). -r (recursive): Used for syncing directories, similar to cp and scp. -v (verbose): Lists files being transferred during synchronization. -z: Enables additional data compression for improved network usage. When specifying the source directory and the destination directory, keep the following points in mind: A local directory can be represented using either a relative or absolute path. A remote directory is represented using the username@domain:&lt;absolute path on the remote machine&gt; notation, similar to scp. A source directory must end with a trailing slash. A destination directory must not end with a trailing slash. Here's an example that illustrates how to sync the local directory TypeWriter_dataset to the remote directory /home/ubuntu/TypeWriter_dataset with additional data compression. This assumes connecting to ubuntu@104.171.203.254 via the command ssh -i SSH-key.pem: 12345rsync \\-e &#x27;ssh -i SSH-key.pem&#x27; \\-r -v -z \\TypeWriter_dataset/ \\ubuntu@104.171.203.254:/home/ubuntu/TypeWriter_dataset Conversely, to sync the remote directory /home/ubuntu/TypeWriter_dataset to the local directory TypeWriter_dataset: 12345rsync \\-e &#x27;ssh -i SSH-key.pem&#x27; \\-r -v -z \\ubuntu@104.171.203.254:/home/ubuntu/TypeWriter_dataset/ \\TypeWriter_dataset References: https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories https://en.wikipedia.org/wiki/Rsync","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"}],"tags":[]},{"title":"Task Parallelism and Data Parallelism Thread Pools","slug":"Task-Parallelism-and-Data-Parallelism-Thread-Pools","date":"2023-07-11T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/07/11/Task-Parallelism-and-Data-Parallelism-Thread-Pools/","link":"","permalink":"https://abbaswu.github.io/2023/07/11/Task-Parallelism-and-Data-Parallelism-Thread-Pools/","excerpt":"","text":"Task Parallelism and Data Parallelism Thread Pools Parallel computing environments often involve distributing code across multiple processors for efficient execution. Two common paradigms of parallelization are task parallelism and data parallelism. Task parallelism focuses on distributing encapsulated tasks that can execute the same or different code on the same or different data across different processors. On the other hand, data parallelism involves performing the same operations on different subsets of the same data on multiple processors. Both task parallelism and data parallelism can be implemented using thread pools. This article explores simple implementations of thread pools for task parallelism and data parallelism scenarios. Task Parallelism Thread Pool The following code demonstrates a simple thread pool for task parallelism. It utilizes a task queue to distribute tasks, which are represented as Callable[[], None] objects (callables accepting no parameters and returning None) across multiple threads. Each thread continuously fetches a task from the task queue and executes it. If an exception occurs during task execution, a traceback is printed, and a new task is obtained from the task queue. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import tracebackfrom collections.abc import Callable, Iterable, Generatorfrom queue import Queuefrom threading import Threadfrom typing import Anyclass TaskParallelismThreadPoolThread(Thread): def __init__(self, task_queue: Queue[Callable[[], None] | None]): super().__init__() self.task_queue: Queue[Callable[[], None] | None] = task_queue def run(self) -&gt; None: while True: task: Callable[[], None] | None = self.task_queue.get() if task is None: break try: task() except: traceback.print_exc()def run_simple_task_parallelism_thread_pool(tasks: Iterable[Callable[[], None]], num_threads: int) -&gt; None: # Create task queue. task_queue: Queue[Callable[[], None] | None] = Queue() # Create all threads which share a task queue. threads: list[Thread] = [] for _ in range(num_threads): thread: Thread = TaskParallelismThreadPoolThread(task_queue) thread.start() threads.append(thread) # Enqueue all tasks. for task in tasks: task_queue.put(task) # Enqueue sentinel values for all threads to stop once all tasks are finished. for _ in range(num_threads): task_queue.put(None) # Wait for all threads to stop. for thread in threads: thread.join() To use this task parallelism thread pool, provide it with a collection of tasks (represented as Callable[[], None] objects) and the desired number of threads. The tasks will be executed in parallel by the thread pool until all tasks have finished. Data Parallelism Thread Pool The following code showcases a simple thread pool for data parallelism. It distributes data as argument tuples across multiple threads using an argument tuple queue. Each thread is assigned an operation created using an operation factory. After executing an operation on an argument tuple, the resulting return value is passed to a return value callback. As with the task parallelism thread pool, all threads in the thread pool are always busy by getting a new argument tuple whenever its operation finishes execution on a previous argument tuple. Should an exception be raised when executing an operation on an argument tuple, a traceback is printed, and a new argument tuple is taken from the shared argument tuple queue. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import tracebackimport multiprocessingfrom collections.abc import Callable, Iterable, Generatorfrom queue import Queuefrom threading import Threadfrom typing import Anyclass DataParallelismThreadPoolThread(Thread): def __init__(self, operation: Callable[[...], Any], argument_tuple_queue: Queue[tuple[Any, ...] | None], return_value_callback: Callable[[Any], None]): super().__init__() self.operation: Callable[[...], Any] = operation self.argument_tuple_queue: Queue[tuple[Any, ...] | None] = argument_tuple_queue self.return_value_callback: Callable[[Any], None] = return_value_callback def run(self) -&gt; None: while True: argument_tuple: tuple[Any, ...] | None = self.argument_tuple_queue.get() if argument_tuple is None: break try: self.return_value_callback(self.operation(*argument_tuple)) except: traceback.print_exc()def run_simple_data_parallelism_thread_pool( operation_factory: Callable[[], Callable[[...], Any]], argument_tuples: Iterable[tuple[Any, ...]], return_value_callback: Callable[[Any], None] = lambda return_value: None, num_threads: int = multiprocessing.cpu_count()) -&gt; None: # Create argument tuple queue. argument_tuple_queue: Queue[tuple[Any, ...] | None] = Queue() # Create all threads which share an argument tuple queue. threads: list[Thread] = [] for _ in range(num_threads): thread: Thread = DataParallelismThreadPoolThread(operation_factory(), argument_tuple_queue, return_value_callback) thread.start() threads.append(thread) # Enqueue all argument tuples. for argument_tuple in argument_tuples: argument_tuple_queue.put(argument_tuple) # Enqueue sentinel values for all threads to stop once execution on all argument tuples have finished. for _ in range(num_threads): argument_tuple_queue.put(None) # Wait for all threads to stop. for thread in threads: thread.join() To utilize the data parallelism thread pool, provide an operation factory, a collection of argument tuples, and a return value callback. The operation factory, when called, creates operations represented as Callable[[...], Any] objects, accepting arguments from an argument tuple and returning a value. Each thread in the thread pool will execute these operations on the provided argument tuples. Any return values will be passed to the return value callback, which can be customized according to your needs. The data parallelism thread pool will process the argument tuples in parallel until all tuples have been processed. Example Say that we want to sleep for \\(0, 1, 2, \\dots, N - 1\\) seconds before printing the number of seconds slept in parallel, where \\(N\\) is the number of threads in our thread pool. We can adopt a task parallelism approach, where we create tasks which encapsulate how long they sleep, and add them to a task parallelism thread pool: 1234567891011121314151617181920import timeimport randomN = 8def create_task(sleep_time: int) -&gt; Callable[[], None]: def task(): nonlocal sleep_time time.sleep(sleep_time) print(f&#x27;Slept for &#123;sleep_time&#125;&#x27;) return taskrun_simple_task_parallelism_thread_pool( (create_task(i) for i in range(N)), N) As an alternative, we can also use a data parallelism approach, in which operations accept the number of seconds they sleep from argument tuples, sleep for those time durations, and return those time durations before return value callbacks operate on the return values and print those time durations: 12345678910111213141516171819202122232425262728import timeimport randomfrom collections.abc import CallableN = 8def operation(sleep_time: int) -&gt; int: time.sleep(sleep_time) return sleep_timedef operation_factory() -&gt; Callable[[int], int]: return operationdef return_value_callback(sleep_time: int) -&gt; None: print(f&#x27;Slept for &#123;sleep_time&#125;&#x27;)run_simple_data_parallelism_thread_pool( operation_factory, ((i,) for i in range(N)), return_value_callback, N) Running both thread pools takes the same time and produces the same output. References https://en.m.wikipedia.org/wiki/Task_parallelism https://en.wikipedia.org/wiki/Data_parallelism","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"}],"tags":[]},{"title":"Batch Killing Processes Looked up Through ps -aux | grep <process_name>","slug":"Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name","date":"2023-06-10T07:00:00.000Z","updated":"2023-11-06T07:31:36.161Z","comments":true,"path":"2023/06/10/Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name/","link":"","permalink":"https://abbaswu.github.io/2023/06/10/Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name/","excerpt":"","text":"Sometimes we acidentally spawn a series of processes, and we want to kill them. We can look up their pid's through ps -aux | grep &lt;process_name&gt; (as shown below) and manually run the kill command to kill each process by providing its pid, but how can we automate this tedious task? 12345678910$ ps aux | grep pipreqsjifengwu 58180 0.0 0.1 46440 27332 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/spectria/tildes/tildes --mode no-pinjifengwu 58205 0.1 0.1 48140 29392 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/PnX-SI/GeoNature --mode no-pinjifengwu 58224 5.7 0.2 51856 33108 pts/0 T 13:38 0:23 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/fabiandevia/home --mode no-pinjifengwu 58267 4.4 0.2 57880 38204 pts/0 T 13:39 0:17 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/377312117/gitproject --mode no-pinjifengwu 58272 2.3 0.2 53756 34252 pts/0 T 13:39 0:08 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/crazyfish1111/home --mode no-pinjifengwu 58282 0.1 0.1 47840 28132 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/Piratenpartei/ekklesia-portal --mode no-pinjifengwu 58295 0.1 0.1 48220 28492 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/jauhararifin/ugrade/server --mode no-pinjifengwu 58659 0.3 0.1 48608 29324 pts/0 T 13:41 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/klen/pylama --mode no-pinjifengwu 59564 0.0 0.0 19612 2516 pts/2 S+ 13:45 0:00 grep --color=auto pipreqs First, we can add grep -v grep to the pipe to hide the grep processes from the output: 123456789$ ps aux | grep pipreqs | grep -v grepjifengwu 58180 0.0 0.1 46440 27332 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/spectria/tildes/tildes --mode no-pinjifengwu 58205 0.1 0.1 48140 29392 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/PnX-SI/GeoNature --mode no-pinjifengwu 58224 5.6 0.2 51856 33108 pts/0 T 13:38 0:23 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/fabiandevia/home --mode no-pinjifengwu 58267 4.4 0.2 57880 38204 pts/0 T 13:39 0:17 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/377312117/gitproject --mode no-pinjifengwu 58272 2.2 0.2 53756 34252 pts/0 T 13:39 0:08 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/crazyfish1111/home --mode no-pinjifengwu 58282 0.1 0.1 47840 28132 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/Piratenpartei/ekklesia-portal --mode no-pinjifengwu 58295 0.1 0.1 48220 28492 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/jauhararifin/ugrade/server --mode no-pinjifengwu 58659 0.3 0.1 48608 29324 pts/0 T 13:41 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/klen/pylama --mode no-pin Then, we can add awk '&#123;print $2&#125;' to the pipe to invoke awk to trim the second space-delimited component (which in this case is the pid). Now we have a list of the pid's of the processes we want to kill: 123456789$ ps aux | grep pipreqs | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;5818058205582245826758272582825829558659 Finally, we can iterate over the pid's in a for-loop to kill them. 1234$ for pid in $(ps aux | grep pipreqs | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;)&gt; do&gt; kill -15 $pid&gt; done References https://www.baeldung.com/linux/grep-exclude-ps-results https://stackoverflow.com/questions/46008880/how-to-always-cut-the-pid-from-ps-aux-command","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Unix","slug":"Code/Unix","permalink":"https://abbaswu.github.io/categories/Code/Unix/"}],"tags":[]},{"title":"Parsing Command-line Options in Shell Scripts Using `getopts`","slug":"Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts","date":"2023-06-09T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/06/09/Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts/","link":"","permalink":"https://abbaswu.github.io/2023/06/09/Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts/","excerpt":"","text":"What is getopts getopts is a built-in Unix shell command for parsing command-line options. It is a wrapper around getopt, a POSIX C library function used to parse command-line options of the Unix/POSIX style. Specifically: Options are single-character alphanumerics preceded by a - (hyphen-minus) character, i.e. -a. -b, -c. Options can take an argument or none. Multiple options can be chained together, as long as the non-last ones are not argument-taking. If -a and -b take no arguments while -c takes an argument, -abc foo is the same as -a -c -e foo, but -bca is not the same as -b -c a due to the preceding rule. When an option takes an argument, this can be in the same token or in the next one. In other words, if -c takes an argument, -cfoo is the same as -c foo. optstring's Both getopt and getopts specifies specify options using a optstring. Specifically: Begin an optstring with :. To specify an option that does not take an argument, append its name to the optstring. To specify an option that takes an argument, append its name and : to the optstring. For example, the optstring that specifies two options -a, -b that do not take arguments and two options -c, -d that take arguments is :abc:d:. Using getopts in a Shell Script In Shell scripts, getopts invoked with an optstring is used with a while-loop to parse command-line options. Say that our Shell script test_getopts.sh accepts two options -a, -b that do not take arguments and two options -c, -d that take arguments. Our Shell script can look like this: 12345678910111213141516171819202122232425#!/bin/shwhile getopts &#x27;:abc:d:&#x27; namedo case $name in a) echo &quot;You provided option -a&quot; ;; b) echo &quot;You provided option -b&quot; ;; c) echo &quot;You provided option -c with argument $OPTARG&quot; ;; d) echo &quot;You provided option -d with argument $OPTARG&quot; ;; :) echo &quot;Option -$OPTARG requires an argument&quot; ;; ?) echo &quot;You provided an invalid option -$OPTARG&quot; ;; esacdone Here, getopts is invoked with the optstring for specifying our options, :abc:d:. In each iteration of the while-loop, the next option is parsed and the Shell variables name and OPTARG are set to different values based on different conditions we may encounter. If a valid option is detected and that option does not take an argument, the Shell variable name is set to the name of the option. If a valid option is detected and that option takes an argument: If we have provided an argument, the Shell variable name is set to the name of the option, and the Shell variable OPTARG is set to the value of the argument. If we haven't provided an argument, the Shell variable name is set to :, and the Shell variable OPTARG is set to the name of the argument. If an invalid option is detected, the Shell variable name is set to ?, and the Shell variable OPTARG is set to the name of the argument. We can see getopts at work by providing different command-line options when invoking our Shell script. Providing no command-line options: 1$ sh test_getopts.sh Providing option -a that do not take arguments: 12$ sh test_getopts.sh -aYou provided option -a Providing option -a that do not take arguments twice: 123456$ sh test_getopts.sh -a -aYou provided option -aYou provided option -a$ sh test_getopts.sh -aaYou provided option -aYou provided option -a Providing option -c that takes an argument with an argument foo: 12$ sh test_getopts.sh -c fooYou provided option -c with argument foo Providing option -c that takes an argument with an argument foo twice: 123$ sh test_getopts.sh -c foo -c barYou provided option -c with argument fooYou provided option -c with argument bar Providing option -c that takes an argument without an argument: 12$ sh test_getopts.sh -cOption -c requires an argument Providing an invalid argument -e: 12$ sh test_getopts.sh -eYou provided an invalid option -e References https://en.wikipedia.org/wiki/Getopts https://pubs.opengroup.org/onlinepubs/9699919799/utilities/getopts.html https://en.wikipedia.org/wiki/Getopt https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap12.html","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Unix","slug":"Code/Unix","permalink":"https://abbaswu.github.io/categories/Code/Unix/"}],"tags":[]},{"title":"PNW PLSE Workshop 2023 Observations and Gained Insights","slug":"PNW-PLSE-Workshop-2023-Observations-and-Gained-Insights","date":"2023-05-10T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/05/10/PNW-PLSE-Workshop-2023-Observations-and-Gained-Insights/","link":"","permalink":"https://abbaswu.github.io/2023/05/10/PNW-PLSE-Workshop-2023-Observations-and-Gained-Insights/","excerpt":"","text":"10:30 - Talks Linear Types for Systems Verification It is a good idea to embed verification information in type system of a programming language. Verified Program Construction Program verification is hard per se. Proofs are brittle. Existing techniques provide poor support for commonly-used datatypes such as vectors, sets, and maps. Partial verification is important due to the complexity of programs. Program verification for general-purpose programming languages and frameworks for general-purpose programming languages is tedious compared to focusing on a DSL. The direction for program verification should be verified program construction. Be aware of the pain point you are trying to solve and the day-to-day engineering reality in real-world software development. 13:00 - Keynote: Patrick Lam Hot Takes on Machine Learning for Program Analysis A crucial step in Machine Learning for Program Analysis is deciding what things could be used as features based on experience. Generative AI can replace junior developers doing raw coding instead of contextual work. 13:30 - Lightning Talks Lakeroad: Hardware Compilation via Program Synthesis If you don't use DSLs, FPGAs bring crappy performance running programs in general-purpose programming languages. Checked C Retrofitting verification into non-verified languages is an arduous task. 15:15 - Talks An Anti-Capitalist, Multicultural, Accessible Programming Language An event-based language enabling time-traveling to all points in program execution history would greatly benefit debugging.","categories":[{"name":"Conferences","slug":"Conferences","permalink":"https://abbaswu.github.io/categories/Conferences/"}],"tags":[]},{"title":"Paper Reading: Sized Types","slug":"Paper-Reading-Sized-Types","date":"2023-03-25T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/03/25/Paper-Reading-Sized-Types/","link":"","permalink":"https://abbaswu.github.io/2023/03/25/Paper-Reading-Sized-Types/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Type Systems. The original paper can be found here. Summary You can check the presentation that I made for this paper in this GitHub repository. Critique Although I liked the idea of Sized Types proposed in the motivation, this paper was difficult for me to grasp, and after spending days reading it, there are still sections which I am confused about. I have summarized my understanding of this paper in the uploaded PDF, and I will discuss my thoughts here. I really like the idea of Sized Types that they can be used to prove data computations terminate and codata computations are productive using the same formalization. Apparently, the requirement that size indexes in Sized Types be natural number size variables, the special index \\(\\omega\\), or linear functions of the size variables facilitates generating constraints in the type checking algorithm that can be solved by a constraint solver (e.g. an SMT solver). Although this may lead to overapproximation in certain scenarions (for example, representing the type of the factorial function), over all, I consider it to be a good balance point between expressiveness and usability. 3.2 Semantics of Expressions, 3.3 The Universe of Types, 3.4 Continuity and Ordinals, 3.5 Semantics of Types, and 3.7 \\(\\omega\\)-Types used a lot of concepts before properly introducing them, and I couldn't understand this part. The example presented to demonstrate the type checking algorithm involves generating constraints. However, only the generated constraints are presented, while how the constraints are generated and what each symbol in the constraints stand for with regards to the aforementioned AST nodes is unknown.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: Refinement Types","slug":"Paper-Reading-Refinement-Types","date":"2023-03-19T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/03/19/Paper-Reading-Refinement-Types/","link":"","permalink":"https://abbaswu.github.io/2023/03/19/Paper-Reading-Refinement-Types/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Type Systems. The original paper can be found here. Summary This paper presents a clear and organized guide to refinement type systems by condensing the extensive literature on the topic and demonstrating the implementation of a refinement type checker. It first states the motivation for requirement types, a history of requirement types, and refinement logic, which is a logic system used in the proposed refinement type checker. The rest of the paper shows the implementation of a refinement type checker through a series of programming languages, beginning with simply-typed lambda calculus and incrementally adding additional features. This approach is influenced by the nanopass framework, which is used to teach compilation. Critique Honestly, I have found the section on implementing a refinement type checker through a series of programming languages challenging to understand. Still, I have understood much of the paper before that. Therefore, I will summarize my gained insights and state questions that I have in mind. Insights Refinement Types as Subtypes Type systems are the most commonly employed technique for ensuring the correct behavior of software. However, even well-typed programs can contain various bugs, such as buffer overflows, divisions by zero, logic bugs, and out-of-bounds array accesses. One approach to address this issue is to enhance a language's types with subtypes that limit the range of valid values with predicates, such as 'non-negative integer' from 'integer.' These subtypes are known as 'refinement types.' They enable developers to write precise contracts for valid inputs and outputs of functions and specify the correctness properties. This brings formal verification into mainstream software development. Refinement Logic and How it Maps to SMT Expressions I was partically impressed by refinement logic, the logic system used in the proposed refinement type checker, as it is both expressive and easy to be verified using an SMT solver. Refinement logic consists of two parts: predicates and constraints. Predicates are drawn from the quantifier-free fragment of linear arithmetic and uninterpreted functions (commonly used in SMT solvers), and may include boolean and integer literals, boolean and integer variables, arithmetic operators, boolean operators, comparisons, the 'if-then-else' expression, and uninterpreted functions (resembling those in z3). Predicates are the building block of constraints, which are generated from refinement type checking. A constraint is either a predicate, an implication \\(\\forall t: T \\: p \\Rightarrow c\\) which states that for each term \\(t\\) of type \\(T\\), if the predicate \\(p\\) holds then another constraint \\(c\\) must be true, or a conjunction of two other constraints. Constraints can be verified by checking whether there is no satisfying assignment for the negated constraint. In this process, they can be converted into SMT expressions in a straightforward way. For example, the constraint presented in the paper \\[c = \\forall x: array \\: 0 \\le length(x) \\Rightarrow \\forall n: int \\: n = length(x) \\Rightarrow \\forall i: int \\: i = n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x)\\] can be negated as follows: \\[\\neg c\\] \\[\\neg (\\forall x: array \\: 0 \\le length(x) \\Rightarrow \\forall n: int \\: n = length(x) \\Rightarrow \\forall i: int \\: i = n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))\\] \\[\\exists x: array \\: 0 \\le length(x) \\land \\neg (\\forall n: int \\: n = length(x) \\Rightarrow \\forall i: int \\: i = n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))\\] \\[\\exists x: array \\: 0 \\le length(x) \\land \\exists n: int \\: n = length(x) \\land \\neg (\\forall i: int \\: i = n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))\\] \\[\\exists x: array \\: 0 \\le length(x) \\land \\exists n: int \\: n = length(x) \\land \\exists i: int \\: i = n - 1 \\land \\neg (0 \\le i \\land i &lt; length(x))\\] \\[\\exists x: array \\: 0 \\le length(x) \\land \\exists n: int \\: n = length(x) \\land \\exists i: int \\: i = n - 1 \\land (0 &gt; i \\lor i \\ge length(x))\\] We can verify the negated constraint using an SMT solver: 123456789101112131415161718192021222324252627In [1]: import z3In [2]: L = z3.Int(&#x27;L&#x27;)In [3]: n = z3.Int(&#x27;n&#x27;)In [4]: i = z3.Int(&#x27;i&#x27;)In [5]: solver = z3.Solver()In [6]: solver.add(0 &lt;= L)In [7]: solver.add(n == L)In [8]: solver.add(i == n - 1)In [9]: solver.add(z3.Or(i &lt; 0, i &gt; L))In [10]: check_sat_result = solver.check()In [11]: check_sat_resultOut[11]: satIn [12]: model_ref = solver.model()In [13]: model_refOut[13]: [i = -1, L = 0, n = 0] Note that check_sat_result is sat and we can find a satisfying assignment for \\(\\neg c\\): \\(i = -1, length(x) = 0, n = 0\\). This means that the original constraint \\(c\\) is invalid. Questions Although the concept of refinement types is neat, what is the burden on programmers of writing refinement types that describe legal inputs and outputs of functions? This is a critical aspect to determine whether refinement types can bring formal verification into mainstream software development. Furthermore, constraints in the proposed refinement logic generated by the refinement type checker can be negated and converted into SMT expressions. However, what is the feasibility of doing such checking for large-scale programs? Would it become unscalable?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Rapidly Prototyping Presentations Using Markdown with Marp","slug":"Rapidly-Prototyping-Presentations-Using-Markdown-with-Marp","date":"2023-03-17T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/03/17/Rapidly-Prototyping-Presentations-Using-Markdown-with-Marp/","link":"","permalink":"https://abbaswu.github.io/2023/03/17/Rapidly-Prototyping-Presentations-Using-Markdown-with-Marp/","excerpt":"","text":"In the need to create a decent, academic presentation fast? LaTeX overly verbose? No time to spend on adjusting style? Have notes written in Markdown? The solution: create presentations with Markdown using Marp! Source code and compiled PDF of the presentation for \"Rapidly Prototyping Presentations Using Markdown with Marp\" presented at the SPL Workshop 2023W2 is available in this GitHub repository.","categories":[{"name":"Talks","slug":"Talks","permalink":"https://abbaswu.github.io/categories/Talks/"}],"tags":[]},{"title":"Paper Reading: How to make ad-hoc polymorphism less ad-hoc","slug":"Paper-Reading-How-to-make-ad-hoc-polymorphism-less-ad-hoc","date":"2023-03-06T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/03/06/Paper-Reading-How-to-make-ad-hoc-polymorphism-less-ad-hoc/","link":"","permalink":"https://abbaswu.github.io/2023/03/06/Paper-Reading-How-to-make-ad-hoc-polymorphism-less-ad-hoc/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Type Systems. The original paper can be found here. Summary The paper first defines and compares parametric and ad-hoc polymorphism and points out the limitations of existing implementations of ad-hoc polymorphism. It then presents type classes that extend the Hindley/Milner type system to support ad-hoc polymorphism as a remedy to these limitations and explains how to translate a program using type classes into an equivalent program without them at compile-time. Furthermore, it showcases the power of type classes and the translation mechanism using the example of a polymorphic equality operation. Finally, it explores subclassing of type classes. Critique The paper is easy to follow as it is written in a lucid manner and gives an informal introduction to type classes and its translation rules. Furthermore, the motivation for type classes and how it connects to object-oriented programming languages is explicitly stated in the paper. I have further looked up some material following these lines. I will summarize them before presenting some questions and comments. My Takeaways Different Types of Polymorphism See my Paper Reading for \"Types and Programming Languages\" Chapter 15 and Chapter 16. Type Classes and Protocols/Interfaces in Smalltalk/Objective-C/Java/C An interface is an abstract type used to provide a collection of methods compliant classes must implement in the Java (and C#) programming languages. Java is mostly influenced by Objective-C, and Java's interfaces are adaptations of the protocols in Objective-C and Smalltalk, which in turn is based on protocols in networking, notably the ARPANet. Although Type Classes and Interfaces do not share a common lineage, it is straightforward to implement Type Classes with Generic Interfaces whose Generic Parameters should be Classes that comply with the Interface. For instance, the Type Class below specifies the equal (==) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented in Java using the following Generic Interface: 123interface Eq&lt;T&gt; &#123; boolean isEqual(T other);&#125; Type Classes and Concepts in C++ Although Java's syntax resembles C++'s, its semantics of late-binding, single inheritance, class objects, and an extensive runtime system are in the lineage of Smalltalk and Objective-C, far away from that of C++'s. However, in C++'s Template Metaprogramming world, Concepts, added in C++20, resembles Type Classes. Template Metaprogramming in C++ had been untyped, with template parameters being generic type variables substituted at template instantiation. In C++20, a type system has been added to this untyped template language through concepts. They are Boolean predicates on template parameters evaluated at the point of, not after, template instantiation. The compiler will produce a clear error immediately if a programmer tries to use a template parameter that doesn't meet the requirements of a concept. This starkly contrasts the challenging-to-grasp errors reported after an invalid type substitutes a generic type variable emanating from the implementation context rather than the template instantiation itself. For instance, the first two arguments to std::sort must be random-access iterators. If an argument is not a random-access iterator, an error will occur when std::sort attempts to use it as a bidirectional iterator. 12std::list&lt;int&gt; l = &#123;2, 1, 3&#125;;std::sort(l.begin(), l.end()); Without concepts, compilers may produce large amounts of error information, starting with an equation that failed to compile when it tried to subtract two non-random-access iterators: 123In instantiation of &#x27;void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = std::_List_iterator&lt;int&gt;; _Compare = __gnu_cxx::__ops::_Iter_less_iter]&#x27;: error: no match for &#x27;operator-&#x27; (operand types are &#x27;std::_List_iterator&lt;int&gt;&#x27; and &#x27;std::_List_iterator&lt;int&gt;&#x27;) std::__lg(__last - __first) * 2, However, if concepts are used, the problem can be found and reported at template instantiation: 12error: cannot call function &#x27;void std::sort(_RAIter, _RAIter) [with _RAIter = std::_List_iterator&lt;int&gt;]&#x27;note: concept &#x27;RandomAccessIterator()&#x27; was not satisfied It is straightforward to implement Type Classes with concepts. For instance, the Type Class below specifies the equal (==) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented using the following C++ concept: 123456789#include &lt;concepts&gt;// Declaration of the concept &quot;Eq&quot;, which is satisfied by any type &#x27;T&#x27;// such that for values &#x27;t&#x27; of type &#x27;T&#x27;, the expression t == t compiles// and its type satisfies the concept std::same_as&lt;bool&gt;template &lt;typename T&gt; concept Eq = requires (T t) &#123; &#123; t == t &#125; -&gt; std::same_as&lt;bool&gt;;&#125;; Afterwards, such a concept can be specified when template parameters are being introduced in a template definition, to indicate that the corresponding template parameter must satisfy the concept. 123template&lt;Eq T&gt; void f(const T&amp; t) &#123; // ...&#125; References https://stackoverflow.com/questions/6948166/javas-interface-and-haskells-type-class-differences-and-similarities https://cs.gmu.edu/~sean/stuff/java-objc.html https://functionalcpp.wordpress.com/2013/08/16/type-classes/ https://stackoverflow.com/questions/32124627/how-are-c-concepts-different-to-haskell-typeclasses https://wiki.haskell.org/OOP_vs_type_classes https://doi.org/10.1145/1411318.1411324 https://www.foonathan.net/2021/07/concepts-structural-nominal/ https://www.reddit.com/r/haskell/comments/1e9f49/concepts_in_c_template_programming_and_type/ Questions and Comments The translation mechanism (pre-processor) proposed in this paper translates a program using type classes into an equivalent program without them at compile-time so that an existing Hindley/Milner type system can be used afterward instead of having to develop a new, complex type system to support type classes. This is indeed a very clever mechanism. Can this be viewed as an example of desugaring?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Type-Theoretic Constructs in C++","slug":"Type-Theoretic-Constructs-in-CXX","date":"2023-03-01T08:00:00.000Z","updated":"2023-12-20T19:54:50.722Z","comments":true,"path":"2023/03/01/Type-Theoretic-Constructs-in-CXX/","link":"","permalink":"https://abbaswu.github.io/2023/03/01/Type-Theoretic-Constructs-in-CXX/","excerpt":"","text":"Fixed-point Combinators, Tying the Recursive Knot, and Recursive Lambda Expressions In Lambda Calculus, we cannot refer to the Lambda Abstraction itself within a Lambda Abstraction. Similarly, C++ does not allow defining a recursive lambda expression. Thus, we cannot straightforwardly implement recursion. A workaround for this is to define a lambda expression that: Add an additional first parameter to the lambda expression. Call that additional first parameter inside the lambda expression at each recursive call site. Such an additional first parameter should have the value of a yet-unknown hypothetical recursive function. Thus, we should use auto to represent its type. Using auto in a lambda expression's parameter list requires C++14 or above. For example, we can define the following lambda expression to calculate the nth Fibonacci Number recursively: 12345678910auto fib = []( auto recursive_fib, unsigned int n) -&gt; unsigned long &#123; if (n == 0) return 0UL; else &#123; if (n == 1) return 1UL; else return recursive_fib(n - 1) + recursive_fib(n - 2); &#125;&#125;; After defining such a lambda expression, we can use a Fixed-point Combinator to tie the recursive knot and return a new recursive function without the additional first parameter. What is a Fixed-Point Combinator? In mathematics, a fixed-point for function \\(f\\) refers to an element \\(x\\) that is mapped to itself by the function, i.e., \\(x = f(x)\\). For example, given \\(f(x)=x^{2}-3x+4\\), then \\(2\\) is a fixed point of \\(f\\), because \\(f(2) = 2\\). A combinator is a function that operates on a function (i.e., a higher-order function). A fixed-point combinator g for function f satisfies g(f)(...) = f(g(f), ...). This is reminiscent of the form \\(x = f(x)\\) for fixed points in mathematics, and g(f) can be seen as a fixed-point of function f. This implies that a fixed-point combinator g, when called on f, returns a new function (the g(f) above), that, when called with parameters ..., is equivalent to directly calling f with both g(f) and .... This means that a fixed-point combinator returns a new recursive function without the additional first parameter of f. This is done by tying the recursive knot of f. Tying the recursive knot refers to, for such a previously defined lambda expression f, passing a function that represents the hypothetical recursive function, which in this case is g(f), to its first parameter. We can implement fixed-point combinators in C++ using the following struct, whose instance represents g(f) and contains an operator() method, in which can use this to self-reference to g(f), allowing us to support f(g(f), ...): 1234567template &lt;typename F, typename R, typename... Args&gt; struct FixedPointCombinator &#123; F f; R operator()(Args... args) const &#123; return f(*this, args...); &#125;&#125;; C++ supports compiler optimizations for this pattern. For example, the following code: 1234567891011121314151617181920212223242526272829303132333435#include &lt;stdio.h&gt;template &lt;typename F, typename R, typename... Args&gt; struct FixedPointCombinator &#123; F f; R operator()(Args... args) const &#123; return f(*this, args...); &#125;&#125;;auto fib = []( auto recursive_fib, unsigned int n) -&gt; unsigned long &#123; if (n == 0) return 0UL; else &#123; if (n == 1) return 1UL; else return recursive_fib(n - 1) + recursive_fib(n - 2); &#125;&#125;;int main() &#123; auto fib_ = FixedPointCombinator&lt;decltype(fib), unsigned long, unsigned int&gt; &#123;fib&#125;; unsigned int input; scanf(&quot;%u&quot;, &amp;input); unsigned long result = fib_(input); printf(&quot;%lu\\n&quot;, result); return 0;&#125; compiles to the following LLVM IR under clang++ -std=c++14 -O2 -S -emit-llvm, in which FixedPointCombinator&lt;decltype(fib), unsigned long, unsigned int&gt; &#123;fib&#125; has been optimized to the recursive function @_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071; ModuleID = &#x27;fib_recursive.cpp&#x27;source_filename = &quot;fib_recursive.cpp&quot;target datalayout = &quot;e-m:e-i64:64-f80:128-n8:16:32:64-S128&quot;target triple = &quot;x86_64-unknown-linux-gnu&quot;@.str = private unnamed_addr constant [3 x i8] c&quot;%u\\00&quot;, align 1@.str.1 = private unnamed_addr constant [5 x i8] c&quot;%lu\\0A\\00&quot;, align 1; Function Attrs: norecurse nounwind uwtabledefine dso_local i32 @main() local_unnamed_addr #0 &#123; %1 = alloca i32, align 4 %2 = bitcast i32* %1 to i8* call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4 %3 = call i32 (i8*, ...) @scanf(i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str, i64 0, i64 0), i32* nonnull %1) %4 = load i32, i32* %1, align 4, !tbaa !2 %5 = call fastcc i64 @&quot;_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j&quot;(i32 %4) #4 %6 = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.1, i64 0, i64 0), i64 %5) call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #4 ret i32 0&#125;; Function Attrs: argmemonly nounwinddeclare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1; Function Attrs: nofree nounwinddeclare dso_local i32 @scanf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: nofree nounwinddeclare dso_local i32 @printf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: argmemonly nounwinddeclare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1; Function Attrs: inlinehint nounwind readnone uwtabledefine internal fastcc i64 @&quot;_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j&quot;(i32) unnamed_addr #3 align 2 &#123; switch i32 %0, label %3 [ i32 0, label %9 i32 1, label %2 ]2: ; preds = %1 br label %93: ; preds = %1 %4 = add i32 %0, -1 %5 = tail call fastcc i64 @&quot;_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j&quot;(i32 %4) #4 %6 = add i32 %0, -2 %7 = tail call fastcc i64 @&quot;_ZNK3$_0clI20FixedPointCombinatorIS_mJjEEEEmT_j&quot;(i32 %6) #4 %8 = add i64 %7, %5 ret i64 %89: ; preds = %1, %2 %10 = phi i64 [ 1, %2 ], [ 0, %1 ] ret i64 %10&#125;attributes #0 = &#123; norecurse nounwind uwtable &quot;correctly-rounded-divide-sqrt-fp-math&quot;=&quot;false&quot; &quot;disable-tail-calls&quot;=&quot;false&quot; &quot;less-precise-fpmad&quot;=&quot;false&quot; &quot;min-legal-vector-width&quot;=&quot;0&quot; &quot;no-frame-pointer-elim&quot;=&quot;false&quot; &quot;no-infs-fp-math&quot;=&quot;false&quot; &quot;no-jump-tables&quot;=&quot;false&quot; &quot;no-nans-fp-math&quot;=&quot;false&quot; &quot;no-signed-zeros-fp-math&quot;=&quot;false&quot; &quot;no-trapping-math&quot;=&quot;false&quot; &quot;stack-protector-buffer-size&quot;=&quot;8&quot; &quot;target-cpu&quot;=&quot;x86-64&quot; &quot;target-features&quot;=&quot;+cx8,+fxsr,+mmx,+sse,+sse2,+x87&quot; &quot;unsafe-fp-math&quot;=&quot;false&quot; &quot;use-soft-float&quot;=&quot;false&quot; &#125;attributes #1 = &#123; argmemonly nounwind &#125;attributes #2 = &#123; nofree nounwind &quot;correctly-rounded-divide-sqrt-fp-math&quot;=&quot;false&quot; &quot;disable-tail-calls&quot;=&quot;false&quot; &quot;less-precise-fpmad&quot;=&quot;false&quot; &quot;no-frame-pointer-elim&quot;=&quot;false&quot; &quot;no-infs-fp-math&quot;=&quot;false&quot; &quot;no-nans-fp-math&quot;=&quot;false&quot; &quot;no-signed-zeros-fp-math&quot;=&quot;false&quot; &quot;no-trapping-math&quot;=&quot;false&quot; &quot;stack-protector-buffer-size&quot;=&quot;8&quot; &quot;target-cpu&quot;=&quot;x86-64&quot; &quot;target-features&quot;=&quot;+cx8,+fxsr,+mmx,+sse,+sse2,+x87&quot; &quot;unsafe-fp-math&quot;=&quot;false&quot; &quot;use-soft-float&quot;=&quot;false&quot; &#125;attributes #3 = &#123; inlinehint nounwind readnone uwtable &quot;correctly-rounded-divide-sqrt-fp-math&quot;=&quot;false&quot; &quot;disable-tail-calls&quot;=&quot;false&quot; &quot;less-precise-fpmad&quot;=&quot;false&quot; &quot;min-legal-vector-width&quot;=&quot;0&quot; &quot;no-frame-pointer-elim&quot;=&quot;false&quot; &quot;no-infs-fp-math&quot;=&quot;false&quot; &quot;no-jump-tables&quot;=&quot;false&quot; &quot;no-nans-fp-math&quot;=&quot;false&quot; &quot;no-signed-zeros-fp-math&quot;=&quot;false&quot; &quot;no-trapping-math&quot;=&quot;false&quot; &quot;stack-protector-buffer-size&quot;=&quot;8&quot; &quot;target-cpu&quot;=&quot;x86-64&quot; &quot;target-features&quot;=&quot;+cx8,+fxsr,+mmx,+sse,+sse2,+x87&quot; &quot;unsafe-fp-math&quot;=&quot;false&quot; &quot;use-soft-float&quot;=&quot;false&quot; &#125;attributes #4 = &#123; nounwind &#125;!llvm.module.flags = !&#123;!0&#125;!llvm.ident = !&#123;!1&#125;!0 = !&#123;i32 1, !&quot;wchar_size&quot;, i32 4&#125;!1 = !&#123;!&quot;clang version 9.0.1 (https://github.com/conda-forge/clangdev-feedstock 2ea3b72da24769de0dfc6dac99251a5d7a46144d)&quot;&#125;!2 = !&#123;!3, !3, i64 0&#125;!3 = !&#123;!&quot;int&quot;, !4, i64 0&#125;!4 = !&#123;!&quot;omnipotent char&quot;, !5, i64 0&#125;!5 = !&#123;!&quot;Simple C++ TBAA&quot;&#125; We can slightly modify the code above to support a fixed-point combinator that also provides memoization. Note that the instance of the fixed-point combinator is now stateful, and we should pass it by reference to fib (i.e., change the first parameter of fib to reference type): 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;#include &lt;map&gt;#include &lt;tuple&gt;template &lt;typename F, typename R, typename... Args&gt; struct FixedPointCombinatorWithMemoization &#123; F f; std::map&lt;std::tuple&lt;Args...&gt;, R&gt; memo; R operator()(Args... args) &#123; std::tuple&lt;Args...&gt; args_tuple(args...); // Check if result is in the memoization map auto found = memo.find(args_tuple); if (found != memo.end()) &#123; return found-&gt;second; // Return the cached result &#125; // Otherwise, compute and store the result R result = f(*this, args...); memo[args_tuple] = result; return result; &#125;&#125;;auto fib = []( auto&amp; recursive_fib, unsigned int n) -&gt; unsigned long &#123; if (n == 0) return 0UL; else &#123; if (n == 1) return 1UL; else return recursive_fib(n - 1) + recursive_fib(n - 2); &#125;&#125;;int main() &#123; auto fib_ = FixedPointCombinatorWithMemoization&lt;decltype(fib), unsigned long, unsigned int&gt; &#123;fib&#125;; unsigned int input; scanf(&quot;%u&quot;, &amp;input); unsigned long result = fib_(input); printf(&quot;%lu\\n&quot;, result); return 0;&#125; Type Abstractions and Template Functions Polymorphic Lambda Calculus (also known as Second Order Lambda Calculus or System F) introduces Type Abstractions and Type Applications. A Type Abstraction, written as  X . t, represents a Term (often a Lambda Abstraction) t containing a Type Variable X. A Type Application, written as t [T], uses a Concrete Type T to replace all instances of the Type Variable in the Term of the Type Abstraction. This can be used to implement Polymorphic Lambda Abstractions. For example, the following Type Abstraction representing a Polymorphic Identity Function: 1id =  X .  x: X . x can be instantiated to yield any concrete identity function that may be required, such as id [Nat]: Nat -&gt; Nat. Such Type Abstractions can be implemented in C++ using template functions: 123template &lt;typename X&gt; auto id(X x) &#123; return x;&#125; while Type Applications correspond to template instantiations: 1id&lt;int&gt; Should the template function be passed a callable, we usually want to use a template typename to support functions, function pointers, functors, and lambda expressions. Alternatively, we can also use auto to represent its type in the template function's parameter list. Note that using auto in a (non-lambda expression) function's parameter list requires C++20 or above. For example, the following Type Abstraction: 1double =  X .  f: X -&gt; X .  a: X . f(f a) can be represented using the following template function: 12345template &lt;typename X, typename F&gt; auto double_(const F f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125; or in C++20 or above: 12345template &lt;typename X&gt; auto double_(const auto f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125; C++ compilers support aggressive inlining optimizations when lambda expressions are used. For example, the call to const auto g = double_&lt;int&gt;([](int x) &#123; return 2 * x; &#125;); in the following source code: 1234567891011121314151617181920#include &lt;stdio.h&gt;template &lt;typename X&gt; auto double_(const auto f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125;int main() &#123; const auto g = double_&lt;int&gt;([](int x) &#123; return 2 * x; &#125;); int input; scanf(&quot;%d&quot;, &amp;input); printf(&quot;%d\\n&quot;, g(input)); return 0;&#125; has been completely inlined to %5 = shl i32 %4, 2 in the LLVM IR generated with clang++ -std=c++20 -O2 -S -emit-llvm: 123456789101112131415161718192021222324252627@.str = private unnamed_addr constant [3 x i8] c&quot;%d\\00&quot;, align 1@.str.1 = private unnamed_addr constant [4 x i8] c&quot;%d\\0A\\00&quot;, align 1; Function Attrs: norecurse nounwind uwtabledefine dso_local i32 @main() local_unnamed_addr #0 &#123; %1 = alloca i32, align 4 %2 = bitcast i32* %1 to i8* call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #3 %3 = call i32 (i8*, ...) @__isoc99_scanf(i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str, i64 0, i64 0), i32* nonnull %1) %4 = load i32, i32* %1, align 4, !tbaa !2 %5 = shl i32 %4, 2 %6 = call i32 (i8*, ...) @printf(i8* nonnull dereferenceable(1) getelementptr inbounds ([4 x i8], [4 x i8]* @.str.1, i64 0, i64 0), i32 %5) call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #3 ret i32 0&#125;; Function Attrs: argmemonly nounwind willreturndeclare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1; Function Attrs: nofree nounwinddeclare dso_local i32 @__isoc99_scanf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: nofree nounwinddeclare dso_local i32 @printf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: argmemonly nounwind willreturndeclare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1 Type Classes and Concepts in C++ Template Metaprogramming in C++ had been untyped, with template parameters being generic type variables substituted at template instantiation. In C++20, a type system has been added to this untyped template language through concepts. They are Boolean predicates on template parameters evaluated at the point of, not after, template instantiation. The compiler will produce a clear error immediately if a programmer tries to use a template parameter that doesn't meet the requirements of a concept. This starkly contrasts the challenging-to-grasp errors reported after an invalid type substitutes a generic type variable emanating from the implementation context rather than the template instantiation itself. For instance, the first two arguments to std::sort must be random-access iterators. If an argument is not a random-access iterator, an error will occur when std::sort attempts to use it as a bidirectional iterator. 12std::list&lt;int&gt; l = &#123;2, 1, 3&#125;;std::sort(l.begin(), l.end()); Without concepts, compilers may produce large amounts of error information, starting with an equation that failed to compile when it tried to subtract two non-random-access iterators: 123In instantiation of &#x27;void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = std::_List_iterator&lt;int&gt;; _Compare = __gnu_cxx::__ops::_Iter_less_iter]&#x27;: error: no match for &#x27;operator-&#x27; (operand types are &#x27;std::_List_iterator&lt;int&gt;&#x27; and &#x27;std::_List_iterator&lt;int&gt;&#x27;) std::__lg(__last - __first) * 2, However, if concepts are used, the problem can be found and reported at template instantiation: 12error: cannot call function &#x27;void std::sort(_RAIter, _RAIter) [with _RAIter = std::_List_iterator&lt;int&gt;]&#x27;note: concept &#x27;RandomAccessIterator()&#x27; was not satisfied It is straightforward to implement Type Classes with concepts. For instance, the Type Class below specifies the equal (==) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented using the following C++ concept: 123456789#include &lt;concepts&gt;// Declaration of the concept &quot;Eq&quot;,// which is satisfied by any type &#x27;T&#x27; such that for values &#x27;t&#x27; of type &#x27;T&#x27;, the expression t == t compiles and its type satisfies the concept std::same_as&lt;bool&gt;// This is represented using a &quot;requires expression&quot; which returns a booltemplate &lt;typename T&gt; concept Eq = requires (T t) &#123; &#123; t == t &#125; -&gt; std::same_as&lt;bool&gt;;&#125;; Afterwards, such a concept can be specified when template parameters are being introduced in a template definition, to indicate that the corresponding template parameter must satisfy the concept. 123template&lt;Eq T&gt; void f(const T&amp; t) &#123; // ...&#125; or (using a \"requires clause\"): 123template&lt;typename T&gt; requires Eq&lt;T&gt; void f(const T&amp; t) &#123; // ...&#125; References https://stackoverflow.com/questions/6948166/javas-interface-and-haskells-type-class-differences-and-similarities https://cs.gmu.edu/~sean/stuff/java-objc.html https://functionalcpp.wordpress.com/2013/08/16/type-classes/ https://stackoverflow.com/questions/32124627/how-are-c-concepts-different-to-haskell-typeclasses https://wiki.haskell.org/OOP_vs_type_classes https://doi.org/10.1145/1411318.1411324 https://www.foonathan.net/2021/07/concepts-structural-nominal/ https://www.reddit.com/r/haskell/comments/1e9f49/concepts_in_c_template_programming_and_type/","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"C++","slug":"Code/C","permalink":"https://abbaswu.github.io/categories/Code/C/"}],"tags":[]},{"title":"Paper Reading: \"Types and Programming Languages\" Chapter 22","slug":"Paper-Reading-Types-and-Programming-Languages-Chapter-22","date":"2023-02-26T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/02/26/Paper-Reading-Types-and-Programming-Languages-Chapter-22/","link":"","permalink":"https://abbaswu.github.io/2023/02/26/Paper-Reading-Types-and-Programming-Languages-Chapter-22/","excerpt":"","text":"Summary Chapter 22 of \"Types and Programming Languages\" explores the problem of Type Reconstruction (Type Inference) or deriving Types for Unannotated Arguments of Lambda Abstractions. It first introduces Type Variables and Substitutions before formalizing the Type Reconstruction problem. Then, it points out that Type Reconstruction can be implemented using a Constraint Typing Algorithm or an Algorithm that calculates a Set of Constraints between Types involving Type Variables and records them for later consideration, and proves the Completeness and Soundness of Constraint Typing. Moreover, it introduces a Unification Algorithm to calculate Principle Solutions (most general solutions) to Constraint Sets. Finally, the Chapter presents how the Typing Rules for Let Expressions can be modified to support Let Polymorphism - allowing an Untyped Function to generate different Constraints, thus be able to be Reconstructed to Different Types when applied to Terms of different Types. Critique Overall, Chapter 22 is clearly written, and several sections intrigued me (such as that Parametric Polymorphism and Type Reconstruction can result from two different interpretations of Dependent Types containing Type Variables). Moreover, this Chapter provides essential inspiration for my Class Project, \"Inferring Feasible Types for the Parameters and Return Values of Python Functions.\" However, the Chapter also used some Concepts without introducing them (such as the Unification Problem), and I had to look into them to understand parts of the Chapter. Background Knowledge Completeness and Soundness of a Theory Using \\(TRUE\\) and \\(PROVABLE\\) to represent the Set of Facts that are True and Provable under a Theory, respectively: Completeness: \\(TRUE \\subseteq PROVABLE\\) or every Fact that is True is also Provable (but there may be some Facts that are Provable but are not True). Soundness: \\(PROVABLE \\subseteq TRUE\\) or every Fact that is Provable is also True (but there may be some True Facts that are not Provable). Completeness and Soundness: \\(TRUE = PROVABLE\\). An ideal Theory should be both Complete and Sound. Unification Problem Given two Terms containing some Variables, find a Substitution (an Assignment of Terms to Variables) that makes the two Terms equal. For example, given \\(f(x_1, h(x_1), x_2) = f(g(x_3), x_4, x_3)\\), a valid Substitution is \\(\\sigma = \\{g(x_3): x_1, x_3: x_2, h(g(x_3)): x_4\\}\\). Takeaways From This Paper Parametric Polymorphism and Type Reconstruction Given Dependent Types containing Type Variables (often the result of the Programmer leaving out Type Annotations in Source Code), we can make one of the following assumptions. All Substitution Instances are well-typed. Thus, it is possible for Type Variables to be held abstract during Type Checking and only be Substituted for Concrete Types later on. This is the basis of Parametric Polymorphism. Not all Substitution Instances are well-typed. In this case, we want to look for valid Substitutions. This leads us to the problem of Type Reconstruction. Deriving Constraint Sets and Calculating Solutions to Them To explore valid ways that Concrete Types can substitute Type Variables, we can calculate a Set of Constraints between Types involving Type Variables. This is similar to an ordinary Type Checking Algorithm checking Requirements in the Premise but records these Requirements as Constraints for later consideration instead of checking them immediately. After we have generated a Constraint Set, we can use a Unification Algorithm to calculate Solutions to it. The Unification Algorithm proposed in the Chapter removes a Constraint from the Constraint Set, processes it, and recursively processes the remaining Constraint Set. There is a most general way to instantiate the Type Variables. This is known as a Principle Solution, which contains Principle Types, or the most general types, for Type Variables. Inspirations From This Paper This Paper points out a viable way to implement my Class Project \"Inferring Feasible Types for the Parameters and Return Values of Python Functions.\" Propose Typing Rules for Python Expressions. Implement an Algorithm similar to an ordinary Type Checking Algorithm checking Requirements in the Premise, but which records these Requirements as Constraints for later consideration instead of checking them immediately. Questions What are the specific types of Constraints that are recorded when deriving Constraint Sets? What do the derived Constraint Sets look like? Implementing the Unification Algorithm proposed to calculate Solutions to the Constraint Set seems non-trivial. Are there any implementations of it for more \"real-world\" (imperative, non-ML Family) Programming Languages? What adjustments have to be made to accomplish such an implementation?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Displaying Information for Thrown and Caught Exceptions to the User in Python","slug":"Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python","date":"2023-02-20T08:00:00.000Z","updated":"2023-11-06T07:31:36.161Z","comments":true,"path":"2023/02/20/Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python/","link":"","permalink":"https://abbaswu.github.io/2023/02/20/Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python/","excerpt":"","text":"Exception Semantics in Python Exception handling refers to how a program reacts when unexpected events, known as exceptions, occur throughout the program's execution. Exception semantics varies considerably among programming languages. Based on this, we can divide programming languages into two groups: Programming languages that only employ exceptions to address exceptional, unforeseen, or incorrect circumstances, such as C++, Java, and C#. Programming languages that use exceptions as standard flow control structures, such as Ada, ML, OCaml, Python, and Ruby. For example, in Python, when an iterator has exhausted its output, and no more items can be generated, an exception of type StopIteration is thrown. As a result, exceptions are pervasive in Python, and exception catching and handling is a must for writing robust Python code. Displaying Information for Thrown and Caught Exceptions to the User In many situations, it is beneficial to handle the exception and give a user a \"loud and clear\" message of what has happened as feedback. This is also particularly useful in investigating the root cause of the exception and whether it is the tip of the iceberg of a more significant latent bug. This can be simplified by the fact that exceptions thrown by built-in functions, standard library functions, and functions in many well-tested third-party libraries all contain rich semantics in: The class of the exception. Given an exception e, it is accessible via type(e), and type(e).__name__ gives a str representation. The message of the exception. Given an exception e, str(e) generates a representation of the argument(s) to the instance. In command-line programs, we can write both of them to stderr, as shown in the example below: 12345678from sys import stderrtry: # Do some potentially erroneous operationexcept Exception as e: # Write the class of the exception and the message of the exception to stderr print(type(e).__name__, str(e), file=stderr) In GUI programs, we can display them in a message box, with the class of the exception being the title of the message box and the message of the exception being the message of the message box, as shown in the example below: 12345678910111213141516171819202122232425262728from PySide6.QtCore import Slotfrom PySide6.QtWidgets import QDialog, QMessageBoxfrom .ui import Ui_ConnectToServerDialogclass ConnectToServerDialog(QDialog): def __init__(self, parent=None): super().__init__(parent) self.ui=Ui_ConnectToServerDialog() self.ui.setupUi(self) self.ui.connectPushButton.clicked.connect(self.accept) self.server=None @Slot() def accept(self): try: # Do some operation that involves potentially erroneous user input except Exception as e: # Display the exception thrown in a QMessageBox # The type of the exception is the title of the QMessageBox # The message of the exception is the message of the QMessageBox QMessageBox.about(self, type(e).__name__, str(e)) return super().accept() References https://en.wikipedia.org/wiki/Exception_handling#Exception_support_in_programming_languages https://docs.python.org/3/library/exceptions.html#bltin-exceptions","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"}],"tags":[]},{"title":"Using ffmpeg to Manipulate Video Files","slug":"Using-ffmpeg-to-Manipulate-Video-Files","date":"2023-02-18T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/02/18/Using-ffmpeg-to-Manipulate-Video-Files/","link":"","permalink":"https://abbaswu.github.io/2023/02/18/Using-ffmpeg-to-Manipulate-Video-Files/","excerpt":"","text":"FFmpeg is a collection of libraries and tools for manipulating video, audio, and other multimedia files and streams. It is frequently used for basic editing (cutting and joining), video scaling, post-production video effects, and standard compliance (SMPTE, ITU). The main component of FFmpeg is the command-line tool ffmpeg, which reads an input file, applies transformations, and writes to an output file. The basic usage pattern of ffmpeg is ffmpeg -i &lt;input file&gt; [options] &lt;output file&gt;. ffmpeg automatically selects the decoder and encoder based on the extension of &lt;input file&gt; and &lt;output file&gt;, and we specify what transformations we apply to the original video file in [options]. We now explain what to write in [options] to address typical video manipulation demands. Cut a Video Add the option -ss &lt;start time in hours&gt;:&lt;start time in minutes&gt;:&lt;start time in seconds&gt; -to &lt;end time in hours&gt;:&lt;end time in minutes&gt;:&lt;end time in seconds&gt;. If there is no need to transcode the video format or apply other transformations, add -c:v copy -c:a copy for increased speed. For example, ffmpeg -i input.mp4 -ss 00:05:10 -to 00:15:30 -c:v copy -c:a copy output.mp4 saves input.mp4 from 00:05:10 to 00:15:30 to output.mp4 with no transcoding or other transformations, resulting in a 10 minutes and 20 seconds video. Change Frame Rate Add the option -r &lt;frame rate&gt;. For example, ffmpeg -i input.mp4 -r 12 output.gif controls the frame rate of output.gif generated by transcoding to 12FPS. Change Output Resolution Add the option -s &lt;length&gt; x &lt;width&gt;. The units of &lt;length&gt; and &lt;width&gt; are in pixels. It is usually required that the length and width be scaled equally from the original length and width to avoid distortion of the picture. For example, ffmpeg -i input.mp4 -s 320x180 output.gif controls the resolution of the output.gif generated by transcoding to 320x180. Change Playback Speed To speed up the output video to k times its original size: add the option -filter:v 'setpts=PTS/&lt;k&gt;'. To slow down the output video to k times the original: add the option -filter:v 'setpts=&lt;k&gt;*PTS'. For example, ffmpeg -i input.mp4 -filter:v 'setpts=PTS/2' output.gif (or ffmpeg -i input.mp4 -filter:v 'setpts=0.5*PTS' output.gif) will transcode the resulting output.gif to speed up to 2 times the original. Extract Audio Before we extract audio, we need to first look up media information using ffprobe, a command-line tool installed with ffmpeg: 12345678910$ ffprobe video.mkv...Input #0, matroska,webm, from &#x27;video.mkv&#x27;: Metadata: encoder : no_variable_data creation_time : 1970-01-01T00:00:00.000000Z Duration: 00:23:30.07, start: 0.000000, bitrate: 1392 kb/s Stream #0:0: Audio: aac (LC), 48000 Hz, stereo, fltp (default) Metadata:... We can see that the format of the audio stream is aac. Now, we can add the option -map 0:a -acodec copy to copy the audio stream. Note that the extension of output file should correspond with the format of the audio stream. For example, we run the following command to save the audio stream of video.mkv to audio.mp4: ffmpeg -i video.mkv -map 0:a -acodec copy audio.mp4. Note that .mp4 is a valid extensions of aac audio streams. References https://en.wikipedia.org/wiki/FFmpeg https://shotstack.io/learn/use-ffmpeg-to-trim-video/ https://homehack.nl/create-animated-gifs-from-mp4-with-ffmpeg/ https://superuser.com/questions/1261678/how-do-i-speed-up-a-video-by-60x-in-ffmpeg/1261681 https://www.baeldung.com/linux/ffmpeg-audio-from-video","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Unix","slug":"Code/Unix","permalink":"https://abbaswu.github.io/categories/Code/Unix/"}],"tags":[]},{"title":"Syncing a Local Directory With a Remote Directory via FTP","slug":"Syncing-a-Local-Directory-With-a-Remote-Directory-via-FTP","date":"2023-02-16T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/02/16/Syncing-a-Local-Directory-With-a-Remote-Directory-via-FTP/","link":"","permalink":"https://abbaswu.github.io/2023/02/16/Syncing-a-Local-Directory-With-a-Remote-Directory-via-FTP/","excerpt":"","text":"Syncing a Local Directory With a Remote Directory via FTP Overview We can use the following Sequence Diagram to depict the process of modifying the Files within a Local Directory and a Remote Directory before syncing the two Directories via the File Transfer Protocol (FTP), a standard communication protocol used on computer networks to send files between a Client and a Server. sequenceDiagram loop Machine With the FTP Client -&gt;&gt; Machine With the FTP Client: Modify Files Machine With the FTP Server -&gt;&gt; Machine With the FTP Server: Modify Files Machine With the FTP Server -&gt;&gt; Machine With the FTP Server: Start FTP Server Machine With the FTP Client -&gt;&gt; Machine With the FTP Client: Start FTP Client Machine With the FTP Client -&gt;&gt; Machine With the FTP Server: Connect to FTP Server Machine With the FTP Client -&gt;&gt; Machine With the FTP Server: Mirror Remote Directory -&gt; Local Directory Machine With the FTP Server --&gt;&gt; Machine With the FTP Client: Update Local Directory With Changed Files in Remote Directory Machine With the FTP Client -&gt;&gt; Machine With the FTP Server: Reverse Mirror Local Directory -&gt; Remote Directory Machine With the FTP Client --&gt;&gt; Machine With the FTP Server: Update Remote Directory With Changed Files in Local Directory end Setting Up the Machine With the FTP Client Obviously, we need to install an FTP Client on this Machine. Personally, I recommend installing lftp, a \"sophisticated\" file transfer program. Unlike a standard FTP Client, which only enables you to upload or download files, lftp additionally enables you to maintain file synchronisation using its built-in mirror command. Setting Up the Machine With the FTP Server We would also need an FTP Server on the other Machine. Although there are many existing UNIX FTP servers, such as proftpd and vsftpd, they are usually tricky to compile, configure, and set up and require Root Privileges to run, which is tedious, if not impossible, in many situations. As an alternative, we write our own FTP Server using pyftpdlib, a pure Python FTP server library written which offers a high-level interface to creating portable and efficient FTP servers. Such a solution requires us to have a Python environment running on the Machine With the FTP Server and install pyftpdlib, which is very simple in today's world where the Python ecosystem is ubiquitous. After setting up a Python environment and installing pyftpdlib, we can write a script for an FTP server. Below is the script that I am using. By default, it sets up a user user with password 12345 and listens on port 2121 of the Machine With the FTP Server's Outbound IP Address, but these settings can all be tweaked by providing command-line arguments. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#!/usr/bin/env python3import argparseimport osimport socketfrom pyftpdlib.authorizers import DummyAuthorizerfrom pyftpdlib.handlers import FTPHandlerfrom pyftpdlib.servers import FTPServerdef get_outbound_ip_address(): s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) try: # doesn&#x27;t even have to be reachable s.connect((&#x27;10.255.255.255&#x27;, 1)) IP = s.getsockname()[0] except Exception: IP = &#x27;127.0.0.1&#x27; finally: s.close() return IPdef parse_command_line_arguments(): parser = argparse.ArgumentParser() parser.add_argument(&#x27;--user&#x27;, type=str, required=False, default=&#x27;user&#x27;, help=&#x27;Define a user having full read/write permissions&#x27;) parser.add_argument(&#x27;--password&#x27;, type=str, required=False, default=&#x27;12345&#x27;, help=&#x27;The password of the user having full read/write permissions&#x27;) parser.add_argument(&#x27;--anonymous&#x27;, action=&#x27;store_true&#x27;, required=False, default=False, help=&#x27;Add read-only anonymous user&#x27;) parser.add_argument(&#x27;--root&#x27;, type=str, required=False, default=os.getcwd(), help=&#x27;Root directory in FTP server&#x27;) parser.add_argument(&#x27;--host&#x27;, type=str, required=False, default=get_outbound_ip_address(), help=&#x27;Host to listen on&#x27;) parser.add_argument(&#x27;--port&#x27;, type=int, required=False, default=2121, help=&#x27;Port to listen on&#x27;) args = parser.parse_args() return args.user, args.password, args.anonymous, args.root, args.host, args.portdef main(): # Parse command line arguments user, password, anonymous, root, host, port = parse_command_line_arguments() # Instantiate a dummy authorizer for managing &#x27;virtual&#x27; users authorizer = DummyAuthorizer() # Define a new user having full r/w permissions authorizer.add_user(user, password, root, perm=&#x27;elradfmwMT&#x27;) # Add anonymous user if anonymous: authorizer.add_anonymous(root) # Instantiate FTP handler class handler = FTPHandler handler.authorizer = authorizer # Define a customized banner (string returned when client connects) handler.banner = &quot;pyftpdlib based ftpd ready.&quot; # Instantiate FTP server class and listen on &lt;host&gt;:&lt;port&gt; server = FTPServer((host, port), handler) # Start FTP server server.serve_forever()if __name__ == &#x27;__main__&#x27;: main() Save this to a file, and run chmod +x on the file to make it executable. 12345678910111213141516abbas@abbas-ThinkPad-X1-Carbon-Gen-9:~$ ./pyftpd -husage: pyftpd [-h] [--user USER] [--password PASSWORD] [--anonymous] [--root ROOT] [--host HOST] [--port PORT]optional arguments: -h, --help show this help message and exit --user USER Define a user having full read/write permissions --password PASSWORD The password of the user having full read/write permissions --anonymous Add read-only anonymous user --root ROOT Root directory in FTP server --host HOST Host to listen on --port PORT Port to listen onabbas@abbas-ThinkPad-X1-Carbon-Gen-9:~$ ./pyftpd [I 2023-02-16 15:47:39] &gt;&gt;&gt; starting FTP server on 10.43.111.144:2121, pid=11969 &lt;&lt;&lt;[I 2023-02-16 15:47:39] concurrency model: async[I 2023-02-16 15:47:39] masquerade (NAT) address: None[I 2023-02-16 15:47:39] passive ports: None Demonstration Last but not least, we will present a demonstration of syncing a Local Directory With a Remote Directory via FTP. Our Local Directory is a directory named mirror_ubuntu with two files mirror_ubuntu_1.txt and mirror_ubuntu_2.txt on the Machine With the FTP Client. Local Directory Our Remote Directory is a directory named mirror_ipad with two files mirror_ipad_1.txt and mirror_ipad_2.txt on the Machine With the FTP Server, which is the iSH app running within an iPad. Remote Directory We start the FTP Server on the Machine With the FTP Server, and start the FTP Client on the Machine With the FTP Client. As depicted in Overview, we first Mirror Remote Directory to Local Directory, which can be accomplished by running mirror --continue --no-perms &lt;Remote Directory&gt; &lt;Local Directory&gt; within lftp. Mirroring Remote Directory to Local Directory The Local Directory will now contain Files that were modified the Remote Directory. Local Directory After Mirroring Remote Directory to Local Directory Afterwards, we Reverse Mirror Local Directory to Remote Directory, which can be accomplished by running mirror --continue --no-perms --reverse &lt;Local Directory&gt; &lt;Remote Directory&gt; within lftp. Reverse Mirroring Local Directory to Remote Directory The Remote Directory will now contain Files that were modified the Local Directory. Remote Directory After Reverse Mirroring Local Directory to Remote Directory At this point, the Local Directory has been successfully synced with the Remote Directory. References https://www.geekbitzone.com/posts/lftp/lftp-mirror-remote-folders/ https://github.com/giampaolo/pyftpdlib","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"}],"tags":[]},{"title":"Paper Reading: \"Types and Programming Languages\" Chapter 15 and Chapter 16","slug":"Paper-Reading-Types-and-Programming-Languages-Chapter-15-and-Chapter-16","date":"2023-02-10T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/02/10/Paper-Reading-Types-and-Programming-Languages-Chapter-15-and-Chapter-16/","link":"","permalink":"https://abbaswu.github.io/2023/02/10/Paper-Reading-Types-and-Programming-Languages-Chapter-15-and-Chapter-16/","excerpt":"","text":"Summary Chapter 15, \"Subtyping,\" describes adding Subtyping with Functions and Records into Simply Typed Lambda Calculus. It formalizes the Subtype Relation as a collection of Inference Rules, verifies that verify that the Preservation and Progress Theorems of Simply Typed Lambda Calculus still apply, examines Ascription (or Casting) in the context of Subtyping, and proposes Subtyping Rules for Variants, Lists, References, and Arrays. Finally, it presents alternative Coercion Semantics for Subtyping. Chapter 16, \"Metatheory of Subtyping,\" observes that the Subtyping Rules presented in the previous chapter are not syntax-directed and have overlapping conclusions, which impedes implementing a Typechecking Algorithm, and develops the Algorithmic Subtype Relation and the Algorithmic Typing Relation to address these problems. Critique Acquired Insights I will first summarize the insights that I gained while reading these Chapters. An empty Bottom Type is useful, both as a way of expressing that a Function is not intended to return and telling the Typechecker that the Term can be associated with any Type. Implementing Ascription (Casting) in Subtyping is non-trivial, especially for Downcasting. As blindly following Type Assertions may lead to potentially serious consequences, the Compiler would need to insert a Runtime Type Check, essentially adding the Machinery for Typechecking to the Runtime System. This might incur a significant performance overhead. Different from an Inheritance Based Class Hierarchy, which is a physical relationship between Types, Subtyping generally is more of a logical relationship between Types. For example, in the alternative Coercion Semantics for Subtyping, we can consider that int and float, two Types that do not inherit from one another, have a Subtyping Relation, as they can be converted to one another. In this case, the Subtyping Relation is compiled to Coercions at runtime (instructions physically converting an int to a float, or vice versa), which are much more efficient than virtual function calls frequently seen in an Inheritance Based Class Hierarchy. Background Knowledge There is no doubt that the Chapters are written in great detail. However, I find some of the content, especially the terminology, a little difficult to understand, and I have looked into background knowledge concerning the topic. Below summarizes what I have read. Polymorphism Polymorphism describes that a single Interface can work with Terms of Different Types in Programming Languages. There are different kinds of Polymorphism in the context of Programming Languages, including: Parametric Polymorphism Also known as \"Generic Programming\". Using Abstract Symbols that can substitute for any Type instead of specifying Concrete Types in Interfaces. C++'s Template Metaprogramming comes close to Parametric Polymorphism (except for Template Specializations). Ad Hoc Polymorphism Defining a Common Interface for a Set of Individually Specified Types. Includes Function Overloading, Operator Overloading, and C++'s Template Metaprogramming with Template Specializations. Subtyping It is a form of Polymorphism in which the Terms of a Subtype T, which is related to another Type known as the Supertype T' in some way, can be safely used in any Context where the Terms of T' are used. The Concept of Subtyping has gained visibility with the advent of Object Oriented Programming Languages, where it is frequently the case that an Inheritance Based Class Hierarchy forms the basis of Subtyping, and such Safe Substitution is known as the Liskov Substitution Principle. However, stepping out of this specific and widely known context, there are several different Schemes of Subtyping. They can be broadly classified along two dimensions: Nominal Subtyping vs. Structural Subtyping and Inclusive Implementations vs. Coercive Implementations. Nominal Subtyping requires the Subtyping Relation to be explicitly declared among the two Types. This is the case with the Subtyping based on an Inheritance Based Class Hierarchy frequently encountered in Object Oriented Programming Languages. In contrast, in Structural Subtyping, a Type T is implicitly the Subtype of another Type T' if Terms of T has all the Properties of Terms of T' and can handle all the Messages Terms of T' can handle. This is closely related to Row Polymorphism or the so-called Duck Typing in Dynamically Typed Programming Languages. On another dimension, Implementations of Subtyping can be divided into Inclusive Implementations and Coercive Implementations. In Inclusive Implementations, any Term of a Subtype, left unchanged, is automatically a Term of a Supertype. This is often the case with the Subtyping based on an Inheritance Based Class Hierarchy frequently encountered in Object Oriented Programming Languages. A Term can have multiple Types in this situation. In contrast, Coercive Implementations are defined by Type Conversion Functions from Subtype to Supertype and allow a Term of a Subtype to be converted to a Term of a Supertype, such as the case for int's, float's, and str's. It is also worth noticing that applying the Type Coercion Function from A to B and then from B to C might have a different result from directly applying the Type Coercion Function from A to C. For example, str(float(2)) returns a value different from str(2). Based on the concept of Subtyping, the concept of Variance reference to how the Subtyping Relations between more complex Types relates to the Subtyping Relations between the simpler Types they include. For example, given that Cat is a Subtype of Animal, should a List of Cat's be a Subtype of a List of Animal's? What about a Function that takes a Term of Type Cat as an Arugument and a Function that takes a Term of Type Animal as an Arugument? Different Programming Languages have different implementations, but most Programming Languages respect the following patterns. If the Complex Types are Read Only and/or capable of returning Terms of the Simple Types, they should have the same Subtyping Relations as the Simple Types. This is known as Covariance. For example, A read-only List of Cat's can be used whenever a read-only List of Animal's is required, as each Term read from the read-only List of Cat's is of Type Cat, which is a Subtype of Animal. In other words, const List&lt;Cat&gt; is a Subtype of const List&lt;Animal&gt;. It is not safe to use a const List&lt;Animal&gt; where a const List&lt;Cat&gt; is required, as a Term read from a const List&lt;Animal&gt; may not be of Type Cat. In other words, const List&lt;Animal&gt; is not a Subtype of const List&lt;Cat&gt;. If the Complex Types are Write Only and/or capable of accepting Terms of the Simple Types as Parameters, they should have the opposite Subtyping Relations as the Simple Types. This is known as Contravariance. For example, A Function that takes a Term of Type Animal as a Parameter may be used where a Function that takes a Term of Type Cat as a Parameter is used, as each Term of Type Cat can also be passed as a Parameter of Type Animal. In other words, Animal -&gt; T is a Subtype of Cat -&gt; T. It is not safe to use a Cat -&gt; T where an Animal -&gt; T is required, as a Term of Type Animal may not be passed as a Parameter of Type Cat. In other words, Cat -&gt; T is not a Subtype of Animal -&gt; T. If the Complex Types are Read/Write, they should have no Subtying Relations. This is known as Invariance. For example, A Term written into a List&lt;Animal&gt; need not be of Type Cat, but a Term written into a (non-constant) List&lt;Cat&gt; must be of Type Cat. Thus, it is not safe to use a List&lt;Cat&gt; where a List&lt;Animal&gt; is required. In other words, List&lt;Cat&gt; is not a Subtype of List&lt;Animal&gt;. A Term read from a (non-constant) List&lt;Animal&gt; may not be of Type Cat. Thus it is not safe to use a List&lt;Animal&gt; where a List&lt;Cat&gt;is required. In other words, List&lt;Animal&gt; is not a Subtype of List&lt;Cat&gt;. References https://en.wikipedia.org/wiki/Polymorphism_(computer_science) https://stackoverflow.com/questions/36948205/why-is-c-said-not-to-support-parametric-polymorphism https://en.wikipedia.org/wiki/Subtyping https://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science) Having acquired such Background Knowledge, I will also summarize the insights that I gained while reading these Chapters. Acquired Insights An empty Bottom Type is useful, both as a way of expressing that a Function is not intended to return and telling the Typechecker that the Term can be associated with any Type. Implementing Ascription (Casting) in Subtyping is non-trivial, especially for Downcasting. As blindly following Type Assertions may lead to potentially serious consequences, the Compiler would need to insert a Runtime Type Check, essentially adding the Machinery for Typechecking to the Runtime System. This might incur a significant performance overhead. Different from an Inheritance Based Class Hierarchy, which is a physical relationship between Types, Subtyping generally is more of a logical relationship between Types. For example, in the alternative Coercion Semantics for Subtyping, we can consider that int and float, two Types that do not inherit from one another, have a Subtyping Relation, as they can be converted to one another. In this case, the Subtyping Relation is compiled to Coercions at runtime (instructions physically converting an int to a float, or vice versa), which are much more efficient than virtual function calls frequently seen in an Inheritance Based Class Hierarchy.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: \"Types and Programming Languages\" Chapter 13 and Chapter 14","slug":"Paper-Reading-Types-and-Programming-Languages-Chapter-13-and-Chapter-14","date":"2023-02-05T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/02/05/Paper-Reading-Types-and-Programming-Languages-Chapter-13-and-Chapter-14/","link":"","permalink":"https://abbaswu.github.io/2023/02/05/Paper-Reading-Types-and-Programming-Languages-Chapter-13-and-Chapter-14/","excerpt":"","text":"Summary Chapters 13 and 14 of \"Types and Programming Languages\" discuss adding Impure Features, also known as Computational Effects, into Simply Typed Lambda Calculus. Specifically, Chapter 13 discusses adding References to Mutable Cells that can be Allocated, Dereferenced, and Assigned and formalizes their Operational Behavior. Chapter 14 gradually adds Raising and Handling Exceptions, starting from a Term error of any Type that completely aborts Evaluation when applied as a Function or passed as an Argument to a Function, before supporting Exception Handling, as well as Raising a Value (potentially containing information about what unusual thing happened) as an Exception. Critique Overall, I believe these two Chapters are written very well, as they progressively add realistic features to Simply Typed Lambda Calculus. I will summarize takeaways from this paper before presenting some questions and comments. Takeaways From This Paper References to Mutable Cells The Formalization of the Operational Behavior of References to Mutable Cells encompasses Allocations (providing an initial value to a Mutable Cell), Dereferences (reading the current value of the referenced Cell), and Assignments (changing the value stored in the referenced Cell), but not Deallocations. Explicit Deallocations lead to the Dangling Reference Problem, which undermines Type Safety. Instead, References to Mutable Cells that are no longer needed should be Garbage Collected. An interpretation of how Aliasing makes Program Analysis tricky is that Aliasing essentially sets up \"Implicit Communication Channels in the form of Shared State\" between different parts of a Program. To formalize the Operational Behavior of References to Mutable Cells, we can consider a Reference \\(l \\in L\\), where \\(L\\) is the set of Locations of the Program's Store (a.k.a. Heap Memory) \\(\\mu\\). As the result of Evaluating an Expression depends on the current contents of the Store and may cause Side Effects for the Store, Evaluation Rules should, in addition to Terms and Types, take the Store as an Argument and return a new Store as part of the result of Evaluating an Expression. Furthermore, in a naive implementation of Typing Rules for References to Mutable Cells, the Type of the Reference depends on the Type of the Mutable Cell, e.g., \\(\\frac{\\Gamma \\vdash \\mu(l): T}{\\Gamma \\vdash l: \\text{Ref} \\: T}\\). However, this is inefficient where there are multiple levels of Indirection and is problematic where there are Cyclic References. To solve this problem, the Chapter proposes extending Typing Rules with a Store Typing \\(\\Sigma\\), which maps every Location \\(l \\in L\\) to a fixed, definite Type. In this case, the Typing Rule is written as \\(\\frac{\\Gamma | \\Sigma \\vdash \\Sigma(l): T}{\\Gamma | \\Sigma \\vdash l: \\text{Ref} \\: T}\\). Raising and Handling Exceptions The first (and most straightforward) Approach to Raising and Handling Exceptions, a Term error that completely aborts Evaluation when applied as a Function or passed as an Argument to a Function, effectively simulates Unwinding the Call Stack when it propagates error to the top level. The final approach that supports both Exception Handling and Raising a Value as an Exception considers an Exception to be a Value \\(t_{exp}\\) of Type \\(T_{exp}\\) (instead of a Term error). It proposes a Term Constructor raise t_&#123;exp&#125; that describes Raising a Value as an Exception, and models Exception Handling with try t_1 with t_2: T_1, in which \\(t_1: T_1\\) and \\(t_2: T_{exp} \\rightarrow T_1\\) (i.e., \\(t_2\\) is a function, called when an Exception is Raised, taking a Raised Exception as Input and Returning a Value of the same Type as \\(t_1\\) as Output). Questions and Comments After reading these two Chapters, the power of Functions as a Universal Abstraction has left a deep impression on me. For example: Arrays containing Terms of Type \\(T\\) can be modeled as References to Functions of type \\(Nat \\rightarrow T\\). The Referenced Function looks up the Element given an Index. Exception Handling is modeled with try t_1 with t_2, in which \\(t_2\\) is a function called when an Exception is Raised, taking a Raised Exception as Input and Returning a Value of the same Type as \\(t_1\\) as Output). This describes complex Side Effects in a realistic Programming Language in a Side Effect Free manner that is clean and easy to reason about while not sacrificing Expressiveness. Are there any other complex Side Effects that can be modeled like this using Functions?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: Bi-directional type checking","slug":"Paper-Reading-Bi-directional-type-checking","date":"2023-01-30T08:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/01/30/Paper-Reading-Bi-directional-type-checking/","link":"","permalink":"https://abbaswu.github.io/2023/01/30/Paper-Reading-Bi-directional-type-checking/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Type Systems. The original paper can be found here. Summary The paper first explains that except Syntax Directed Systems, Typing Rules cannot be directly translated into Algorithms for Type Checking and Type Inference. It presents a motivating example of this using a Simply Typed Lambda Calculus having Bool and Function as Types and Bool Constants, Variables, Function Abstractions, Function Applications, and Conditional Expressions as Terms, in which the Typing Rule for Function Abstractions cannot be directly translated into a Function for Type Inference. It then presents Bidirectional Typing as a remedy to this problem. It explains what Bidirectional Typing is, discusses its advantage, and adds Bidirectional Typing into the previously presented Simply Typed Lambda Calculus, presenting how Bidirectional Typing works during the process. Finally, it discusses the limitations of Bidirectional Typing and presents academic literature for further reading. Critique Overall, I believe this paper is written very well, as I can grasp most of it after reading it. I will summarize my takeaways from this paper before presenting some questions and comments. My Takeaways From This Paper What Bidirectional Typing Is Bidirectional Typing splits each Typing Rule \\(\\Gamma \\vdash t: T\\) into: An Inference Rule \\(\\Gamma \\vdash t \\Rightarrow T\\), which infers \\(t\\)'s type to be \\(T\\) in Context \\(\\Gamma\\). A Type Checking Rule \\(\\Gamma \\vdash t \\Leftarrow T\\), which checks \\(t\\)'s type to be \\(T\\) in Context \\(\\Gamma\\). The Inference Rules and Type Checking Rules would work together and call each other. Advantages of Bidirectional Typing Makes general Typing Rules more Syntax Directed, thus, simplifying implementing Algorithms for Type Checking and Type Inference. Requires relatively few additional Type Annotations. Produces good error messages that report where the error occurs. Limitations of Bidirectional Typing Variables in a Derivation can no longer be replaced by the Derivation for a Term of the same Type. This is because Bidirectional Typing uses Inference Mode to check Variables but uses Checking Mode to check many other Terms. In some situations, explicit Type Annotations may need to be written within complex Terms, such as a direct Application of a Function Abstraction, like ( b . if b then false else true) true: Bool Questions and Comments Page 8 mentions, \"remember that the derivation, like the bidirectional typing rules, should be read bottom-to-top and left-to-right.\" However, Inference Rules have the form of \\(\\frac{Premise}{Conclusion}\\). So, why should the derivation be read from Conclusion to Premise? What are the meanings of the small-step rule \\(\\frac{}{t : T \\rightarrow t}\\) and the large-step rule \\(\\frac{t \\Downarrow t&#39;}{t : T \\Downarrow t&#39;}\\) on Page 8? I believe explicit Type Annotations should be enforced for the Parameters within Function Abstractions, such as ( b: Bool . if b then false else true) instead of ( b . if b then false else true). This aligns with real-world programming languages (C++, Java, Rust, Swift, Haskell, etc.) This increases readability. This simplifies both the Typing Rules and the Inference Rules and Type Checking Rules of Bidirectional Typing. Feedback from the Class Discussion Small Step Semantics, represented using \\(\\rightarrow\\)'s, depict one step in Evaluation. For example, if \\(e\\) is \\(true\\) itself, \\(\\text{if}\\: e \\: \\text{then} \\: e_1 \\: \\text{else} \\: e_2\\) can be Evaluated in one step to \\(e_1\\). This can be represented using \\(\\frac{e \\rightarrow true}{\\text{if}\\: e \\: \\text{then} \\: e_1 \\: \\text{else} \\: e_2 \\rightarrow e_1}\\) Big Step Semantics, represented using \\(\\Downarrow\\)'s, depict Reducing a Subexpression to a Value through several Small Steps. For example, if \\(e\\) is a Subexpression that can be Reduced to \\(true\\) after several Small Steps, \\(\\text{if}\\: e \\: \\text{then} \\: e_1 \\: \\text{else} \\: e_2\\) can be Reduced to \\(e_1\\) after several Small Steps. This can be represented using \\(\\frac{e \\Downarrow true}{\\text{if}\\: e \\: \\text{then} \\: e_1 \\: \\text{else} \\: e_2 \\Downarrow e_1}\\). Syntax Directed means a one-to-one correspondence between the Type of the Term and the Syntax (Derivation of the Grammar Rules) of the Term. There is no precise definition for Bidirectional Typing. Instead, Bidirectional Typing points a direction toward implementing a Type Inference/Type Checking Algorithm. In Bidirectional Typing, we prefer to start from Inference Mode because if we can Infer the Type of a Term, we can Check the Type of the Term, while Checking falls back on Inference. Why is there only a Checking Rule and no Inference Rule for if t then t else t? This gives better error messages. Should the Terms in the then branch and the else branch have different types, it is possible to give an error message directly stating this information. If an Inference Rule had been proposed instead, it would blame one branch for having a wrong type, which may be confusing and go against programmer intent. We can read Typing Rules either from top to bottom or from bottom to top, with slightly different interpretations. Reading from top to bottom describes how to use the information for Type Checking. Reading from bottom to top describes how a Type Inference Algorithm works, e.g., what needs to be Checked to Infer the Type of a Term. From a historical perspective, there are two Design Philosophies for Type Systems. The first is to augment a Programming Language with more information, such as C which uses it to determine how much space a variable would take up in memory. The second is to express programmer intent. Type Annotations (Ascriptions) for Parameters are required for Functions that are not immediately used, such as Top Level Functions. However, it is helpful to omit Type Annotations (Ascriptions) for Parameters for immediately used Lambda Terms within Higher Order Functions.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Reading: \"Types and Programming Languages\" Chapter 9 and Chapter 11","slug":"Paper-Reading-Types-and-Programming-Languages-Chapter-9-and-Chapter-11","date":"2023-01-25T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2023/01/25/Paper-Reading-Types-and-Programming-Languages-Chapter-9-and-Chapter-11/","link":"","permalink":"https://abbaswu.github.io/2023/01/25/Paper-Reading-Types-and-Programming-Languages-Chapter-9-and-Chapter-11/","excerpt":"","text":"Summary Chapter 9 of \"Types and Programming Languages\" presents the simply typed lambda calculus, which constructs a type system for pure lambda calculus, explaining theoretical aspects such as the typing relation and the Curry-Howard Correspondence along the way. Chapter 11 introduces simple extensions to the simply typed lambda calculus presented in Chapter 9, such as base types, derived forms, type ascriptions, let bindings, and some compound data structures (pairs, tuples, records, sums, variants, and lists), making it better resemble a real-world programming language. Critique Foreword I have found the textbook hard to follow in many places. Thus, I have followed the textbook and looked into many online resources to grasp the content. Below summarizes my understanding after studying the material. Basic Concepts in Type Theory Terms and Types In Type Theory, every Term has a Type, often written together as &lt;Term&gt;: &lt;Type&gt;. Types include Natural Numbers (nat) and Boolean Logic Values (bool). For example (assuming x: nat and y: nat): 0: nat x: nat 1 + 1: nat x + y: nat true: bool x + y: nat Functions Functions are also Terms with Types, represented as Lambda Terms. A Lambda Term looks like ( &lt;First Parameter Name&gt;: &lt;First Parameter Type&gt; &lt;Second Parameter Name&gt;: &lt;Second Parameter Type&gt; ... . &lt;Term to Return&gt;). It has type &lt;First Parameter Type&gt;  &lt;Second Parameter Type&gt;  ...  &lt;Type of Term to Return&gt;. This indicates that the Lambda Term is a function that takes Parameters of &lt;First Parameter Type&gt;, &lt;Second Parameter Type&gt;, etc., and returns a Term of &lt;Type of Term to Return&gt;. Examples of Lambda Terms: ( x: nat . (x + x)): nat  nat: a Function which takes in a Parameter x of Type nat and returns the doubled Parameter. ( x: nat y: nat . (x + y)): nat  nat  nat: a Function which takes in two Parameters x, y all of Type nat and returns their sum. A Lambda Term is often called an Anonymous Function because it has no Name. We can use the notion to give a Name to a Lambda Term: add: nat  nat  nat ::= ( x: nat y: nat . (x + y)) Function Applications In Type Theory, a Function Call is called a Function Application, which \"takes a Term of a Type and results in a Term of another Type.\" Function Application is written as &lt;Function&gt; &lt;Argument&gt; &lt;Argument&gt; ... (akin to Function Calls in Haskell and Commands in Unix Shell) instead of the conventional &lt;Function&gt;(&lt;Argument&gt;, &lt;Argument&gt;, ...) in Programming Languages. If we define a Function add that takes two nat's and returns a nat, the following are valid Terms: add 0 0: nat add 2 3: nat add 1 (add 1 (add 1 0)): nat Dependent Typing Sometimes, the Type returned by a Function depends on the Value of its Argument. This is known as Dependent Typing. For example, a function if takes three arguments, with if true b c returning b, and if false b c returning c. If b and c have different Types, then the type of if depends on the value of a. Dependent Typing is a reasonably complicated subject that is an active domain of research. Zero Type, Unit Type, and Universal Type Zero Type In some programming languages, there is a Zero Type or Bottom Type - a Type whose Set of Terms is the empty set and a Subtype of all other Types. In these programming languages, denoting the Zero Type as a Function's Return Type frequently indicates that the Function never returns (never completes computation) - instead, it may loop forever, throw an exception, or terminate the process. As a real-world example, in Rust, the Zero Type is called the Never Type and is denoted by !. It is the kind of calculation that never returns any result. For example, the exit function fn exit(code: i32) -&gt; ! terminates the process without returning. Unit Type In some programming languages, the Unit Type is a Type whose Set of Terms is a singleton set, i.e., the type allows only one value. It is typically used to describe the Argument Type of a Function that doesn't need arguments or the Return Type of a Function whose only goal is to have a side effect. For example: In Haskell, Rust, and Elm, the Unit Type is the Type of the 0-tuple (). In Python, the Unit Type is NoneType, which only has a single instance None. In JavaScript, both Null (which only has a single instance null) and Undefined (which only has a single instance undefined) are Unit Types. In languages such as C, C++, Java, and C#, void, which designates that a Function accepts no Arguments or does not return anything, plays a similar role to the Unit Type. However, there are also key differences: There are no Terms (Instances) of void. A proper Unit Type may always be the Type of an Argument to a Function, but void cannot be the Type of an Argument. Universal Type Most object-oriented programming languages include a universal base class. In Type Theory, this is known as a Universal Type or a Top Type. Its Set of Terms encompasses any valid Term in the programming language, and all other types in the programming language are subtypes. For example: Object in Smalltalk and JavaScript java.lang.Object in Java System.Object in C#, Visual Basic .NET, and other .NET Framework languages object in Python (can also be type-annotated as typing.Any) Any in Scala and Julia Some object-oriented programming languages, such as C++, Objective-C, and Swift, do not have a universal base class. In these languages, some constructs function similarly to the Universal Type. In C++, void * can accept any non-function pointer (even though void itself is more akin to the Unit Type). In Objective-C, id can accept pointers to any object. In Swift, the protocol Any can accept any type. Languages that are not object-oriented usually do not have a Universal Type. Typing Context A Typing Context (or Typing Environment) \\(\\Gamma\\) is a Mapping from Terms to Types (or a collection of Term - Type Pairs). The judgement \\(\\Gamma \\vdash e: \\tau\\) is read as \"\\(e\\) has type \\(\\tau\\) in Context \\(\\Gamma\\)\". In Statically Typed Programming Languages, these Typing Contexts are used and maintained by Typing Rules to Type Check a given Program or Expression. Type Inhabitation Given a Typing Environment, a Type is inhabitated if an existing Term of the Type is available or a Term of the Type can be readily obtained (i.e., via Function Application). Derived Forms In Type Theory, Syntactic Sugar is known as Derived Forms, while replacing a Derived Form with its lower-level definition (usually during compile time) is known as desugaring. For example: In C, a[i] and *(a + 1), a-&gt;x and (*a).x. In the tidyverse collection of R packages, x %&gt;% f(y) is equivalent to f(x, y). A programming language is typically divided into a compact core language, a rich set of syntax defined in terms of that core (Derived Forms), and a comprehensive standard library. This makes the language maintainable for engineers while making it convenient for users. Type Ascription Type Ascription is an assertion within source code that a term has a particular type. This can lead to cleaner, easier-to-understand code documentation. Important Derived Forms Tuple Record (Struct, Rows in a Database) - a collection of Fields, possibly of different Types Variant (Datatype, Tagged Union, Discriminated Union, Disjoint Union) A data structure to hold a Term that could take on \"several different, but fixed Types.\" Contains a Value field and a Tag field Widely used for defining recursive data structures (e.g. Trees containing Leaves and Internal Nodes) List Curry-Howard Correspondence The Curry-Howard Correspondence, independently discovered by logicians Haskell Curry in 1958 and William Howard in 1969, states that \"proofs in a given subset of mathematics are exactly programs from a particular programming language\". Specifically, Types correspond to logical formulas. A Term having a Type can be understood as evidence that the Type is inhabited. For example, 3110: int is evidence that int is inhabited. Logical Atoms \\(a\\), \\(b\\) correspond to whether Types A, B are inhabited. true corresponds to a Type that is always inhabited. The simplest of them all is the Unit Type. false corresponds to a Type that is never inhabited - the Zero Type. Conjunction \\(a \\land b\\) corresponds to a Type inhabited when both Types A and B are inhabited - Tuple[A, B]. Disjunction \\(a \\lor b\\) with the added condition that you know which one of \\(a\\), \\(b\\) is true when \\(a \\lor b\\) is true corresponds to a Type that is inhabited when one of A, B is inhabited, and you know which one is inhabited - Variant[A, B]. Implication \\(a \\rightarrow b\\) corresponds to a Type that, when inhibited, ensures B must be inhabited when A is inhabited - a Function Type, A -&gt; B. Programs correspond to proofs. Analyzing the types of expressions evaluated during the execution of a program corresponds to simplifying a proof. References https://en.wikipedia.org/wiki/Type_theory https://en.wikipedia.org/wiki/Bottom_type https://en.wikipedia.org/wiki/Typing_environment https://softwareengineering.stackexchange.com/questions/277197/is-there-a-reason-to-have-a-bottom-type-in-a-programming-language https://stackoverflow.com/questions/32505911/what-is-the-role-of-bottom-%E2%8A%A5-in-haskell-function-definitions https://doc.rust-lang.org/std/primitive.never.html https://en.wikipedia.org/wiki/Unit_type https://en.wikipedia.org/wiki/Top_type https://cs3110.github.io/textbook/chapters/adv/curry-howard.html#types-correspond-to-propositions https://wiki.haskell.org/Curry-Howard-Lambek_correspondence https://www.pdrot.fr/slides/inria-junior-02-15.pdf https://math.stackexchange.com/questions/2686280/what-do-logicians-mean-by-type https://homepages.inf.ed.ac.uk/stg/NOTES/node35.html https://cs.wellesley.edu/~cs251/s02/scheme-intro.pdf https://cs.brown.edu/~sk/Publications/Papers/Published/pk-resuarging-types/paper.pdf https://en.wikipedia.org/wiki/Syntactic_sugar https://www.wikidata.org/wiki/Q73072308 https://stackoverflow.com/questions/36389974/what-is-type-ascription https://github.com/rust-lang/rfcs/blob/master/text/0803-type-ascription.md https://medium.com/@andrew_lucker/things-you-cant-do-in-rust-type-ascription-5253951c7427 https://docs.scala-lang.org/style/types.html https://futhark-lang.org/examples/type-ascriptions.html https://en.wikipedia.org/wiki/Record_(computer_science) https://en.m.wikipedia.org/wiki/List_(abstract_data_type) Feedback from the Class Discussion An Introduction Rule describes how Elements of the Type can be Created, and is akin to a description of a Constructor. Similarly, an Elimination Rule describes how Elements of the Type can be used in an Expression, and is akin to a description of an Overloaded Operator. A lot of papers propose Typing Rules that don't make much sense in isolation, but can be plugged into other Type Systems to add a Feature (i.e., allow the non-intrusive addition of other Typing Rules). Well-designed Type Systems provide guarantees on a program's behavior (i.e., guarantee predictable runtime behavior). C introduced types, not for verification, but to determine how much space a variable would take up in memory. Uniqueness of Typing (i.e., a Term can only have one Type) doesn't hold when there is Subtyping. Curry Style allows representing errors explicitly and describing the type of errors, which is suitable for languages where things can go wrong. In comparision, Church Style does not allow errors The Erasure Property is built upon the assumption that the Execution of the Program doesn't rely on Types. Type Ascription woule be beneficial for giving hints to the Type Inference/Type Checking Algorithm. Usually, Desugaring happens before Type Checking, as the Type System does not directly handle the Syntactic Sugar. Tuples are also called Sum Types, and Variants are also called Product Types. This is based on how many possible values the Tuple or Variant Type has. For example, std::pair&lt;char, bool&gt; has 256 * 2 = 512 values, std::variant&lt;char, bool&gt; has 256 + 2 = 258 values, and std::optional&lt;char&gt; has 256 + 1 = 257 values. Enums can be seen as Variants where each value is associated with the Unit Type. Tuples and Records are distinct Types because Compilers implement them differently Programming in Dynamically Typed Programming is akin to programming with variables which are Variants of all possible types.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Modules Within PySide6 for Cross-platform GUI Building and System API Calling","slug":"Modules-Within-PySide6-for-Cross-platform-GUI-Building-and-System-API-Calling","date":"2023-01-02T08:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2023/01/02/Modules-Within-PySide6-for-Cross-platform-GUI-Building-and-System-API-Calling/","link":"","permalink":"https://abbaswu.github.io/2023/01/02/Modules-Within-PySide6-for-Cross-platform-GUI-Building-and-System-API-Calling/","excerpt":"","text":"A Brief Introduction to Qt Qt is a C++ library with official Python bindings that allows creating GUIs and cross-platform applications targeting different software and hardware platforms, such as Linux, Windows, macOS, Android, or embedded systems, with little to no change to the underlying codebase, while still having native capabilities, speed, and \"look and feel\". There are several advantages of using Qt, including: Well-designed object-oriented framework. Excellent documentation. Large user base. Widely endorsed. Many industrial-grade cross-platform software use Qt, including Autodesk Maya, Autodesk 3ds Max, Google Earth, Mathematica, OBS Studio, QGIS, Sibelius, Teamviewer, VirtualBox, VLC media player, Wireshark, and WPS Office. Modules Within PySide6 To aid use, we have summarized the relevant modules of PySide6, Qt's official Python bindings. PySide6.QtCore: Qt's essential classes, including platform-independent command-line parsing, multithreading, date and time utilities, object serialization, etc. Most of these features have already been introduced into the C++ and Python standard libraries. However, we should pay attention to Qt's event handling mechanism - a mechanism for emitting events in the form of objects across threads (known as signals), and handling these events in designated functions (known as slots). Modules Related to Building GUIs The modules of PySide6 directly related to building GUIs are as follows: PySide6.QtGui: Classes used internally by Qt's user interface technologies, including classes for windowing system integration, event handling, OpenGL and OpenGL ES integration, 2D graphics, basic imaging, fonts, and text. PySide6.QtWidgets: Provides a set of UI elements to create classic desktop-style user interfaces. All UI elements, including user-defined ones, inherit from PySide6.QtWidgets.QtWidget. PySide6.QtMultimediaWidgets: Provides multimedia-related widgets and controls. PySide6.QtOpenGL, PySide6.QtOpenGLWidgets: Used to support the OpenGL widget class, which operates similarly to other Qt widgets with the exception that it opens an OpenGL display buffer whose contents can be rendered using the OpenGL API. PySide6.QtPdf, PySide6.QtPdfWidgets: Classes for rendering pages from PDF documents. PySide6.QtWebEngineCore, PySide6.QtWebEngineWidgets: Provides a Chromium web browser engine as well as C++ classes to render web content and interact with it. Modules Related to System API Calling Qt is more than a GUI framework. It also provides a cross-platform way of doing a lot of stuff that desktop applications often need to do - calling into system APIs, especially platform-specific multimedia APIs not covered by the POSIX standard and/or the C++ and Python standard libraries. Stock Image: Multimedia To better document this crucial yet often overlooked use case, we have summarized the relevant modules of PySide6. PySide6.QtBluetooth: Enables connectivity between Bluetooth enabled devices. Currently the API is supported to different degrees on Android, iOS, macOS, Linux, and Windows. PySide6.QtMultimedia: Provides APIs for rendering audio and video files on screen and playing them back, as well as a thorough API for recording audio and video via system cameras and microphones. PySide6.QtNfc: Provides APIs for dealing with NFC Forum Tags and NFC Forum Devices, including target identification and loss, the registration of NDEF message handlers, the reading and writing of NDEF messages on NFC Forum Tags, and the sending of tag-specific commands. PySide6.QtPositioning: Allows developers to locate themselves using a variety of sources, such as satellite, wifi, text files, and so on. The position on a map, for instance, can be determined using that information. Additionally, it is possible to retrieve satellite data and carry out area-based monitoring. Currently the API is supported on Android, iOS, macOS, Linux, and Windows (with GPS receivers exposed as a serial port providing NMEA sentences or using Windows.Devices.Geolocation). PySide6.QtPrintSupport: Offers broad cross-platform printing capability, including printing to attached printers, printing to remote printers over networks, and creating PDF files. PySide6.QtSensors: Provides access to sensor hardware. Currently the API is supported on Android, iOS, and Windows (MSVC). References: https://en.wikipedia.org/wiki/Qt_(software) https://www.quora.com/What-are-the-Pros-and-Cons-of-using-QT-framework-for-cross-platform-programming-Win-Mac https://wiki.qt.io/Qt_for_Python","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"},{"name":"PySide6","slug":"Code/Python/PySide6","permalink":"https://abbaswu.github.io/categories/Code/Python/PySide6/"}],"tags":[]},{"title":"Date and Time Types in Python","slug":"Date-and-Time-Types-in-Python","date":"2022-12-31T08:00:00.000Z","updated":"2023-11-06T07:31:36.161Z","comments":true,"path":"2022/12/31/Date-and-Time-Types-in-Python/","link":"","permalink":"https://abbaswu.github.io/2022/12/31/Date-and-Time-Types-in-Python/","excerpt":"","text":"There are many types in Python which can store date and time information. These types can be broadly divided into two categories: JSON Serializable Formats UNIX Timestamp (e.g. 0) ISO 8601 String (e.g. '1970-01-01T00:00:00') UNIX Timestamp has its roots in the system time of Unix operating systems. It is now widely used in databases, programming languages, file systems, and other computer operating systems. It counts the number of seconds that have passed since the Unix epoch began on January 1, 1970 at 00:00:00 UTC, minus any modifications made for leap seconds. ISO 8601 is an international standard for the transmission and interchange of time- and date-related information on a global scale. Dates in the Gregorian calendar, hours based on the 24-hour timekeeping system, with an optional UTC offset, time intervals, and combinations of these are covered by ISO 8601. The standard offers a clear, unambiguous manner of expressing calendar dates and times in international communications, notably to prevent numeric dates and times from being misinterpreted when such data is sent between nations. As the categorization suggests, these formats can be used in JSON serialization, and are widely adopted in data exchange formats and APIs. For example, Stripe APIs use UNIX Timestamps, while Twitter and Dropbox APIs use ISO 8601 Strings. UNIX Timestamps are easier and more efficient to handle, while ISO 8601 Strings have the virtue of being human-readable. Widely Used In Memory Data Structures datetime.datetime (e.g. datetime.datetime(1970, 1, 1, 0, 0)) datetime.date (e.g. datetime.date(1970, 1, 1)) pandas.Timestamp (e.g. Timestamp('1970-01-01 00:00:00')) As the categorization suggests, these formats are in-memory, structured representations of date and time information. datetime.datetime and datetime.date are types implemented (and widely used) in the Python Standard Library. datetime.date represents a date (year, month, day) in an idealized calendar, which is the existing Gregorian calendar infinitely stretched in both directions, while datetime.datetime also combines the data from a time object (hour, minute, second, microsecond). pandas.Timestamp is implemented in pandas. It is the pandas replacement for datetime.datetime, and is the type used for the entries that make up a pandas.DatetimeIndex, and other time series-oriented data structures in pandas. Furthermore, it is also widely used across the Python Ecosystem for Data Science, such as being used by matplotlib as the xticks for plotting a pandas.Series with a pandas.DatetimeIndex, as shown below. pandas.Timestamp is the type used for the entries that make up a pandas.DatetimeIndex, and is used by matplotlib as the xticks for plotting a pandas.Series with a pandas.DatetimeIndex Converting Between These Types With so many types in Python which can store date and time information, it is important to know how to convert between them. The following State Diagram depicts how we should perform the conversions. stateDiagram state &quot;UNIX Timestamp&quot; as UNIXTimestamp: 0 state &quot;ISO 8601 String&quot; as ISO8601String: &#x27;1970-01-01T00:00:00&#x27; state &quot;datetime.datetime&quot; as DatetimeDatetime: datetime.datetime(1970, 1, 1, 0, 0) state &quot;datetime.date&quot; as DatetimeDate: datetime.date(1970, 1, 1) state &quot;pandas.Timestamp&quot; as PandasTimestamp: Timestamp(&#x27;1970-01-01 00:00:00&#x27;) UNIXTimestamp --&gt; DatetimeDatetime: datetime.datetime.fromtimestamp function UNIXTimestamp --&gt; PandasTimestamp: constructor ISO8601String --&gt; DatetimeDatetime: isoformat method ISO8601String --&gt; PandasTimestamp: constructor DatetimeDatetime --&gt; UNIXTimestamp: datetime.datetime.timestamp function DatetimeDatetime --&gt; ISO8601String: datetime.datetime.fromiso function DatetimeDatetime --&gt; DatetimeDate: date method DatetimeDatetime --&gt; PandasTimestamp: constructor DatetimeDate --&gt; PandasTimestamp: pandas.Timestamp constructor PandasTimestamp --&gt; UNIXTimestamp: timestamp method PandasTimestamp --&gt; ISO8601String: isoformat method PandasTimestamp --&gt; DatetimeDatetime: to_pydatetime method References: https://en.wikipedia.org/wiki/Unix_time https://en.wikipedia.org/wiki/ISO_8601 https://dev.to/xngwng/do-you-prefer-unix-epoch-a-number-or-iso-8601-a-string-for-timestamps--28ll https://stackoverflow.com/questions/15554586/timestamps-iso8601-vs-unix-timestamp https://www.dataquest.io/blog/tutorial-time-series-analysis-with-pandas/ https://www.programiz.com/python-programming/datetime/timestamp-datetime https://stackoverflow.com/questions/3743222/how-do-i-convert-a-datetime-to-date https://stackoverflow.com/questions/969285/how-do-i-translate-an-iso-8601-datetime-string-into-a-python-datetime-object https://www.programiz.com/python-programming/datetime/timestamp-datetime https://pynative.com/python-iso-8601-datetime/ https://docs.python.org/3/library/datetime.html https://stackoverflow.com/questions/1937622/convert-date-to-datetime-in-python https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html https://stackoverflow.com/questions/993358/creating-a-range-of-dates-in-python https://stackoverflow.com/questions/41046630/set-time-formatting-on-a-datetime-index-when-plotting-pandas-series","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"}],"tags":[]},{"title":"Social Media Strategy (tentative)","slug":"Social-Media-Strategy-tentative","date":"2022-12-31T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/12/31/Social-Media-Strategy-tentative/","link":"","permalink":"https://abbaswu.github.io/2022/12/31/Social-Media-Strategy-tentative/","excerpt":"","text":"Social media accounts are windows that exhibit our image to the public, and we must pay attention to them. However, on the one hand, the use of social media should complement, and not negatively affect, our routine work, study, and life patterns; on the other hand, different social media platforms generally differ in terms of appropriate content to display. After a period of observation and reflection, we have developed the following social media strategy to address these issues, as depicted in the Bipartite Graph below. stateDiagram state &quot;Life Moments&quot; as LifeMoments state &quot;Reflections on Life&quot; as ReflectionsOnLife state &quot;Work Moments&quot; as WorkMoments state &quot;Reflections on Work&quot; as ReflectionsOnWork state &quot;Reflections on Development&quot; as ReflectionsOnDevelopment state &quot;Detailed Explanations&quot; as DetailedExplanations state &quot;Planning&quot; as Planning state &quot;Instagram&quot; as Instagram state &quot;&quot; as XiaoHongShu state &quot;WeChat&quot; as WeChat state &quot;QQ&quot; as QQ state &quot;LinkedIn&quot; as LinkedIn state &quot;Blog&quot; as Blog LifeMoments --&gt; Instagram LifeMoments --&gt; XiaoHongShu LifeMoments --&gt; WeChat LifeMoments --&gt; QQ ReflectionsOnLife --&gt; XiaoHongShu ReflectionsOnLife --&gt; Twitter ReflectionsOnLife --&gt; WeChat ReflectionsOnLife --&gt; QQ WorkMoments --&gt; Twitter WorkMoments --&gt; WeChat WorkMoments --&gt; QQ ReflectionsOnWork --&gt; Twitter ReflectionsOnWork --&gt; WeChat ReflectionsOnWork --&gt; QQ ReflectionsOnDevelopment --&gt; LinkedIn DetailedExplanations --&gt; Blog Planning --&gt; Blog In addition, a pain point we will encounter is that we will post some content to both Chinese and English social media platforms. To overcome this problem, we can first write a Chinese (or English) version and use automated tools such as Google Translate and DeepL before manually touching up the machine-translated version. Furthermore, to boost the following of our social media accounts, when we need to share the content posted on social media with others via private chat, we can share the link of the content posted to social media instead of copying and pasting the content itself.","categories":[{"name":"Planning","slug":"Planning","permalink":"https://abbaswu.github.io/categories/Planning/"}],"tags":[]},{"title":"Command-line HTTP Servers for Rapid File Sharing","slug":"Command-line-HTTP-Servers-for-Rapid-File-Sharing","date":"2022-12-26T08:00:00.000Z","updated":"2023-11-06T07:31:36.161Z","comments":true,"path":"2022/12/26/Command-line-HTTP-Servers-for-Rapid-File-Sharing/","link":"","permalink":"https://abbaswu.github.io/2022/12/26/Command-line-HTTP-Servers-for-Rapid-File-Sharing/","excerpt":"","text":"Sometimes, we need an ad-hoc, quick-and-dirty way of sharing files with others while maintaining complete control of the data transmission. We should not store our data on any third-party servers. The connections established through the network should be point-to-point. Being supported by virtually every Internet-capable device and from both command-line tools (such as wget and curl) and graphical Web browsers, the HTTP protocol is one of our best bets. Thus, we have compiled a list of command-line HTTP servers that enable rapid file sharing and compare their features. python3 -m http.server The Python standard library has a barebones built-in HTTP server. Not recommended for production. Language: Python mjpclab/go-http-file-server Simple command line based HTTP file server to share local file system. Features: Cross-Origin Resource Sharing (CORS) Frontend Features: File Upload File Delete Create Subdirectory Download the Current Directory as an Archive HTTP Basic Authentication HTTP Range Requests HTTP Strict Transport Security (HSTS) HTTPS GitHub Stars: 175 Language: Go mar10/wsgidav A generic and extendable WebDAV server written in Python and based on WSGI. Features: HTTP Range Requests GitHub Stars: 640 Language: Python Multithreaded WebDAV Server TheWaWaR/simple-http-server Screenshot Features: Cross-Origin-Embedder-Policy (COEP) Cross-Origin Resource Sharing (CORS) Cross-Origin-Opener-Policy (COOP) HTTP Basic Authentication HTTP Range Requests HTTPS Frontend Features: File Upload GitHub Stars: 785 Language: Rust Multithreaded http-party/http-server http-server is a simple, zero-configuration command-line static HTTP server. It is powerful enough for production usage, but it's simple and hackable enough to be used for testing, local development and learning. Features: Cross-Origin Resource Sharing (CORS) HTTP Basic Authentication HTTP Range Requests HTTPS GitHub Stars: 12.4k Language: node.js EstebanBorai/http-server Simple and configurable command-line HTTP server Features: Cross-Origin Resource Sharing (CORS) HTTP Basic Authentication HTTPS GZip Compression","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"}],"tags":[]},{"title":"Paper Reading: Efficient scalable thread-safety-violation detection: finding thousands of concurrency bugs during testing","slug":"Paper-Reading-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing","date":"2022-11-27T08:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/11/27/Paper-Reading-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing/","link":"","permalink":"https://abbaswu.github.io/2022/11/27/Paper-Reading-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. This paper presents Thread Safety Violation Detection (TSVD), a tool that dynamically detects thread safety violations with low runtime overhead, and which is compatible with real-world, distributed-developed code employing different synchronization mechanisms. The tool frames thread safety violations as two methods, with one of them being a write operation, occurring concurrently. It infers thread safety violations using a very creative approach. First, it instruments the program and detects method calls that access objects behind thread-safety contracts. Later on, during the execution of the program, TSVD injects delays into threads with method calls accessing those objects and monitors whether another thread also accesses the same objects during the delay. As this may incur significant overhead, the tool uses two strategies to determine when to inject delays - keeping track of \"near misses\", where the two method calls of two threads occur within a time threshold apart from each other, and inferring \"happens before\" relations, to rule out two accesses which are causally related. The tool was tested on 43000 .NET programs in Microsoft teams, and its bug-finding capability outperformed both existing tools and configuring TSVD to emulate the strategies of existing tools, which shows the feasibility of TSVD. There are two questions that come to mind after reading this paper: How does the tool acquire the information on which methods are thread-unsafe? The approach the tool uses to infer thread safely - injecting delays and monitoring the behavior of other threads - sounds very interesting to me. Have there been any other applications of such an approach? What is the sensitivity of the relevant parameters used in TSVD to its effectiveness and efficiency? Is there any guide on how to properly adjust these parameters? Feedback from the Class Discussion The proposed approach can handle different concurrency models, such as: async task-based thread-based But can it handle unstructured concurrency? The approach generalizes data race for objects and data structures at the method-level (e.g. there cannot be two simultaneous calls to add() for a List class). Using delays can handle many more cases than reasoning about thread scheduling. It is a \"simple thing\" which works for many cases (akin to fuzzing). The approach requires manually specifying read and write APIs. Is it possible to create a semi-automatic approach starting from contracts labeled for standard library APIs?","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Hybrid dynamic data race detection","slug":"Paper-Reading-Hybrid-dynamic-data-race-detection","date":"2022-11-23T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/11/23/Paper-Reading-Hybrid-dynamic-data-race-detection/","link":"","permalink":"https://abbaswu.github.io/2022/11/23/Paper-Reading-Hybrid-dynamic-data-race-detection/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. The paper proposes a hybrid approach to dynamically (at runtime) data races in multithreaded Java programs. It first proposes two specific detection approaches, each with its strengths and weaknesses. The first is lockset-based detection, which identifies a data race when multiple threads use a shared memory location without holding a shared lock object. Such an approach is fast but may lead to false positives. As a result, the paper proposes another approach, happens-before detection, which uses several heuristics to reason about relations between events and infer whether a potential race has occurred at a particular memory location. In comparison, this approach is more computationally expensive and may lead to false negatives. Considering that neither approach is sound, they combine the two approaches by first using lockset-based detection to identify potential data races before using happens-before detection to reason whether these are probable. The paper then conducts an experimental study of their hybrid approach on various Java programs, demonstrating its effectiveness and efficiency. I like this paper's idea of combining a pessimistic and optimistic approach when doing program analysis. Are there any other works that use such an idea? However, I have a question concerning the applicability of the hybrid approach in real life. Although pessimistic, shouldn't lockset-based detection be enough to stamp out all potential data races by providing programmers with feedback to add relevant locks to prevent such possible data races? This is relevant to the requirements for defensive programming. Or are there design patterns where multiple threads can safely use a shared memory location without holding a common lock and not lead to data races? Feedback from the Class Discussion Difference Between Race Condition and Data Race: - Race Condition: There are multiple threads, and the behavior of program depends on thread scheduling. - Data Race: Different from race condition. This frequently happens when you parallelize a program that shouldn't be parallelized. Data race can be solved by using locks, but there may still be race coditions. 12345Thread-1:synchronized(...) &#123; x = 1;&#125; 12345Thread-2:synchronized(...)&#123; x = 2;&#125; Modelling in the Paper: - Lamport Timestamps/Vector Clocks - Thread events: statement executions in threads. A thread event is dependent on previous thread events. This is captured used using the happens-before formal definition in the paper, but leads to false negatives. - Thread communications: signals (enforce order) and locks (mutually exclusive). - Message send/receive: enqueue and dequeue. Architecure-dependednt atomic operations can also be a lock-free solution (e.g. C++'s atomic).","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Lightweight Verification of Array Indexing","slug":"Paper-Reading-Lightweight-Verification-of-Array-Indexing","date":"2022-11-16T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/11/16/Paper-Reading-Lightweight-Verification-of-Array-Indexing/","link":"","permalink":"https://abbaswu.github.io/2022/11/16/Paper-Reading-Lightweight-Verification-of-Array-Indexing/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Summary of the Paper The authors propose a methodology to detect out-of-bound array accesses statically. They first define that criteria that ideal techniques for detecting out-of-bound array accesses should satisfy, before analyzing the insufficiency of existing academic and industrial approaches, and presenting their own approach, Index Checker, implemented for Java. Index Checker reduces checking array bonds to identifying 7 kinds of knowledge, which concern array index and array length, and form a hierarchy. It models such hierarchical knowledge as a Type System, requires the user to write \"Type\" Annotations at procedure boundaries, and verifies that values have the given \"Type\" at runtime. This is implemented using Checker Framework, an \"industrial-strength, open-source tool for building Java type systems\". The authors evaluate Index Checker on 3 large-scale, well-tested Java projects (Google Guava, JFreeChart, Plume-lib), and compare Index Checker with 3 other approaches (FindBugs, KeY, and Clousot), proving the effectiveness of Index Checker (scalability, finding bugs in well-tested programs, and low false positive rate). They also assess the burden of writing type annotations for Index Checker. Questions What is the rationale behind the 7 kinds of knowledge concerning array index and array length proposed in the paper? I am not very familiar with Type Theory, which may have impeded my understanding of the value of the paper. What are the benefits of using Type Systems and Type Inference, and using Type Annotations to capture known constraints? Is it just to leverage the power of Checker Framework, an \"industrial-strength, open-source tool for building Java type systems\", for sound inference? Or are there any further benefits? No matter what the benefits are, from this paper, modeling hierarchical knowledge as a Type System, using Type Annotations to capture known constraints, and using Type Inference to verify such constraints sounds like a very innovative technique with many potential use cases. Have there been any other applications of such a technique? Feedback from the Class Discussion The hierarchy of knowledge is derived from Exploratory Data Analysis (trying stuff until it works, see Section 2.8). \"Subtype\" is a kind of Comparable Partial Ordering ('&lt;'). The Types in the Bottom have more information, while the Types in the Top have less information. In Java, aside from Inheritance, another form of Subtyping is Function Subtyping. e.g. Comparator (to compare two Dog's we can pass a function that compares two Animal's) the inputs can be more general types. Rules define what to do when a Pattern is encountered; however, it takes a (nontrivial) search to determine the order to apply the rules. Fixed Point: Convergence of Information. Reach a Fixed Point: Iterate until Convergence. The Paper uses Subtyping to implement Widening.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Modular Checking for Buffer Overflows in the Large","slug":"Paper-Reading-Modular-Checking-for-Buffer-Overflows-in-the-Large","date":"2022-11-13T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/11/13/Paper-Reading-Modular-Checking-for-Buffer-Overflows-in-the-Large/","link":"","permalink":"https://abbaswu.github.io/2022/11/13/Paper-Reading-Modular-Checking-for-Buffer-Overflows-in-the-Large/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Background Information Datalog Declarative Programming Language which began as a Query Language for Relational Databases, and is now used in Data Integration, Information Extraction, Program Analysis, Cloud Computing, Machine Learning, etc. Akin to SQL in many aspects. Not Turing Complete. Used as a Domain Specific Language. No Canonical Implementation, many different Implementations exist for different Applications (c.f. SQLite, MySQL, PostgreSQL, etc. for SQL). Follows the 'Logic Programming' Paradigm. A Program consists of Constants, Variables, Facts, and Rules (based on First Order Logic, in a form similar to \"a new Fact A is true if B, C, and D are already known to be true\"). The Execution of a Program is iteratively inferring new Facts given the Rules. Maps very nicely to many problems encountered during Program Analysis. Summary of the Paper The authors proposed a Methodology for detecting possible Buffer Overflow-based Security Exploits in C code and providing developers with instant feedback during the build process. The Methodology prefers usability over accuracy, and should be used alongside other tools in a Swiss Cheese Model against Security Exploits. First, the authors proposed a Simple Annotations Language for annotating Pointers passed as parameters to and returned from Functions, to denote Preconditions and Postconditions of Function Execution. The authors propose that for new code, annotation should be inserted manually, and code should be fully annotated before being checked in to Version Control. For legacy codebases and/or third-party code without such Annotations, the authors propose an Inference Engline, SALInfer, which tries to infer such Annotations, preferring Coverage over Accuracy. SALInfer supports specifying Inference Algorithms using Datalog. Finally, the authors propose a modular checker, ESPX, which tries to infer if a program is potentially vulnerable to Buffer Overflow-based Security Exploits by statically analyzing the annotations within the program's code. The confidence of the inference results vary based on the extent and quality of the annotations. Questions Regarding the Paper What is the relevance of such a technique to \"safe\" programming languages that do not allow using overflowable buffers? The authors state that \"control over annotation insertion is given to individual developers\". However, developers might be reluctant to insert Annotations, and inserting Annotations can negatively affect developer productivity. Furthermore, the quality of the inserted Annotations is not guaranteed. Last but not least, inferring Annotations for legacy codebases and/or third-party code without such Annotations prefers Coverage over Accuracy, which may not lead to sound results. Considering all these real concerns, the practical usability of this tool is seriously compromised. The authors did an evaluation on an unnamed Microsoft product. With little information regarding the product being disclosed, such an evaluation is far from convincing, and I suspect that there might be manipulation of some kind within the evaluation.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Precise Interprocedural Dataflow Analysis via Graph Reachability","slug":"Paper-Reading-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability","date":"2022-11-07T08:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/11/07/Paper-Reading-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability/","link":"","permalink":"https://abbaswu.github.io/2022/11/07/Paper-Reading-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. To be honest, I found the paper to be almost unreadable due to it being full of unfamiliar concepts and abstract formalizations. I tried my best to do some studying into the topic so that I can understand the problem that they are trying to solve, and important aspects of their algorithm, better. Graph Reachability Graph Reachability means whether it is possible to get from one vertex to another vertex within a graph. In an Undirected Graph \\(G(V, E)\\), Graph Reachability between one pair of nodes can be calculated using Breadth-First Search, while Graph Reachability between all pair of nodes can be reduced to calculating the Connected Components of the Undirected Graph, which is an efficient algorithm with \\(O(|V| + |E|)\\) time complexity. Connected Components within an Undirected Graph In a Directed Graph, Graph Reachability between one pair of nodes can also be calculated using Breadth-First Search. However, there is no efficient algorithm that can calculate Graph Reachability between all pair of nodes for all Directed Graphs. For any Directed Graph, calculating Graph Reachability between all pair of nodes can be reduced to calculating All Pairs Shortest Distance using the Floyd-Warshall Algorithm, which has an \\(O({|V|}^3)\\) time complexity. More efficient algorithms are only applicable to Planar Directed Graphs. Data Flow Analysis Constant Propogation (determining whether variables at a given point in the program are guaranteed to have constant values) and Live Variable Analysis (determining at a given point in the program, which variables might be used before being overwritten) are two commonly encountered examples of Data Flow Analysis. Given a program's Control Flow Graph, Data Flow Analysis: Associates each Node of the Control Flow Graph with Information concerning the Variables within that Node (known as Dataflow Fact's, usually a Mapping between Variables and their Values or Properties) Models the effect of executing a Node with a Dataflow Function. In most Data Flow Analysis problems, we take one of the following approaches to obtain the Dataflow Facts for each Node: Summarizing paths entering the Node from the Start, such as in Constant Propogation. Known as \"Forward Problem\"'s. Summarizing paths exiting the Node from the Exit, such as in Live Variable Analysis. Known as \"Backward Problem\"'s. How we summarize paths is known as the Confluence Operator. Data Flow Analysis problems can also be divided into \"may\" problems and \"must\" problems. In \"may\" problems, the Dataflow Facts for each Node include information about what may be true. An example is Live Variable Analysis, where we determine whether a variable may be used before being overwritten in a given point in the program. In \"must\" problems, the Dataflow Facts for each Node include information about what must be true. An example is Constant Propogation, where we determine whether a variable must have a given value in a given point in the program. Many interesting Data Flow Analysis problems, such as Live Variable Analysis, can be modeled as GEN/KILL problems, or bit-vector problems, in which: A set of variables, \\(KILL[n]\\), is defined at Node \\(n\\). A set of variables, \\(GEN[n]\\), is used at Node \\(n\\). We use Union or Intersection to summarize paths entering a Node to obtain the Dataflow Facts for the Node. Interprocedural Dataflow Analysis The goal of Interprocedural Dataflow Analysis is to capture an Abstraction of the Effect of calling a Procedure in Dataflow Analysis. A naive approach to Interprocedural Dataflow Analysis is to reduce it to Intraprocedural Dataflow Analysis in some way. Procedure Inlining Exponentially increases the Control Flow Graph Cannot handle recursion Context Sensitive Procedure Inlining Uses Context Information (often an Approximation of the Call Stack) to distinguish between different Calls of the same Procedure, and reduce the number of inlined Procedures. However, even after research, I have failed to understand the more complicated approaches (as well as the approaches proposed in this Paper). I can only get the point that the author shows that many Interprocedural Dataflow Analysis problems, in which: A finite set of Dataflow Facts Dataflow Functions distribute over the Confluence Operator (which I don't fully understand) including GEN/KILL problems, or bit-vector problems, can be reduced to a Graph Reachability Problem on a Directed Graph. Furthermore, I believe the main contribution of this paper is theoretical, but what is its value in real-world Dataflow Analysis problems, especially considering that the Time Complexity of Graph Reachability Problems on Directed Graphs are high? I honestly hope that I can get some insight into these approaches during our class on Monday. Thank you! Feedback from the Class Discussion Some of the paper's idea comes from Abstract Interpretation. It is nice theoretically, but it is far from implementation. Graph Reachability in the context of Interprocedure Analysis is also known as Context-Free Language Reachability and Dyck Reachability. In the context of this paper, there are multiple Dataflow Functions, one for each Node in the Control Flow Graph. Given a Node in the Control Flow Graph, we use Pattern Matching to determine what its Dataflow Function is. Lambdas are used to represent these Dataflow Functions. Explanation for the notations: \\(\\lambda &lt;parameters&gt;.&lt;return\\_value&gt;\\) means def f(&lt;parameters&gt;): return &lt;return_value&gt;. In the context of this paper, we require all Dataflow Functions to be distributive over the Meet Function (Confluence Function). This means that, given the Meet Function \\(\\Pi\\) and a Dataflow Function \\(f\\), \\(f(X~\\Pi~Y) = f(X)~\\Pi~f(Y)\\) for any two Dataflow Facts \\(X, Y\\). Each Dataflow Function can be visualized using a Graph Representation. The Edges represent Dependencies between Facts of the Variables in the Old Dataflow Facts and Facts of the Variables in the New Dataflow Facts. Graph Representation of Dataflow Functions, x is Shorthand for the Facts of x Worklist Algorithm: an Algorithm which takes Objects from a Worklist (a Queue of some sort) one at a time, processes it in some way, and perhaps further adds new Objects to the Worklist, until some Target is reached. Example: Breadth First Search.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Mining Input Grammars from Dynamic Taints","slug":"Paper-Reading-Mining-Input-Grammars-from-Dynamic-Taints","date":"2022-11-02T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/11/02/Paper-Reading-Mining-Input-Grammars-from-Dynamic-Taints/","link":"","permalink":"https://abbaswu.github.io/2022/11/02/Paper-Reading-Mining-Input-Grammars-from-Dynamic-Taints/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. A program usually accepts a formal language as input. Inferring the grammar of this formal language is an important task with many use cases. Helps humans understand the structure of the formal language. Manually writing valid inputs Reverse Engineering Generate inputs for testing and fuzzing The authors propose Autogram, a method that infers a Context Free Grammar given a set of sample inputs and a Java program that accepts that set of inputs and uses it in some way. Autogram adapts a Dynamic Taining-based approach: It monitors the data flow of each character within the input, with \"the Input Fragment it came from\" as the taint. It traces method entries, method exits, field accesses, and array accesses within the execution of the program. From such a trace, the Dynamic Call Tree is reconstructed, and the sets of Intervals (Input Fragments) processed by functions, stored in variables, and returned by functions is derived. This is used to build an Interval Tree, and the Interval Tree is refined into a Pure Input Tree free of conflicting overlaps (resulting from parsers using lookaheads). The pure input tree is assumed to be a Parse Tree, and Production Rules are derived from it. The leaf nodes are considered to be Terminals, and Regular Expressions matching them are learned. The authors then conduct an experimental study concerning the accuracy and completeness of the inferred Context Free Grammars using \"parts of the Java Standard API that are used to process URLs and property files\", and \"open source projects that implement support for CSV, INI, and JSON formats\". However, I had more questions than answers after reading this paper. One of the use cases that the authors mentioned is \"the grammar vastly simplifies the creation of parsing programs that decompose existing inputs into their constituents\". Why don't we directly extract the parsing logic out of the program Autogram runs on? The type of Context Free Grammar inferred by Autogram seems to be an LL(1) Grammar. This type of Grammar is only able to represent simple Grammars, and does not support for Left Recursion, which is pervasive in real-world Grammars. Why don't they infer an LALR(1) Grammar, which is both simple and expressive (it supports representing may real-world Programming Languages). Perhaps, a Hidden Markov Model could be trained to infer the Transitions between the States within the LALR(1) Parse Table should an LALR(1) Grammar be inferred? In the current implementation of Autogram, tracing is efficient, as the authors have mentioned: \"millions of calls result in traces of a few Megabytes\". However, the current implementation incurs a ~100x performance overhead, and there is a lot of room for performance optimization. Maybe ideas that we have discussed for TaintCheck and Qsym (direct Binary Analysis, preinstrumenting Bytecode, JIT compilation etc.) could be used here? The specific process of refining an Interval Tree into a Pure Interval Tree free of conflicting overlaps is not described clearly in the paper. Why don't the author present an example with figures showing the manipulation of nodes within the Interval Tree during this process? The author also mentions applying \"a simple heuristic that assumes left to right processing of the input\" to resolve possible ambiguities associated with parsers using lookaheads. However, what is the rationale behind this \"simple heuristic\"? The specific process of deriving Production Rules from the Pure Interval Tree is also unclear. What do the authors mean by \"We can thus check if nodes are compatible and can be used to derive productions for for the same nonterminal symbol\"? What is the meaning of \"compatible\" in this context? The programs used in the experimental study are all open-source programs of very high code quality (containing accurately named variables and functions). However, how well does Autogram work with closed-source programs, and/or programs with low code quality, containing obscure variable and function names? This is frequently the situation we encounter when we try to reverse engineer the (often closed-source and/or obscure) structure of a program's input, one of the major use cases of Autogram. Also some inspiration and ideas I got from the paper: The author mentions that \"dynamic tainting allows us to precisely identify which parts of a programs input are read, stored and processed at any point in time\". Could this technique be used in a Fuzzing context to identify which bits generated by a Coverage-Guided Fuzzer are used in which sections of a fuzzed program? The logic of building an Interval Tree is very interesting, and it reads like the \"Subset Tree\" mentioned in the KLEE paper. I conject that both these Tree Structures could be generalized and used in a much wider range of contexts. Feedback from the Class Discussion A Context Free Grammar may not capture the structure of binary files. How does the approach compare to unsupervised parsing in NLP or fine-tuning language models, especially with a lot of input? Is it possible to use feedback to improve the mined grammar? From one tree, we infer one set of grammar rules; from 1000 trees, we infer 1000 sets of grammar rules. They are merged together to derive the final Context Free Grammar.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Various Solutions for Different Types of Reverse Proxying","slug":"Various-Solutions-for-Different-Types-of-Reverse-Proxying","date":"2022-10-30T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/30/Various-Solutions-for-Different-Types-of-Reverse-Proxying/","link":"","permalink":"https://abbaswu.github.io/2022/10/30/Various-Solutions-for-Different-Types-of-Reverse-Proxying/","excerpt":"","text":"There are some situations in which we have to expose a locally running web service to the Internet. This is know as Reverse Proxying. Depending on the situation in hand, there are multiple ways to do this: Server with Public IP Available In this case, the Server is also known as a Jump Server. Client Accessible from Server Run a port-forwarding tool such as socat on the Server. 1socat TCP-LISTEN:&lt;Port the Server listens on&gt;,fork,reuseaddr TCP:&lt;IP address of the Client&gt;:&lt;Port of the Service on the Client&gt; Client Inaccessible from Server Use nat-tunnel on both the Server and the Client. Or, use Reverse SSH Tunneling on the Client. Reverse SSH Tunneling Before Tunneling On the Server: Update the sshd config file (/etc/ssh/sshd_config). Set GatewayPorts to yes. Restart the SSH Service. Make sure the Port the Server listens on allows Inbound Traffic. To Tunnel On the Client: 1ssh [-f] [-N] [-T] -R &lt;Port the Server listens on&gt;:localhost:&lt;Port of the Service on the Client&gt; [How you connect to the Server (e.g. `-i key-pair.pem &lt;username&gt;@&lt;domain&gt;`)] -f tells the SSH to background itself after it authenticates, saving you time by not having to run something on the remote server for the tunnel to remain alive. -N if all you need is to create a tunnel without running any remote commands then include this option to save resources. -T useful to disable pseudo-tty allocation, which is fitting if you are not trying to create an interactive shell. -R tells the tunnel to answer on the remote side. Server with Public IP Unavailable Use a commercial service such as ngrok on the Client. References: https://en.wikipedia.org/wiki/Reverse_proxy https://www.kvm.la/1328.html https://blog.csdn.net/weixin_35867652/article/details/104362302 https://www.hostinger.com/tutorials/how-to-set-up-nginx-reverse-proxy/ https://stevessmarthomeguide.com/understanding-port-forwarding/ https://jfrog.com/connect/post/reverse-ssh-tunneling-from-start-to-end/ https://linuxhint.com/ssh-port-forwarding-linux/ https://www.ssh.com/academy/ssh/tunneling-example https://superuser.com/questions/1408427/remote-port-forwarding-through-a-jump-server https://unix.stackexchange.com/questions/436290/single-step-ssh-port-forwarding-not-working-but-only-works-when-ssh-port-forward?rq=1&amp;newreg=def5dfc9fb43466d8685fd7639eb17cc https://www.opensourceforu.com/2021/09/how-to-do-reverse-tunnelling-with-the-amazon-ec2-instance/ https://superuser.com/questions/1194105/ssh-troubleshooting-remote-port-forwarding-failed-for-listen-port-errors https://docs.hevodata.com/getting-started/connection-options/connecting-through-reverse-ssh/ https://www.youtube.com/watch?v=TZ6W9Hi9YJw https://blog.devolutions.net/2017/03/what-is-reverse-ssh-port-forwarding/ https://chenhuijing.com/blog/tunnelling-services-for-exposing-localhost-to-the-web/ https://johackim.com/how-to-expose-local-server-behind-firewall https://gabrieltanner.org/blog/port-forwarding-frp/ https://www.techiediaries.com/public-localhost/ https://superuser.com/questions/121435/is-it-possible-to-host-a-web-server-from-behind-a-nat/1360660 https://medium.com/tech-learnings/how-to-expose-a-local-server-to-the-internet-without-any-additional-tools-ae49e6b8fe93 https://serverfault.com/questions/282959/how-do-i-reach-my-internal-server-on-the-external-ip https://superuser.com/questions/624925/how-to-access-internal-valid-ip-through-internet","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"}],"tags":[]},{"title":"Paper Reading: Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software","slug":"Paper-Reading-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software","date":"2022-10-29T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/10/29/Paper-Reading-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software/","link":"","permalink":"https://abbaswu.github.io/2022/10/29/Paper-Reading-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. To combat worms spread by the Internet exploiting software vulnerabilities, the paper proposes TaintCheck, a dynamic taint analysis technique for automatic detection of exploits on software. Summary of TaintCheck: TaintCheck directly operates on an arbitrary executable and does not require its source code. It uses Valgrind to translate basic blocks being executed into Valgrind's RISC-like instruction set (UCode), inserts UCode instructions for instrumentation, and passes the modified UCode back to Valgrind for execution. TaintCheck by default considers data originating from the network as \"untrusted\" and taints it. It keeps track of \"the propagation of tainted data as the program executes\", which involves monitoring data movement instructions and arithmetic instructions, with the exception of constant functions such as xor eax, eax. To accomplish this, TaintCheck associates \"each byte of memory\" with a Taint data structure. Different instances of such a data structure are \"chained\" to record \"how tainted data is propagated\". TaintCheck checks whether tainted data is used in ways it considers illegitimate, such as being used as a return address, a function pointer, a format string, and (optionally) as an argument of a system call. When such illegitimate uses are detected, it is possible to collect information about a software vulnerability, especially \"the execution path from tainted data's entry and its use in a probable exploit\". The paper also proposes a new semantic-based automatic signature generation approach on top of TaintCheck. There are several questions that came to my mind when I was reading this paper: The paper mentions that \"the current implementation slows program execution between 1.5 and 40 times\", but also mentions that \"the prototype has not been optimized\", and proposes optimization techniques. Why didn't the authors implement these optimization techniques and conduct experiments on the optimized TaintCheck? There is no doubt that using Valgrind to translate basic blocks being executed into UCode greatly simplifies dynamic taint analysis on an arbitrary executable, as TaintCheck deals with an RISC-like instruction set instead of raw machine code. However, this incurs significant overhead. Would directly performing dynamic taint analysis on machine code at runtime using a dynamic binary instrumentation tool such as Intel Pin boost performance (like the case of QSym)? What about generating UCode, inserting instructions for instrumentation, and passing the modified UCode back to Valgrind before the executable is executed? What is the overhead of using the Taint data structure? Would the total size of all Taint data structures explode for long-running processes? And why do they use this Taint data structure, instead of using a conventional Data Flow Graph? What is the list of constant functions that TaintCheck supports? Is it representative, and is it extensible? Are the ways tainted data is used considered by TaintCheck to be illegitimate representative of real exploits? How well can TaintCheck discriminate from \"illegitimate\" uses with intentional uses, and/or uses with checks? Specifically, the paper mentions that TaintCheck can \"untaint the data immediately after it has been sanity checked\", but how is this situation detected? In the evaluation section, why are the benchmarks used in assessing \"compatibility and false positives\" different from those used in assessing \"attack detection\" on actual exploits? What does a \"signature\" look like, and how is it used to filter attacks? Feedback from the Class Discussion Performance is not a priority. The paper is more of a proof-of-concept, and even \"reads like a grant proposal\", especially Section 6. The \"taint data structure\" includes more information than a dataflow graph (snapshots of the stacks etc.). It can also use a \"memory arena\" instead of vanilla heap allocation to improve performance. Some of the detected attacks may not be present in \"safe\", managed languages. Due to the large overhead, the technique cannot be used to handle requests in production, but requests can be forked to it instead. Dynamic taint analysis can have applications outside of the security domain.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Python in a Functional Style: Closures, Generators, and Coroutines","slug":"Python-in-a-Functional-Style-Closures-Generators-and-Coroutines","date":"2022-10-28T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/28/Python-in-a-Functional-Style-Closures-Generators-and-Coroutines/","link":"","permalink":"https://abbaswu.github.io/2022/10/28/Python-in-a-Functional-Style-Closures-Generators-and-Coroutines/","excerpt":"","text":"Python in a Functional Style: Closures, Generators, and Coroutines Jifeng Wu 2022-10-28 Closures Generators Coroutines All Python functions are closures. Function code. Execution environment of function code (variables it depend on). A nested function can be returned. This is a common design pattern for creating tailored functions. 1234def get_greeting_function(name): def greeting_function(): print(f&#x27;Hello, &#123;name&#125;&#x27;) return greeting_function All Python functions are closures. Function code. Execution environment of function code (variables it depend on). A nested function can be returned. This is a common design pattern for creating tailored functions. 1234567&gt;&gt;&gt; function_greeting_a = get_greeting_function(&#x27;A&#x27;)&gt;&gt;&gt; function_greeting_a()Hello, A&gt;&gt;&gt;&gt;&gt;&gt; function_greeting_b = get_greeting_function(&#x27;B&#x27;)&gt;&gt;&gt; function_greeting_b()Hello, B Look into a closure's cell_contents: 12345678910111213&gt;&gt;&gt; function_greeting_a.__closure__(&lt;cell at 0x7f3c81849ca8: str object at 0x7f3c8185ac70&gt;,)&gt;&gt;&gt; function_greeting_a.__closure__[0]&lt;cell at 0x7f3c81849ca8: str object at 0x7f3c8185ac70&gt;&gt;&gt;&gt; function_greeting_a.__closure__[0].cell_contents&#x27;A&#x27;&gt;&gt;&gt; &gt;&gt;&gt; function_greeting_b.__closure__(&lt;cell at 0x7f3c81849c18: str object at 0x7f3c82f18e30&gt;,)&gt;&gt;&gt; function_greeting_b.__closure__[0]&lt;cell at 0x7f3c81849c18: str object at 0x7f3c82f18e30&gt;&gt;&gt;&gt; function_greeting_b.__closure__[0].cell_contents&#x27;B&#x27; Should an inner function use an outer function's local variable (instead of shadowing it), that local variable should be declared nonlocal within the inner function. Not using nonlocal: 1234567def outer_function(): string = &#x27;Hello&#x27; def inner_function(): # Shadows the local variable `string` of `outer_function` string = &#x27;World&#x27; inner_function() return string 12&gt;&gt;&gt; outer_function()&#x27;Hello&#x27; Should an inner function use an outer function's local variable (instead of shadowing it), that local variable should be declared nonlocal within the inner function. Using nonlocal: 12345678def outer_function(): string = &#x27;Hello&#x27; def inner_function(): # Uses the local variable `string` of `outer_function` nonlocal string string = &#x27;World&#x27; inner_function() return string 12&gt;&gt;&gt; outer_function()&#x27;World&#x27; Creating and returning a nested function based on a function argument is widely used in Python, called decorating a function. 1234567891011121314def cached(function): cache = &#123;&#125; def cached_function(*args): nonlocal function, cache if args in cache: print(f&#x27;Cache hit with args: &#123;args&#125;&#x27;) return cache[args] else: print(f&#x27;Cache miss with args: &#123;args&#125;&#x27;) result = function(*args) print(f&#x27;Writing f(&#123;args&#125;) =&gt; &#123;result&#125; to cache&#x27;) cache[args] = result return result return cached_function Python even has special syntatical support for this. 12345678@cacheddef fib(n): if n &lt; 1: return 0 elif n &lt; 2: return 1 else: return fib(n - 1) + fib(n - 2) 1234567891011121314151617In [4]: fib(5) Cache miss with args: (5,)Cache miss with args: (4,)Cache miss with args: (3,)Cache miss with args: (2,)Cache miss with args: (1,)Writing f((1,)) =&gt; 1 to cacheCache miss with args: (0,)Writing f((0,)) =&gt; 0 to cacheWriting f((2,)) =&gt; 1 to cacheCache hit with args: (1,)Writing f((3,)) =&gt; 2 to cacheCache hit with args: (2,)Writing f((4,)) =&gt; 3 to cacheCache hit with args: (3,)Writing f((5,)) =&gt; 5 to cacheOut[4]: 5 \\(O(n)\\) time complexity. LeetCode problem: Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. Example 1: 12Input: n = 3Output: [&quot;((()))&quot;,&quot;(()())&quot;,&quot;(())()&quot;,&quot;()(())&quot;,&quot;()()()&quot;] Example 2: 12Input: n = 1Output: [&quot;()&quot;] bg left contain We write a Context Free Grammar and analyze it: 12S -&gt; S S&#x27; | S&#x27; .S&#x27; -&gt; ( S ) | ( ) . https://mdaines.github.io/grammophone/# 123456789101112131415161718192021@cacheddef s_generator(number_of_parenthesis): print(f&#x27;s_generator(&#123;number_of_parenthesis&#125;)&#x27;) return_value = [] # s -&gt; ss . if number_of_parenthesis &gt;= 1: for ss_string in ss_generator(number_of_parenthesis): return_value.append(ss_string) # s -&gt; s ss . if number_of_parenthesis &gt;= 2: for i in range(1, number_of_parenthesis): for s_string, ss_string in itertools.product( s_generator(i), ss_generator(number_of_parenthesis - i) ): return_value.append(s_string + ss_string) return return_value 123456789101112131415@cacheddef ss_generator(number_of_parenthesis): print(f&#x27;ss_generator(&#123;number_of_parenthesis&#125;)&#x27;) return_value = [] # ss -&gt; ( ) . if number_of_parenthesis == 1: return_value.append(&#x27;()&#x27;) # ss -&gt; ( s ) . if number_of_parenthesis &gt; 1: for s_string in s_generator(number_of_parenthesis - 1): return_value.append(&#x27;(&#x27; + s_string + &#x27;)&#x27;) return return_value 12Input: n = 3Output: [&quot;((()))&quot;,&quot;(()())&quot;,&quot;(())()&quot;,&quot;()(())&quot;,&quot;()()()&quot;] 1234567891011121314In [4]: s_generator(3) s_generator(3)ss_generator(3)s_generator(2)ss_generator(2)s_generator(1)ss_generator(1)Out[4]: [&#x27;((()))&#x27;, &#x27;(()())&#x27;, &#x27;()(())&#x27;, &#x27;(())()&#x27;, &#x27;()()()&#x27;]In [5]: s_generator.cache_info() Out[5]: CacheInfo(hits=3, misses=3, maxsize=None, currsize=3)In [6]: ss_generator.cache_info() Out[6]: CacheInfo(hits=3, misses=3, maxsize=None, currsize=3) Closures also provide an efficient mechanism for maintaining state between several calls. Traditional (OOP) approach: 12345678class Countdown: def __init__(self, n): self.n = n def next_value(self): old_value = self.n self.n -= 1 return old_value Closure-based approach: 12345678def countdown(n): def get_next_value(): nonlocal n old_value = n n -= 1 return old_value return get_next_value This is not only clean but also fast. 12345678910111213def test_object_oriented_approach(): c = Countdown(1_000_000) while True: value = c.next_value() if value == 0: breakdef test_functional_approach(): get_next_value = countdown(1_000_000) while True: value = get_next_value() if value == 0: break 12345In [5]: %timeit test_object_oriented_approach()182 ms  2.61 ms per loop (mean  std. dev. of 7 runs, 1 loop each)In [6]: %timeit test_functional_approach()96.8 ms  1.18 ms per loop (mean  std. dev. of 7 runs, 10 loops each) Why? 12345678910111213141516In [9]: c = Countdown(1_000_000)In [10]: dis(c.next_value) 6 0 LOAD_FAST 0 (self) 2 LOAD_ATTR 0 (n) 4 STORE_FAST 1 (old_value) 7 6 LOAD_FAST 0 (self) 8 DUP_TOP 10 LOAD_ATTR 0 (n) 12 LOAD_CONST 1 (1) 14 INPLACE_SUBTRACT 16 ROT_TWO 18 STORE_ATTR 0 (n) 8 20 LOAD_FAST 1 (old_value) 22 RETURN_VALUE 12 instructions, 2 LOAD_ATTR instructions, 1 STORE_ATTR instruction. 123456789101112In [11]: get_next_value = countdown(1_000_000)In [12]: dis(get_next_value) 4 0 LOAD_DEREF 0 (n) 2 STORE_FAST 0 (old_value) 5 4 LOAD_DEREF 0 (n) 6 LOAD_CONST 1 (1) 8 INPLACE_SUBTRACT 10 STORE_DEREF 0 (n) 6 12 LOAD_FAST 0 (old_value) 14 RETURN_VALUE 8 instructions, NO LOAD_ATTR, STORE_ATTR instructions. Closures Generators Coroutines When we define a function containing the yield keyword, we define a generator. Defining a generator allows the user to define a custom iterator in the style of defining a function. 1234def countdown(n): while n &gt; 0: yield n n -= 1 We create a generator object when we call a generator definition. The generator object can be used like any iterator: 1234567891011121314In [2]: c = countdown(5)In [3]: next(c)Out[3]: 5In [4]: next(c)Out[4]: 4In [5]: for value in c: ...: print(value) ...:321 When we call next() on a generator object, it will execute code, until it encounters a yield statement. The yield statement tells the generator object to return a value, and continue execution from here when next() is called again. 1234In [2]: c = countdown(5)In [3]: next(c)Out[3]: 5 This executes: 12while n &gt; 0: yield n When we call next() on a generator object, it will execute code, until it encounters a yield statement. The yield statement tells the generator object to return a value, and continue execution from here when next() is called again. 12In [4]: next(c)Out[4]: 4 This executes: 123 n -= 1while n &gt; 0: yield n This is called lazy evaluation. This can dramatically boost performance and reduce memory usage in some applications. For example: 1234567891011def get_comments_from_file(file): with open(file, &#x27;r&#x27;) as fp: for line in fp: # strip whitespace stripped_line = line.strip() # check if the line is empty after stripping whitespace if stripped_line: # check if the line is a comment if stripped_line[0] == &#x27;#&#x27;: # if it is, yield it yield stripped_line This will NOT read the whole file into memory. Only when the user calls next() on the generator object, will the generator read the file LINE BY LINE (with only ONE LINE of the file in memory at once), and return the next comment line. This is an efficient way of extracting comments from GB-sized files (such as logs). itertools Python provides many functions for creating an iterator from another iterator. For example: itertools.permutations(iterable [, r]) itertools.combinations(iterable, r) itertools.product(iter1, iter2, iterN, [repeat=1]) Widely used in algorithms: itertools.permutations(iterable [,r]) 123456789101112131415In [1]: import itertoolsIn [2]: numbers = range(4)In [3]: permutations_of_two_numbers_iterator = itertools.permutations(numbers, r=2)In [4]: next(permutations_of_two_numbers_iterator)Out[4]: (0, 1)In [5]: next(permutations_of_two_numbers_iterator)Out[5]: (0, 2)In [6]: next(permutations_of_two_numbers_iterator)Out[6]: (0, 3) Widely used in algorithms: itertools.combinations(iterable ,r) 12345678910111213In [1]: import itertoolsIn [2]: numbers = range(4)In [3]: for first, second in itertools.combinations(numbers, 2): ...: print(first, second) ...:0 10 20 31 21 32 3 Widely used in algorithms: itertools.product(iter1, iter2, iterN, [repeat=1]) 1234567891011121314In [1]: import itertoolsIn [2]: first_list = [1,2,3]In [3]: second_list = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;]In [4]: third_list = [True,False]In [5]: it = itertools.product(first_list, second_list, third_list)In [6]: next(it)Out[6]: (1, &#x27;a&#x27;, True)In [7]: next(it)Out[7]: (1, &#x27;a&#x27;, False)In [8]: next(it)Out[8]: (1, &#x27;b&#x27;, True) Closures Generators Coroutines Starting from Python 2.5, the yield statement can be used as an right value: 1captured_input = yield value_to_yield Generators defined like this can accept sent input while providing output. These generators are called coroutines. The concept of coroutines was proposed in the 60s, but only gained traction in recent years. Coroutines can be seen as a combination of subroutines and threads. Can pause and restart during execution. Controlled by itself instead of the operating system. Different coroutines run within a thread are concurrent instead of parallel. Simple example: 1234567891011121314import mathdef update_mean(): current_input = yield sum = current_input count = 1 while True: current_input = yield sum / count sum += current_input count += 1 Simple example: 123In [3]: updater = update_mean()In [4]: next(updater) This executes: 1current_input = yield And the coroutine waits for an input to be sent. Send an input: 12In [5]: updater.send(2)Out[5]: 2.0 The coroutine receives the input, and executes: 1234sum = current_inputcount = 1while True: current_input = yield sum / count And the coroutine waits for an input to be sent. Send an input: 12In [6]: updater.send(4)Out[6]: 3.0 The coroutine receives the input, and executes: 1234 sum += current_input count += 1while True: current_input = yield sum / count And the coroutine waits again for an input to be sent. More complicated example: set-associative cache simulation number_of_cache_sets * Set number_of_ways_of_associativity * Block block_size_in_bytes * Byte The whole set-associative cache is a coroutine receiving (address, is_write) tuples as input, and calculating (cache_hit, writeback_address) tuples as output. It models each set as a coroutine receiving (tag, is_write) tuples as input, and calculating (cache_hit, writeback_address) tuples as output. Different coroutine definitions for round-robin, LRU, etc. The whole set-associative cache 1234567891011121314151617181920212223242526272829303132def cache_coroutine(cache_set_coroutine_function, block_size_in_bytes, number_of_ways_of_associativity, number_of_cache_sets): # create cache_set_coroutine_list and activate each cache_set_coroutine cache_set_coroutine_list = [ cache_set_coroutine_function(number_of_ways_of_associativity) for _ in range(number_of_cache_sets) ] for cache_set_coroutine in cache_set_coroutine_list: next(cache_set_coroutine) # get function_to_split_address and function_to_merge_address function_to_split_address, function_to_merge_address = get_functions_to_split_and_merge_address( block_size_in_bytes, number_of_cache_sets ) # receive address, is_write # yields nothing address, is_write = yield while True: # splits address tag, cache_set_index, offset = function_to_split_address(address) # send (tag, is_write) to the appropriate cache_set_coroutine cache_hit, victim_tag, writeback_required = cache_set_coroutine_list[cache_set_index].send((tag, is_write)) # create writeback_address if (victim_tag is not None) and writeback_required if (victim_tag is not None) and writeback_required: writeback_address = function_to_merge_address(victim_tag, cache_set_index, 0) else: writeback_address = None # receive address, is_write # yield cache_hit, writeback_address address, is_write = yield cache_hit, writeback_address Cache Set with LRU replacement policy 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def lru_cache_set_coroutine(associativity): tag_list = [ None for _ in range(associativity) ] dirty_bit_list = [ False for _ in range(associativity) ] indices_in_lru_order = OrderedDict() for index in range(associativity - 1, -1, -1): indices_in_lru_order[index] = None # receive first tag and is_write tag, is_write = yield while True: cache_hit = False victim_tag = None writeback_required = False try: # find tag_index tag_index = tag_list.index(tag) # tag_index found cache_hit = True if is_write: dirty_bit_list[tag_index] = True # move tag_index to the end of indices_in_lru_order indices_in_lru_order.move_to_end(tag_index) except ValueError: # tag_index not found # get index_of_victim from indices_in_lru_order index_of_victim, _ = indices_in_lru_order.popitem(last=False) victim_tag = tag_list[index_of_victim] if dirty_bit_list[index_of_victim]: writeback_required = True tag_list[index_of_victim] = tag if is_write: dirty_bit_list[index_of_victim] = True else: dirty_bit_list[index_of_victim] = False # insert index_of_victim to the end of indices_in_lru_order indices_in_lru_order[index_of_victim] = None # receive tag and is_write # yield (cache_hit, victim_tag, writeback_required) tag, is_write = yield (cache_hit, victim_tag, writeback_required) Suppose our cache has only eight blocks and each block contains four words. The cache is 2-way set associative, so there are four sets of two blocks. The write policy is write-back and write-allocate. LRU replacement is used. https://courses.cs.washington.edu/courses/cse378/02sp/sections/section9-3.html 12345678910111213141516171819202122232425In [3]: cache = cache_coroutine(lru_cache_set_coroutine, block_size_in_bytes=4 * ...: 2, number_of_ways_of_associativity=2, number_of_cache_sets=4) In [4]: next(cache) In [5]: cache.send((0, True)) Out[5]: (False, None)In [6]: cache.send((64, False)) Out[6]: (False, None)In [7]: cache.send((4, True)) Out[7]: (True, None)In [8]: cache.send((40, True)) Out[8]: (False, None)In [9]: cache.send((68, False)) Out[9]: (True, None)In [10]: cache.send((128, True)) Out[10]: (False, 0)In [11]: cache.send((0, False)) Out[11]: (False, None)","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"}],"tags":[]},{"title":"Paper Reading: The Fundamentals of Writing Questions","slug":"Paper-Reading-The-Fundamentals-of-Writing-Questions","date":"2022-10-26T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/26/Paper-Reading-The-Fundamentals-of-Writing-Questions/","link":"","permalink":"https://abbaswu.github.io/2022/10/26/Paper-Reading-The-Fundamentals-of-Writing-Questions/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. This part of the book addresses the problem of crafting survey questions that respondents are willing to answer and respond to accurately. It first discusses issues to consider when designing survey questions, then presents the structure of a survey question and different question formats, before providing specific guidelines on wording survey questions. Issues to consider when designing survey questions What concepts to measure Recommended: Adopt established measures from existing surveys What data to collect Factual information: precise, readily available Opinion: requires time to formulate, strongly influenced by context Behavior: better memory of recent, memorable events compared with distant, mundane events What question format to use Different cognitive information processing for aural and visual surveys What mode to adopt The presence of an interviewer may speed up surveys, but may induce social desirability and acquiescence, leading to interviewer bias. Lack of standardization among different interviewers may lead to interviewer variance. What to modify (from existing surveys) no changes or only minimal changes when replicating or comparing results questions should also be asked in a similar fashion How to motivate respondents think about the cognitive process respondents go through pay attention to the context and wording The structure of a survey question Question stem Additional instructions Answer spaces or choices Different question formats Open-ended rich, detailed more prone to skipping requires lengthy data processing Closed-ended nominal or ordinal categories set of answer choices known in advance easy to analyze Partially closed-ended closed-ended with \"other\" response respondents more likely to select the options instead of \"other\" Specific guidelines on wording survey questions Choose the appropriate question format. Make sure the question applies to the respondent. Ask one question at a time. Make sure the question is technically accurate. Use simple, familiar and specific words. Use short, simple sentences that take a question form. Avoid double negatives. Organize questions in a more straightforward, comprehensible way. Also sprinkled throughout the section is the notion that the crafter should get into a respondent's state of mind when crafting survey questions, and also test the survey questions to evaluate their quality. This section is very comprehensive and convincing, as the author supports his arguments by analyzing specific examples from actual surveys, and also frequently quoting previous work on the topic. From such a chapter we can gain a deep understanding of the nature of survey questions, especially the underlying cognitive, psychology and sociology problems, as well as the best practices within the domain, and we can also refer to this chapter as a guide and checklist when we craft survey questions ourselves.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: All You Ever Wanted to Know About Dynamic Taint Analysis and Forward Symbolic Execution (but might have been afraid to ask)","slug":"Paper-Reading-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask","date":"2022-10-25T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/10/25/Paper-Reading-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask/","link":"","permalink":"https://abbaswu.github.io/2022/10/25/Paper-Reading-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Forward symbolic execution and dynamic taint analysis are quickly becoming \"staple techniques in security analyses\". Dynamic taint analysis runs a program and observes which computations are affected by predefined taint sources such as user input. Dynamic forward symbolic execution automatically builds a logical formula describing a program execution path, which reduces the problem of reasoning about the execution to logic, allowing us to reason about the behavior of a program on many different inputs at one time. The two analyses can be used in conjunction to build formulas representing only the parts of an execution that depend upon tainted values. Forward symbolic execution: Test Case Generation (automatically generate inputs to test programs, generate inputs that cause two implementations of the same protocol to behave differently) Automatic Input Filter Generation (input filters that detect and remove exploits from the input stream) Cristian Cadar, Daniel Dunbar, and Dawson Engler. Klee: Unassisted and automatic generation of high-coverage tests for complex systems programs. In Proceedings of the USENIX Symposium on Operating System Design and Implementation, 2008. Cristian Cadar, Vijay Ganesh, Peter Pawlowski, David Dill, and Dawson Engler. EXE: A system for automatically generating inputs of death using symbolic execution. In Proceedings of the ACM Conference on Computer and Communications Security, October 2006. Patrice Godefroid, Nils Klarlund, and Koushik Sen. DART: Directed automated random testing. In Proceedings of the ACM Conference on Programming Language Design and Implementation, 2005. Dynamic taint analysis: Unknown Vulnerability Detection (misuses of user input) Automatic Network Protocol Understanding. Dynamic taint analysis has been used to automatically understand the behavior of network protocols when given an implementation of the protocol. Malware Analysis (analyze how information flows through a malware binary, explore trigger-based behavior, and detect emulators) However, there has been little effort to formally define them and summarize critical issues that arise when applying these techniques in \"typical security contexts\". The authors formalize the runtime semantics of dynamic taint analysis and forward symbolic execution by using SIMPIL (Simple Intermediate Language), which is \"representative of internal representations used by compilers and is powerful enough to express typical languages\". Concepts: Statements: assignments, assertions, jumps, conditional jumps. Expressions: constants, variables, binary operators, unary operators, get_input. Execution state: the list of program statements, the current memory state, the current value for variables, the program counter, the current statement. Notation: \\(\\Sigma\\): list of program statements \\(\\Sigma[v_1]\\): statement at \\(pc = v_1\\). \\(\\mu\\): memory state \\(\\mu[v_1]\\): memory content at address \\(v_1\\) \\(\\Delta\\): register state (values of all variables) \\(\\Delta[x]\\): value of variable \\(x\\) \\(\\Delta[x \\leftarrow 10]\\): setting the value of variable \\(x\\) to 10 \\(pc\\): program counter. \\(\\mu, \\Delta \\vdash e \\Downarrow v\\): Given memory state \\(\\mu\\) and register state \\(\\Delta\\), the value of expression \\(e\\) is \\(v\\). \\(\\Sigma, \\mu, \\Delta, pc, EXPRESSION \\rightsquigarrow \\Sigma, \\mu&#39;, \\Delta&#39;, pc&#39;, {EXPRESSION}&#39;\\): Given list of program statements \\(\\Sigma\\), memory state \\(\\mu\\), register state \\(\\Delta\\), program counter \\(pc\\), executing expression \\(EXPRESSION\\) leads to new memory state \\(\\mu&#39;\\), new register state \\(\\Delta&#39;\\), new program counter \\(pc&#39;\\), and the next expression is \\({EXPRESSION}&#39;\\). Other high-level language constructs such as functions or scopes can be easily represented using these constructs. Dynamic taint analysis tracks values in a program dependent on data derived from a \"taint source\" at runtime. As it is conducted at runtime, it can be expressed by extending SIMPIL. Dynamic taint analysis is conducted in different ways (i.e., under different \"taint policies\") for different applications. The differences lie in \"how new taint is introduced to a program\", \"how taint propagates as instructions execute\", and \"how taint is checked during execution\". The author presents the example of the \"tainted jump policy\" for attack detection, points out several challenges it faces, and analyzes the proposed solutions. \"Distinguishing between memory addresses and cells is not always appropriate\". An alternative \"tainted addresses policy\" could be used, but this may also overtaint. Information flow can occur through control dependencies in addition to dataflow dependencies. This requires \"reasoning about multiple paths\", while pure dynamic taint analysis \"executes on a single path at a time\". Solutions include \"supplementing dynamic analysis with static analysis\" and \"using heuristics\". Taint is only added and never removed (i.e., \"sanitized\"), leading to the problem of \"taint spread\", reducing precision. Well-known constant functions (i.e. using XOR to zero out registers in x86 code) can be checked. In addition, we can consider the outputs of some functions like cryptographic hash functions as untainted, due to limited influence of input on output. This can be quantified (Newsome et al.) to automatically recognize such cases. Furthermore, values can be untainted \"if the program logic performs sanitization itself\" (e.g., index bounds checking). In conclusion, this paper is an useful introductory paper in forward symbolic execution and dynamic taint analysis, and I have mainly learned the following two things from the paper: The idea of formalizing runtime semantics using RISC-like bytecode An introduction to dynamic taint analysis - what it is, what it can do, and what challenges it faces Feedback from the Class Discussion What is the difference between a statement and an expression? A statement can modify program state when it is executed, while an expression doesn't modify program state. In the formalism of SIMPIL, we determine which expression to evaluate by pattern-matching the rule. LLVM has a dataflow sanitization pass, which may be useful for implementing taint analysis. Dynamic program analysis only looks at a single path. If we are to prove something about a program, static program analysis would be a better direction.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: ReCrash: Making Software Failures Reproducible by Preserving Object States","slug":"Paper-Reading-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States","date":"2022-10-25T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/25/Paper-Reading-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States/","link":"","permalink":"https://abbaswu.github.io/2022/10/25/Paper-Reading-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. Background Reproduction is key to finding and fixing software problems and verifying proposed solutions, but reproduction can be difficult. Nondeterminism: A problem may depend on timing (e.g., context switching), memory layout (e.g., hash codes), or random number generators. Remote detection: A problem may be discovered by someone other than the developer, and it may depend on implicit program inputs such as user GUI actions, environment variables, the state of the file system, operating system behavior, etc. This information may be easy to miss, difficult to collect, or confidential. Test case complexity: The exposing execution might be complex, and the buggy method might be called multiple times before the bug is triggered. Proposed Solution: ReCrash ReCrash maintains a shadow stack with copies of the receiver and arguments to each method during execution of the target program. Several copy strategies Several optimizations When the program crashes, ReCrash serializes the shadow stack, and generates unit tests by calling each method on the shadow call stack with their receiver and arguments. Calling the method at top of the call stack may not provide enough context. Calling a method closer to the bottom provides more context, but is less likely to reproduce the original failure. Proposed Solution: ReCrash Assumption: It is possible to reproduce many failures with only some of the information available on entry to the methods on the stack at the time of the failure. Many bugs are dependent on small parts of the heap. Good object-oriented style encapsulates important state nearby. Good object-oriented style avoids excessive use of globals. ReCrash has access to and will store any parts of the global state or environment that are passed as method arguments. Question: What if global state is read or written in the method? Monitoring Phase Several copy strategies Several optimizations Monitoring fewer methods Second-chance mode Copy Strategies An argument may be side-effected between the method entry and the point of the failure in the method. Copying strategies: Reference: copying only the reference to the argument. Shallow: copying the argument itself. Depth-i: copying all the state reachable with \\(\\le i\\) dereferences from the argument. Deep-copy: copying the entire state. Options: Used-fields: deeper copying on fields that are used (read or written) in the method. ReCrash always uses the reference strategy for immutable parameters. Monitoring Fewer Methods Dosen't monitor methods that cannot be used in the generated tests, or are unlikely to expose problems. non-public methods empty methods simple methods such as getters and setters (no more than 6 opcodes) Second-chance Mode ReCrash initially monitors no method calls. Each time a failure occurs, ReCrash enables method argument monitoring for all methods found on the stack trace. Efficient, but requires a failure to be repeated twice. If the developer doesn't mind missing the first time a failure happens, and the failure occurs relatively often, second chance mode is a good fit. Question: could recording all inputs provided to the program be used in tandom with second-chance mode (such that the failure is probable to happen the second time)? Test Generation Phase ReCrash generates a test for each of the methods in the shadow stack. Restores the state of the arguments that were passed to a method. Invokes the method the same way it was invoked in the original execution. Only tests that end with the same exception as the original failure are saved. Storing more than one test that ends with the same failure is useful. Some tests reproduce a failure, but would not help the developer understand, fix, or check her solution. Experimental Study Subject programs: Javac-jsr308: the OpenJDK Java compiler, extended with JSR308 (\"Annotations on Java Types\"), with four crashes provided by the developers. SVNKit: a subversion client, with three crash examples from bug reports. Eclipsec: a Java compiler included in the Eclipse JDT, with a crash found in the Eclipse bug database. BST: a toy subject program used by Csallner in evaluating CnC, with three crashes found by CnC. Experimental Study For each subject program: Run PIDASA for parameter immutability classification. For different argument copying strategies, with and without second-chance mode: Run ReCrash on inputs that made the subject programs crash. Count how many test cases reproduced each crash. Question: how useful would ReCrash be in reality where it is unknown whether the subject projects could crash, and which inputs would make the subject programs crash? Experimental Study Research questions: How reliably can ReCrashJ reproduce crashes? What is the size of the stored deep copy of the shadow stack? Are the tests generated by ReCrash useful for debugging? Like a case study: an analysis of two crashes, and comments from developers What is the overhead (time and memory) of running ReCrash? Aspects assessed: different argument copying strategies with and without second-chance mode How reliably can ReCrash reproduce crashes? ReCrash was able to reproduce the crash in all cases. For some crashes, every candidate test case reproduces the crash. For other crashes, only a subset of the generated test cases reproduces the crash. In most cases, simply copying references is enough to reproduce crashes. In other cases, using the shallow copying strategy with used-fields was necessary. What is the size of the stored deep copy of the shadow stack? Subject Programs and Crashes Used in our Experimental Study Question: why isn't it compared with the program size and the program memory usage? An analysis of two crashes Eclipsec bug e1: Eclipsec crashes in callee canBeInstantiated because an earlier if statement in the caller resolveType failed to set a boolean flag hasError to true. The test case for canBeInstantiated will reproduce the crash, but is not helpful. Demonstrates importance of generating tests for multiple methods on the stack. Javac-jsr308 bug j4: Compiling source code containing an annotation with too many arguments results in an index-out-of-bounds exception in method visitMethodInvocation. The generated test does not require the whole source code and encodes only the necessary minimum to reproduce the crash. Useful when the compiler crash happens in the field, and the user cannot provide the entire source code for debugging. Comments from Developers We gave the tests for j1-4 to two Javac-jsr308 developers and asked for comments about the tests' usefulness, receiving positive responses. I often have to climb back up through a stack trace when debugging. ReCrash seems to generate a test method for multiple levels of the stack, making it useful. I find that you wouldn't have to wait for the crash to occur again useful. When I set a break point, the break point maybe be executed multiple times before the error. Using ReCrash, I was able to jump (almost directly) to the necessary breakpoint. Question: Why only analyze two crashes and ask only two developers? What is the overhead (time and memory) of running ReCrash? Time overhead Non second-chance mode: Copying only the references can be expensive (11%-42%), and shallow copying with used-fields is similar (13%60%). Usable for in-house testing. Deep copying is completely unusable (12,000%-638,000%). Second-chance mode: A barely noticeable 0%1.7% under copying only the references and shallow copying with used-fields, after a crash has already been observed. What is the overhead (time and memory) of running ReCrash? Memory overhead Non second-chance mode: 0.2M4.7M (2.6%-90.3%) under shallow copying with used-fields. Second-chance mode: negligible Conclusions ReCrashJ is usable in real software deployment Simple to implement Scalable Generates simple, helpful test cases that effectively reproduce failures Time and memory overhead (13%60%, 2.6%-90.3%) under non second-chance mode and shallow copying with used-fields usable for in-house testing Extremely efficient under second-chance mode","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Evolutionary Generation of Whole Test Suites","slug":"Paper-Reading-Evolutionary-Generation-of-Whole-Test-Suites","date":"2022-10-24T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/10/24/Paper-Reading-Evolutionary-Generation-of-Whole-Test-Suites/","link":"","permalink":"https://abbaswu.github.io/2022/10/24/Paper-Reading-Evolutionary-Generation-of-Whole-Test-Suites/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Background Automatically deriving test cases for realistically sized programs: Select one coverage goal (e.g., program branch) at a time, and derive a test case that exercises this particular goal. Solving path constraints generated with symbolic execution / dynamic symbolic execution Meta-heuristic search techniques Mutation testing Alternative approaches not directly aimed to achieve code coverage Randoop incrementally generate sequences of function calls to find buggy test sequences requires automated oracles (e.g. developer-written assertions and exceptions) Problems Many coverage goals are unreachable. 1234567891011void push(int x) &#123; if (size &gt;= values.length) &#123; resize(); &#125; if (size &lt; values.length) &#123; values[size++] = x; &#125; else &#123; // UNREACHABLE &#125;&#125; Problems Some coverage goals are more difficult to satisfy than others. The order of coverage goals is important: a lucky choice can result in a good test suite, while an unlucky choice can result in a waste of resources. 12345678910111213void push(int x) &#123; if (size &gt;= values.length) &#123; // HARD resize(); &#125; else &#123; // EASY &#125; if (size &lt; values.length) &#123; values[size++] = x; &#125;&#125; Problems Satisfying a particular coverage goal frequently entails satisfying further coverage goals by accident. The order of coverage goals is important. 123456789int pop() &#123; if (size &gt; 0) &#123; // May imply coverage in `push` and `resize` return values[size]; &#125; else &#123; throw new EmptyStackException(); &#125;&#125; Our Solution: EvoSuite Optimize an entire test suite at once instead of considering distinct test cases. Evolve a population of test suites towards satisfying a coverage criterion. Assume automated oracles are not available, and require the outputs of the test cases to be manually verified. The generated test suites should be of manageable size. Solves the problem of: difficult and unreachable coverage goals order of coverage goals accidentally satisfying further coverage goals Our Solution: EvoSuite Questions: We are interested in sequences in OOP. Should coverage in terms of a new ordering seen in the last \\(n\\) function calls in the sequence should make more sense? (Praveen) It seems like Evosuite offloads the responsibility of adding in correct assertions to the developers. How easy is it for the developers to do this, especially when compared with manually writing all of the test suite? (Shizuko, ToTo, Larry) EvoSuite Modeling Population 1 .. M Test Suite 1 .. N Test Case 1 .. L Statement Four types of statements are modeled. Primitive statements: numeric variables (e.g. int var0 = 54;) Constructor statements: new instances of a class (e.g. Stack var1 = new Stack();). All parameters of the constructor call have to be values of previous statements. Field statements: public fields of objects (e.g. int var2 = var1.size;). If the field is non-static, then the source object of the field has to be a value of a previous statement. Method statements: public methods of objects (e.g. int var3 = var1.pop();). The source object and all parameters have to be values of previous statements. EvoSuite Modeling The set of available classes, their public constructors, methods, and fields are extracted from the given software under test. An optimal solution is a test suite that covers all the feasible branches/methods and is minimal in the number of statements. EvoSuite Process Overview Randomly generate a set of initial test suites. Evolve using evolutionary search towards satisfying a coverage criterion. Minimize the best resulting test suite. Questions: How are test suites randomly generated? The author discusses \"sampling\". Where are we sampling from? (Larry, Jifeng) Evolutionary Search Test Suite Fitness Function Crossover Accepting the Mutated Offspring Bloat Control Test Suite Fitness Function Covering all branches \\(B\\) and methods \\(M\\) of a program. To estimates how close a test suite \\(T\\) is to covering all branches \\(B\\) of a program, for each branch \\(b\\), minimal branch distance \\(d_{min}(b, T)\\) is measured. If the branch predicate is \\(x \\ge 10\\), and during execution, \\(x == 5\\), then the minimal branch distance is \\(10 - 5 = 5\\). The minimal branch distance is then normalized to get the branch distance \\(d(b, T) = f(d_{min}(b, T))\\), where \\(f(x) = \\frac{x}{x + 1}\\). \\(fitness(T) = |M| - |M_T| + \\sum_{b \\in B}{d(b, T)}\\) If execution exceeds a time limit of 5 minutes, maximum fitness is automatically assigned. Test Suite Fitness Function Questions: What does branch distance actually mean? Why do we use it? (Eric, Rut, Yayu, Udit, Jifeng) Doesn't \\(\\sum_{b \\in B}{d(b, T)}\\) already consider that branch distances are maximal in unvisited methods? Why do we need an additional \\(|M| - |M_T|\\) term? Furthermore, different methods could have a different number of branches. Should the branch distance sum for all branches within a method be normalized? (Jifeng) Crossover Rank selection based on the fitness function is used to select two parent test suites \\(P_1\\) and \\(P_2\\) for crossover. In case of ties, smaller test suites are assigned better ranks. During crossover: a random value \\(\\alpha\\) is chosen from \\((0, 1)\\) the first offspring test suite \\(O_1\\) will contain the first \\(\\alpha |P_1|\\) test cases from \\(P_1\\) and the last \\((1 - \\alpha)|P_2|\\) test cases from \\(P_2\\) the second offspring test suite \\(O_2\\) will contain the first \\(\\alpha |P_2|\\) test cases from \\(P_2\\) and the last \\((1 - \\alpha)|P_1|\\) test cases from \\(P_1\\) because test cases are independent, \\(O_1\\) and \\(O_2\\) will always be valid Mutation The two offspring test suites \\(O_1\\) and \\(O_2\\) are then mutated. When a test suite T is mutated, each of its test cases is mutated with probability \\(\\frac{1}{|T|}\\). If a test case \\(t\\) is mutated, remove statements, change statements, and insert statements are each applied with probability \\(\\frac{1}{3}\\). Then, a number of new random test cases are added to \\(T\\). Remove Statements If a test case \\(t\\) contains \\(n\\) statements, each statement is removed with probability \\(\\frac{1}{n}\\). If the removed statement \\(s_i\\) is subsequently used by \\(s_j (j &gt; i)\\), try to replace this use with another statement before \\(s_j\\). If this is not possible, recursively remove \\(s_j\\). If all statements have been removed from \\(t\\), remove \\(t\\) from \\(T\\). Change Statements If a test case \\(t\\) contains \\(n\\) statements, each statement is changed with probability \\(\\frac{1}{n}\\). If the changed statement \\(s_i\\) is a primitive statement, its numeric value is changed by a random value. Otherwise, a method, field, or constructor with the same return type is randomly chosen. Insert Statements With probability \\(p\\), a new statement is inserted at a random position in the test case. With probability \\(p^2\\), a second statement is inserted, and so on. Questions: What are the justifications for the probabilities? (Kevin) Can we change the probabilities used in the mutation and insertion by using method calls they kept track of and variables generated in each iteration? (Joyce) When deleting, if the statement is chosen from the beginning few statements, is there a high probability that many/multiple following statements would be removed? Because an initial statement usually has a higher probability of containing an initialization/declaration function. (Rut) Why is the probability of inserting the first, second, etc. statement different? This is not the case with remove statements and change statements. (Jifeng) To mutate and generate test cases, the GA algorithm should have knowledge of the programming language constructs, fields &amp; methods of the software under test, etc. Does this require a significant engineering effort? (Udit) Accepting the Mutated Offspring The coverage achieved by the Mutated Offspring is measured by the Test Suite Fitness Function. Conditions for accepting the mutated offspring: The coverage achieved by the Mutated Offspring exceeds that achieved by its parents, or is on par with that achieved by its parents, and that the mutated offspring are shorter. Their length do not exceed twice that of the Test Suite with the best coverage in the community. Accepting the Mutated Offspring Questions: Are the parents removed before adding the children? (Rut) Compared with the single branch strategy, only the crossover is different, and the mutation is done in the same way. (Tarcisio) Bloat Control A variable size representation could lead to bloat, where small negligible improvements in the fitness value are obtained with larger solutions. This is a very common problem in Genetic Programming. The following measures are used for bloat control: Limit the maximum number \\(N\\) of test cases within a test suite and the maximum number of statements \\(L\\) within a test case. (still need to choose comparatively larger \\(N\\) and \\(L\\) and then reduce their length during/after the search to dramatically boost coverage) Crossover selection policy Mutated offspring acception policy Bloat Control Questions: Does coverage-guided fuzzing, which uses a variant of Genetic Programming, suffer from bloat? If so, could any measures be applied to solve this problem? (Jifeng) How to reduce the length during/after the search? (Yayu, Jifeng) Evaluation EvoSuite is compared with the traditional single branch approach on top of EvoSuite infrastructure. Offspring is generated using the crossover function, but is conducted on two sequences of statements. Because there are dependencies between statements, the statements of the second part are appended one at a time, trying to satisfy dependencies with existing values, generating new values if necessary. The traditional approach level plus normalized branch distance fitness function is used. The two approaches are compared on five open source libraries and a subset of an industrial case study project previously used by Arcuri et al. The units are testable without complex interactions with external resources and are not multithreaded. Evaluation \"Best practices\" based on past experience are used for EvoSuite: Population size: 80 Maximum test suite size \\(N = 100\\) Maximum test case size \\(L = 80\\) The initial test suites are generated with 2 test cases each Initial probability for test case insertion: 0.1 Crossover probability: 3 / 4 Initial probability for statement insertion: 0.5 Evaluation The search operators for test cases make use of only the type information in the test cluster, and so difficulties can arise when method signatures are imprecise. To overcome this problem for container classes, we always put Integer objects into container classes, and cast returned Object instances back to Integer. As the length of test cases can vary greatly and longer test cases generally have higher coverage, we decided to take the number of executed statements as execution limit. The search is performed until either a solution with 100% branch coverage is found, or \\(k = 1,000,000\\) statements have been executed as part of the fitness evaluations. Evaluation Questions: Why not compare EvoSuite to any other (non genetic-testing based) approach? (Zack) Why \"the units are testable without complex interactions with external resources and are not multithreaded\"? (Marie) Is there a justification for these \"best practices\"? (Praveen, Kevin, Madonna, Jifeng) Do the \"best practices\" overfit the 5 open-source libraries? (Joyce) Why the choice of an Integer? And does it work in practice? Given that the internals of the program might be expecting something else? (Rut) Results Whole test suite generation achieves higher coverage than single branch test case generation. Whole test suite generation produces smaller test suites than single branch test case generation. Results Questions: While we have focused on branch coverage in this paper, the findings also carry over to other test criteria is an unwarranted extrapolation. (Zack) Evosuite claims that the test cases are smaller, but how much smaller? (not obvious from Figure 7) (ToTo) High coverage test suite does not necessary mean high bug-finding abilities. How does the performance compare to other tools? (ToTo, Praveen, Kevin, Madonna) The authors did not evaluate EvoSuite against a human in software engineering. Whether EvoSuite will improve the ability to test software from a software developer's point of view is unknown. (Marie)","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Personal Website Design Considerations","slug":"Personal-Website-Design-Considerations","date":"2022-10-20T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/20/Personal-Website-Design-Considerations/","link":"","permalink":"https://abbaswu.github.io/2022/10/20/Personal-Website-Design-Considerations/","excerpt":"","text":"Hosting We host our personal website on GitHub Pages, a static site hosting service. Considerations: No need to buy/rent and set up infrastructure, such as Elastic Computing instances, Domain Name, Content Distribution Network, Load Balancer, DDoS protection Hosted directly from GitHub repository Our personal website meets its limitations: Non-commercial. No confidential information. Published GitHub Pages sites may be no larger than 1 GB. GitHub Pages sites have a soft bandwidth limit of 100 GB per month. GitHub Pages sites have a soft limit of 10 builds per hour. Implications: Static pages. Limit content of our personal website to text and lightweight multimedia, such as vector graphics and vector PDFs. Use raster graphics sparingly, and avoid heavyweight multimedia such as audio and video. Do not rebuild too frequently (&gt;10 builds per hour). Framework Our personal website uses the Hexo blog framework. Considerations: Support for GitHub Flavored Markdown. Easy-to-use CLI. One-command deploy to GitHub Pages. Support for two types of pages (Posts and Pages), adequate for a personal website. Huge library of spectacular, feature-packed and customizable themes. Theme Our personal website uses the fluid theme for Hexo. Considerations: Appropriate Features Support for many third-party commenting systems. Mathjax support, renders equations like \\(E=mc^2\\). Mermaid support. Social network links. Extremely Detailed Documentation. Actively Maintained. Our Considerations When Writing Posts Make the Markdown file as self-contained as possible. This includes: Using third-party pictures from the Internet with stable URLs whenever possible. Utilize fluid's support for Mermaid, and use Mermaid to describe and render in real-time diagrams such as Flowcharts, Sequence Diagrams, Class Diagrams, State Diagrams, and Mindmaps whenever possible, as opposed to including diagrams generated using other tools.","categories":[{"name":"Planning","slug":"Planning","permalink":"https://abbaswu.github.io/categories/Planning/"}],"tags":[]},{"title":"Paper Reading: Feedback-Directed Random Test Generation","slug":"Paper-Reading-Feedback-Directed-Random-Test-Generation","date":"2022-10-18T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/18/Paper-Reading-Feedback-Directed-Random-Test-Generation/","link":"","permalink":"https://abbaswu.github.io/2022/10/18/Paper-Reading-Feedback-Directed-Random-Test-Generation/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Overview The authors present Randoop, a feedback-directed random unit test generator for object-oriented programs which generates sequences of method calls that create and mutate objects, and uses feedback obtained from executing the sequences to guide the search towards new sequences. Logic of Randoop Randoop builds sequences incrementally starting from an empty set of sequences. In each iteration, it generates and executes a new sequence. Sequence Generation First, it selects a method randomly among the public methods of classes. Second, it finds arguments to provide to the method. If an argument is a primitive type, select a primitive value from a fixed pool of values. If an argument is a reference type, select an extensible value of the corresponding type from a previously generated sequence in \\(nonErrorSeqs\\) and put the previous generated sequence into a temporary list if possible, or select null otherwise. Third, a new sequence is formed by concatenating the sequences in the temporary list and the randomly selected method. Fourth, the new sequence is checked whether it has been generated before. If so, the process is repeated. Furthermore, the authors considers that repeated calls to a method may increase code coverage (e.g. reach code that increases the capacity of a container object, or reach code that goes down certain branches). Thus, with a probability \\(p = 0.1\\), instead of appending a single call of a chosen method, a maximum of \\(N = 100\\) calls are appended. Sequence Execution After a new sequence is generated, each method call in the sequence is executed, and after each call, contracts are checked. Default contracts checked by Randoop include: method throws no NullPointerException if no input parameter was null method throws no AssertionError o.equals(o) returns true and throws no exception o.hashCode() throws no exception o.toString() throws no exception If at least one contract is violated, the sequence is put in \\(errorSeqs\\), and no values within the sequence can be extended. If all contracts are not violated, the sequence is put in \\(nonErrorSeqs\\), and all values within the sequence are checked whether they can be extended. If the value has been encountered before, is null, or an exception occurs when executing the sequence leading to the value, the value cannot be extended. Experimental Study The authors evaluate the effectiveness of Randoop through three experiments. Comparing the basic block and predicate coverage of Randoop and five systematic input generation techniques on four container data structures used previously to evaluate these systematic input generation techniques. Comparing Randoop with JPF (a systematic testing technique) and undirected random testing on 14 widely-used libraries. A case study using Randoop to find regression errors between different implementations of the Java JDK. The experimental results strongly suggest that Randoop outperforms systematic and undirected random test generation in both coverage and error detection. Personal Thoughts In my opinion, a key advantage of Randoop is the \"sparse, global sampling\" that it performs, which \"retains the benefits of random testing (scalability, simplicity of implementation)\", while avoiding undirected random testing's pitfalls (generation of redundant or meaningless inputs), and is better adapted to large-scale library code than the \"dense, local sampling\" of systematic test generation. The sequences Randoop builds are akin to seeds in coverage-guided fuzzing, and I believe the efficiency and effectiveness of Randoop may be further boosted by applying a power schedule to the built sequences, much like applying a power schedule to the seeds in coverage-guided fuzzing. The built sequences could possibly have overlapping prefixes. Would using a tree structure be better than storing each sequence on its own? Randoop only supports a limited number of contracts, and its error-detection ability is rather weak. It may be appropriate on library code filled with assertions and checks, but may not work well on client code where these may be sparse.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Software tools to facilitate research programming","slug":"Paper-Reading-Software-tools-to-facilitate-research-programming","date":"2022-10-18T07:00:00.000Z","updated":"2023-11-08T20:50:46.820Z","comments":true,"path":"2022/10/18/Paper-Reading-Software-tools-to-facilitate-research-programming/","link":"","permalink":"https://abbaswu.github.io/2022/10/18/Paper-Reading-Software-tools-to-facilitate-research-programming/","excerpt":"","text":"This is a paper recommended to me by Margo Seltzer during a conversation. The original paper can be found here. Definition and Ubiquity of Research Programming Research programming is a unique form of programming where the primary objective is to derive insights from data. It's a widespread activity. Not only is it crucial for academic advancements across various disciplines including natural sciences, engineering, and social sciences, but it also extends beyond academia. By some estimations, the number of individuals engaged in research programming dwarfs the number of professional software developers, suggesting its vast scale and significance. Moreover, even professionals in fields like science, engineering, business, finance, public policy, and journalism engage in research programming. Challenges in Research Programming Data Management and Provenance. Keeping track of where each piece of data originates and ensuring it remains up-to-date is crucial. This process can be tedious and difficult, especially when large amounts of data are involved. Organizing, naming, and managing various versions of data files present challenges. Data Preparation: A significant portion of time is spent on data cleaning and reformatting. This task can often be labor-intensive and not directly contribute to deriving insights, yet it's unavoidable. The process is more than just computational number crunching. It often involves transferring data between different tools, converting data formats, and managing extensive datasets. Analysis Phase The core activity involves writing and refining programs to analyze data. Challenges arise from scripts that take excessive time to run, especially after incremental edits, and from scripts crashing due to various errors. Managing output files, including keeping track of metadata, presents additional challenges. Reflection Phase Researchers analyze outputs, take notes, hold meetings, and make comparisons. Graphs play a significant role in visualizing and interpreting results. Managing and comparing these graphical outputs is vital. Dissemination Phase Once the research is complete, results need to be consolidated and communicated, often in the form of reports or academic publications. Reproducing results becomes challenging with time, especially with evolving software environments. Sharing code and data in collaborative settings introduces its own set of challenges. Distinct Nature of Research Programming Compared to Traditional Software Engineering Purpose: Unlike software engineering, which focuses on creating robust software, research programming prioritizes insights. Environment: Research programmers work in a diverse environment using various languages and tools, making it inherently heterogeneous. Specifications: The research programming process is more fluid and iterative, with changing specifications based on new discoveries. Priorities: The emphasis is on quick iteration for faster discoveries rather than perfecting the code. Expertise: A broad range of individuals, not just professional programmers, engage in research programming. The Role of Modern-Day Tools Modern tools designed for general programming can be beneficial for research programmers. However, these tools are often not optimized for the unique characteristics of research programming. A balance needs to be struck between the robustness of software engineering tools and the flexibility required for research programming. Evolution of Documentation in Research Historically, scientific research was documented meticulously in handwritten lab notebooks. But with the rapid pace of computational research, such traditional methods are no longer sufficient. While many research programmers use digital note-taking methods, an ideal solution would seamlessly integrate notes with source code and data files. Closing Thought Recognizing and understanding these challenges are the first steps. It then becomes possible to leverage techniques from various domains, such as dynamic program analysis and recommendation systems, to enhance the productivity of research programmers. High-level Comments I am very interested in investigating existing formalizations and crystallized best practices for specific tasks in Research Programming, such as \"The Grammar of Graphics\" for visualization tasks, and how functional programming can synergize with them. A clean-sheet functional design has the potential to open new windows in addressing many of these challenges, especially those that lack adequate tool support. Ideally, while embracing a functional cleaniness, it should follow several aspects of the UNIX and C++ philosophies. It must be driven by actual problems and its features should be immediately useful in real world programs. It should support and encourage the user to design and build software, even large-scale ones such as operating systems, to be tried early. It should provide facilities for organising programs into separate, well-defined parts, and provide facilities for combining separately developed parts. It should make it easy to make the output of every parts become the input to another, as yet unknown, parts. Allowing a useful feature is more important than preventing every possible misuse. It should work alongside other existing programming languages, rather than fostering its own separate and incompatible programming environment. If the programmer's intent is unknown, it should allow the programmer to specify it by providing manual control.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Research Programming","slug":"Paper-Reading/Research-Programming","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Research-Programming/"}],"tags":[]},{"title":"Paper Reading: A Practical Guide for Using Statistical Tests to Assess Randomized Algorithms in Software Engineering","slug":"Paper-Reading-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering","date":"2022-10-16T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/10/16/Paper-Reading-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering/","link":"","permalink":"https://abbaswu.github.io/2022/10/16/Paper-Reading-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Overview There are many problems in software engineering which are undecidable and use randomized algorithms, such as automated unit test generation, random testing, and search algorithms (including Genetic Algorithms). As the outcomes of these randomized algorithms vary greatly from run to run, assessing their effectiveness is an important topic. To uncover whether randomized algorithms are properly assessed in software engineering research, the authors conducted a small-scale systematic review on three representative software engineering venues, namely IEEE Transactions of Software Engineering (TSE), IEEE International Conference on Software Engineering (ICSE) and International Symposium on Search Based Software Engineering (SSBSE), in the year 2009. The review shows that the analyses \"are either missing, inadequate, or incomplete\", and \"randomness is not properly taken into account\". The authors then put forward guidelines for properly assessing randomized algorithms in software engineering research. Definitions Censoring a condition in which only the range (i.e. above a certan value, below a certain value, within an interval) of a measurement or observation is known, and its precise value is unknown. Akin to clamping in saturated arithmetic. Commonly encountered in software engineering experiments when time limits are used. Assessment Procedure and Guidelines A novel randomized algorithm is commonly compared against an existing technique. After determing a measure to compare (e.g. source code coverage, execution time), we should run both algorithms a large enough number of times independently (the author recommends \"a very high number of runs\" and not the rule of thumb of \\(n = 30\\) in medicine and behavioral science, as human aspects are not involved. With the collected measure data, we conduct the following: Statistical Testing We use a statistical test to assess \"whether there is enough empirical evidence to claim a difference between the two algorithms\". In such a statistical test, the null hypothesis is typically \"there is no difference\", and we verify whether we should reject the null hypothesis. Definitions related to Statistical Testing There are two conflicting types of error when performing statistical testing: (I) we reject the null hypothesis when it is true, and (II) we accept the null hypothesis when it is false. The p-value of a statistical test is the probability of rejecting the null hypothesis when it is true. The significant level \\(\\alpha\\) of a statistical test is the highest p-value we accept for rejecting the null hypothesis. There is a tradition of using \\(\\alpha = 0.05\\) in the natural sciences. However, an increasing number of researchers believe that, and the author endorses that, such thresholds are arbitrary, and that researchers should \"simply report p-values and let the reader decide in context\". The statistical power of a statistical test is the probability of rejecting the null hypothesis when it is false. Selection of Statistical Test In different statistical tests, different probability distributions of the collected measures are assumed, and different aspects of the probability distributions of the collected measures are being compared. Common statistical tests include: parametric Student's t-test Welch's t-test F-test ANOVA nonparametric Fisher exact test Wilcoxon signed ranks test Mann-Whitney U-test When selecting a statistical test, tt is worth paying attention to the probability distributions of the collected measures: there may be a \"very strong departure from normality\" the mean and variance may not exist the data may be censored Effect Size Measurement In addition to using a statistic test to assess improvement of one algorithm over another, it is also critical to assess \"the magnitude of the improvement\", for which effect size measures are used. Unstandardized effect size measures: dependent on the unit of measurement difference in mean Standardized effect size measures: d family / Mahalanobis distance, assumes the normality of the data Common Language (CL) Statistic. The probability that a randomly selected score from the first population \\(X_1\\) is greater than a randomly selected score from the second population \\(X_2\\), \\(P(X_1 &gt; X_2)\\). Measure of Stochastic Superiority. A generalization of Common Language Statistic, \\(A_{12} = P(X_1 &gt; X_2) + 0.5 P(X_1 = X_2)\\). Recommended. Odds ratio. A measure of \"how many times greater the odds are that a member of a certain population will fall into a certain category than the odds are that a member of another population will fall into that category\". If the total number of runs is \\(n\\), and the number of times two algorithms find optimal solutions are \\(n_1\\) and \\(n_2\\), then the odds ratio is \\(\\psi = \\frac{\\frac{n_1}{n - n_1}}{\\frac{n_2}{n - n_2}}\\). Recommended. Multiple Statistical Tests and Effect Size Measurements When comparing \\(k\\) algorithms, we frequently would like to know the performance of each algorithm \"compared against all other alternatives individually\". This incurs \\(\\frac{k (k - 1)}{2}\\) comparisons. However, when doing multiple stastical tests, given a significant level \\(\\alpha\\) and the number of tests \\(n\\), the probability that at least one null hypothesis is true is \\(1 - {(1 - \\alpha)}^n\\), which converges to \\(1\\) as \\(n\\) increases. A remedy is the Bonferroni adjustment, in which we use an adjusted significant level \\(\\frac{\\alpha}{n}\\). However, this has been \"seriously criticized in the literature\", and the author recommends \"simply report p-values and let the reader decide in context\" instead.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Exploiting Dynamic Information in IDEs Improves Speed and Correctness of Software Maintenance Tasks","slug":"Paper-Reading-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks","date":"2022-10-16T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/16/Paper-Reading-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks/","link":"","permalink":"https://abbaswu.github.io/2022/10/16/Paper-Reading-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. Summary The pervasive use of inheritance, interfaces, and runtime polymorphism in object-oriented software systems leads to it being unclear which concrete method is invoked at a call site. Modern IDEs such as Eclipse offer static views of the source code, but provide little help in revealing the execution paths an object-oriented software system actually takes at runtime. In this situation, developers usually resort to debuggers or profilers. However, the information extracted by such tools are volatile, and developers cannot continuously benefit from dynamic information integrated in the static source views in the IDE. To solve this problem, the authors propose Senseo, an Eclipse plugin that collects dynamic information by running unit and/or system tests of the project with a customized JVM, that enriches the source views of Eclipse with dynamic information, such as: which concrete methods a particular method invokes, and how often which methods invoke this particular method how many objects are allocated in methods the dynamic collaborations between different source artifacts a visualization of the system's Calling Context Tree These are displayed in tooltips, ruler columns, the Package Explorer, and a dedicated Collaboration Overview. The authors conducted an experiment with 30 professional Java developers solving five typical software maintenance tasks in JEdit, an unfamiliar, medium-sized software system, measured the time and correctness of the tasks, and conducted statistical tests on the measurements. Senseo yields a significant decrease in time of 17.5 percent and a significant increase in correctness of 33.5 percent, which validates the practical usefulness of Senseo. Personal Thoughts There is no doubt that the idea of enriching the source views of an IDE with dynamic information, as well as its implementation Senseo, is of great practical value to developers writing object-oriented software systems. However, I do have a few concerns after reading the paper. To enrich the source views of Eclipse with dynamic information, Senseo runs unit and/or system tests of the project with a customized JVM. There are several concerns here. The project should have unit and/or system tests that thoroughly exercise all units in a manner resembling an actual execution of the project in production, otherwise, the dynamic information for some units may be missing and/or inaccurate. The unit and/or system tests should be self-contained and not rely on interacting with the environment, such as getting input from the user, using OS services, etc. If so, a possible remedy would be to carve unit tests from such executions. There is significant overhead in the process of collecting dynamic information. As the authors have reported: \"On average (geometric mean), CCT creation alone causes an overhead of factor 2.68. CCT creation and collection of dynamic information result in an overhead of factor 9.07. The total overhead, including serialization/transmission, is of factor 9.47.\" Although the authors claim that \"even though the overall overhead is high when gathering dynamic information, we do not consider this a major issue as the application does not need to run at productive speed while analyzing it\", this could be a problem for lengthy system tests, especially if units in the system tests are frequently modified, and new dynamic information has to be reacquired. Carving unit tests from such system tests would also be a possible remedy. Furthermore, aside from the idea and implementation of the tool, something else I appreciate and have learned from this paper is the experimental study, in which two measures, the time and correctness of the tasks, are selected, and statistical tests on the measurements are conducted. This convincingly proves the effectiveness of Senseo.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Breaking the Barriers to Successful Refactoring: Observations and Tools for Extract Method","slug":"Paper-Reading-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method","date":"2022-10-13T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/10/13/Paper-Reading-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method/","link":"","permalink":"https://abbaswu.github.io/2022/10/13/Paper-Reading-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. Refactoring is important to software development. Performing a refactoring is not trivial, for which refactoring tools have been developed. Nevertheless, programmers do not use refactoring tools as often as they could. To investigate this problem, the authors focus on one type of refactoring and one specific tool - the Extract Method tool in the Eclipse IDE. Fowler reports that Extract Method is \"one of the most common refactorings\", \"a key refactoring\" which if successful, means \"you can go on [to do] more refactorings\". The Extract Method tool in the Eclipse IDE it is a mature, non-trivial refactoring tool. Most refactoring tool user interfaces are very similar. The authors first conject tools are non-specific and unhelpful in diagnosing problems, and undertake a formative study observing 11 programmers perform a number of Extract Method refactorings on several large, open-source projects, which suggest that programmers fairly frequently encounter a variety of errors arising from violated refactoring preconditions. The authors further conjecture error messages were conflated, insufficiently descriptive, and discouraged programmers from refactoring, and built three visualization tools within the Eclipse IDE as solutions. Then, they conducted a study to assess whether or not the new tools overcome these usability problems by comparing the accuracy and time to complete refactoring tasks with and without the new tools, and administered a post-test questionnaire for the subjects to express their preferences. The results of the study were very positive, and subjects found the new tools superior and helpful outside of the context of the study. Finally, the authors provide recommendations for future tools. Code Selection: A selection tool should be lightweight, task-specific, and help the programmer overcome unfamiliar/unusual code formatting. Displaying Violated Preconditions: quickly comprehensible, indicate location, easily distinguishable from warnings and advisories, display amount of work required, display relations between precondition violations, distinguish different types of violations. The experimental study is very concise, and there are many aspects that can be borrowed. Undertaking a formative study to verify conjections about problems within current tools, before building new tools based on the verified conjections, and evaluating them. The visualization comparing the the accuracy and time of each participant to complete refactoring tasks with and without the new tools is accurate and straightforward. Using a questionnaire to acquire subjective feedback complimentary to an objective evaluation. However, there are still some flaws. Only one type of refactoring (Extract Method) and one specific tool was considered. The takeaways may not apply to other types of refactoring. Several key variates were not controlled in the formative study, such as participants were free to refactor whatever code they thought necessary. Future directions of work include: Replicating the study for other types of refactoring. Build and assess new refactoring tools with increased usability.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: QSYM: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing","slug":"Paper-Reading-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing","date":"2022-10-11T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/11/Paper-Reading-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing/","link":"","permalink":"https://abbaswu.github.io/2022/10/11/Paper-Reading-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? How was it addressed by prior work? There are two notable technologies to automatically find vulnerabilities in software: Coverage-guided fuzzing, quickly explores the input space, but only good at discovering inputs leading to an execution path with loose branch conditions Concolic execution, good at finding inputs driving the program into tight and complex branch conditions, but very expensive to formulate and solve constraints A hybrid approach, hybrid fuzzing, was recently proposed. The fuzzer will quickly explore trivial input spaces (loose conditions) The concolic execution will solve the complex branches (tight conditions) Still suffer from scaling to find real bugs in real-world applications. Bottlenecks are their concolic executors. The symbolic emulation is too slow in formulating path constraints, and it is often not even possible to generate constraints due to incomplete and erroneous environment models. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you? Concolic executors adopt IR in their symbolic emulation. Although IR makes implementation easy, it incurs additional overhead and blocks further optimization. According to our measurement with real-world software, only 30% of instructions require symbolic execution. This implies an instruction-level approach has an opportunity to reduce the number of unnecessary symbolic executions. Concolic execution engines use snapshot techniques to reduce the overhead of re-executing a target program when exploring its multiple paths. However, in hybrid fuzzing, test cases from the fuzzer are associated with greatly different paths, rendering snapshoting inefficient. Furthermore, snapshots cannot reflect external status, and solving this problem through full system concolic execution or external environment modeling is expensive and/or inaccurate. Concolic execution tries to guarantee soundness by collecting complete constraints. However, this can be expensive, and also over-constrain a path, limiting finding future paths. To solve these problems, Qsym uses Intel Pin along with a coverage-guided fuzzer: Get input test cases and validate newly produced test cases (potentially unsound) from the fuzzer. Employ instruction-level taint tracking, and only symbolically execute tainted instructions. Generate more relaxed (incomplete) forms of constraints that can be easily solved (can result in unsound test cases, but quickly checked with fuzzer). Fast execution makes re-execution much preferable to snapshoting for repetitive concolic testing. Considers external environments as \"black-boxes\" and simply executes them concretely (can result in unsound test cases, but quickly checked with fuzzer). Chooses the last constraint of a path for optimistic solving. It typically has a very simple form, and avoids solving irrelevant constraints repeatedly tested by fuzzers. This can be applied to other domains to speed up symbolic execution, if the domain has an efficient validator like a fuzzer. If a basic block has been executed too frequently in a context (a call stack of the current execution), Qsym stops generating further constraints from it. Extremely suitable for loops. This can directly be applied to other concolic executors as a heuristic path exploration strategy. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? A series of experiments are conducted. To highlight the effectiveness, we applied QSYM to non-trivial programs that are large in size and well-tested - all applications and libraries tested by OSS-Fuzz. To show how effectively our concolic executor can assist a fuzzer in discovering new code paths, we measured the achieved code coverage during the fuzzing process using Qsym and AFM with a varying number of input seed files. We selected libpng as a fuzzing target because it contained various narrow-ranged checks. To show the performance benefits of QSYM's symbolic emulation, we used the DARPA CGC dataset to compare QSYM with Driller, which placed third in the CGC competition. To evaluate the effect of optimistic solving, we compared Qsym with others using the LAVA dataset, a test suite that injects hard-to-find bugs in Linux utilities to evaluate bug-finding techniques. To show the effect of basic block pruning, we evaluated Qsym with and without this technique with four widely-used open-source programs - libjpeg, libpng, libtiff, and file. The author then analyzes new bugs found by Qsym. These experiments comprehensively assess different innovations and support the notion that Qsym \"scales to find real bugs in real-world applications\". However, I do have some questions concerning the experimental study, stated below. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? Qsym generates more relaxed (incomplete) forms of constraints that can be easily solved. Specifically how this is done is not clear. Questions concerning the experimental study: The experiments \"to highlight the effectiveness\" and \"to show the performance benefits of QSYM's symbolic emulation\" seem to be redundant. To show how effectively our concolic executor can assist a fuzzer in discovering new code paths, we compared Qsym with AFM on libpng, because it contained various narrow-ranged checks. The benchmark appears to be cherry-picked. This is also the case with \"to show the effect of basic block pruning\". Why are completely different datasets used in different experiments? Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? The coverage-guided fuzzer used within Qsym is \"vanilla\" AFL. Other coverage-guided fuzzers exist that enhance AFL. How Qsym can complement these fuzzers can be a direction for future research. Unlike other IR-based executors, QSYM cannot test programs targeting other architectures. We plan to overcome this limitation by improving QSYM to work with architecture specifications, rather than a specific architecture implementation. (Is taint analysis on IR+JIT also a possible solution?) QSYM currently supports only memory, arithmetic, bitwise, and vector instructions. Other instructions, including floating-point operations, remain to be supported.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Semantic Fuzzing with Zest","slug":"Paper-Reading-Semantic-Fuzzing-with-Zest","date":"2022-10-11T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/11/Paper-Reading-Semantic-Fuzzing-with-Zest/","link":"","permalink":"https://abbaswu.github.io/2022/10/11/Paper-Reading-Semantic-Fuzzing-with-Zest/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? The paper tackles the problem of generating random, syntactically valid inputs to exercise various code paths in the semantic analysis stages of programs and leveraging feedback to generate new inputs via mutations. How was it addressed by prior work? On one hand, QuickCheck-like random-input generators allow generating random, syntactically valid inputs. On the other hand, coverage-guided fuzzing tools such as AFL and libFuzzer randomly mutate known byte sequences to produce new byte sequences, and if the mutated byte sequences lead to new code coverage in the test program, they are saved for subsequent mutation. What are the innovation(s) proposed in this paper? The paper proposes Zest, a technique for automatically guiding QuickCheck-like random-input generators to exercise various code paths in the semantic analysis stages of programs. It first converts a QuickCheck-like random-input generator to a parametric generator, which can generate a syntactically valid input from a byte sequence. It then uses a coverage-guided fuzzing technique with the parametric generator in order to produce syntactically valid input that can increase code coverage in the semantic analysis stages. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? The authors integrated Zest into the open-source JQF framework and evaluated Zest on five real-world Java benchmarks, comparing it to QuickCheck and AFL. They evaluated the three techniques on two fronts: The amount of code coverage achieved in the semantic analysis stage after a fixed amount of time. Their effectiveness in triggering bugs in the semantic analysis stage. QuickCheck and Zest make use of generators for synthesizing syntactically valid input, and do not exercise code paths corresponding to parse errors in the syntax analysis stage. In contrast, AFL performs mutations directly on raw input strings, and spends most of its time testing error paths within the syntax analysis stages. The experimental results suggest that when given QuickCheck-like random-input generators, Zest excels at exercising semantic analyses and is very effective at discovering semantic bugs. The paper's evaluation matches well with the proposed problem statement, as the experimental design accurately assesses factors directly correlated with the problem of \"generating random, syntactically valid inputs to exercise various code paths in the semantic analysis stages of programs and leveraging feedback to generate new inputs via mutations\", and the experimental results support the effectiveness of the proposed approach. Which technical innovations are most compelling to you? The most compelling technical innovation is Zest's design of generating a syntactically valid input from a byte sequence given a QuickCheck-like random-input generator, by using bytes from the byte sequence to \"fill in\" randomly generated primitive data types of various length (bool, char, int, etc.) required within the random-input generator. This allows bit-level mutations on byte sequences to correspond to high-level structural mutations in the space of syntactically valid inputs, enabling Zest to leverage the mature coverage-guided fuzzing algorithm originally designed for byte sequence inputs. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? The author states that the Zest algorithm \"extends the CGF algorithm by keeping track of the coverage achieved by semantically valid inputs\", and that \"we hypothesize that this biases the search towards generating even more valid inputs and in turn increases code coverage in the semantic analysis stage\". However, how semantically valid inputs are used is not stated in the description of the algorithm. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome? Zest assumes the availability of QuickCheck-like random-input generators to exercise the semantic analysis classes and find semantic bugs, which may be unavailable for specialized data structures. There has also been some recent interest in automatically generating input grammars from existing inputs, using machine learning and language inference algorithms. These techniques are complementary to Zest - the grammars generated by these techniques could be transformed into parametric generators for Zest.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: How We Refactor, and How We Know It","slug":"Paper-Reading-How-We-Refactor-and-How-We-Know-It","date":"2022-10-10T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/10/Paper-Reading-How-We-Refactor-and-How-We-Know-It/","link":"","permalink":"https://abbaswu.github.io/2022/10/10/Paper-Reading-How-We-Refactor-and-How-We-Know-It/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? How was the work validated? In his book on refactoring, Fowler catalogs 72 different refactorings, ranging from localized changes to more global changes, and Fowler claims that refactoring produces significant benefits. Although case studies have demonstrated that refactoring is a common practice and can improve code metrics, they tend to examine just a few software products. To help put refactoring research on a sound scientific basis, we replicate the study in wider contexts and explore factors that previous authors may not have explored. We analyze four sets of Eclipse IDE usage data and apply different several different refactoring-detection strategies to them. We then use this data to test nine hypotheses about refactoring, casting doubt on several previously stated assumptions about how programmers refactor, while validating others. Refactoring behavior of refactoring tool developers differs from that of their users. Specifically, RENAMEs and MOVEs are more frequent among users. About 40% of refactorings performed using a tool occur in batches (several refactorings of the same kind within a short time period). About 90% of configuration defaults of refactoring tools remain unchanged when programmers use the tools. messages written by programmers in commit logs do not reliably indicate the presence of refactoring. Programmers frequently floss refactor (interleave refactoring with other types of programming activity). About half of refactorings are not high-level, so refactoring detection tools that look exclusively for high-level refactorings will not detect them. Refactorings are performed frequently. Almost 90% of refactorings are performed manually, and the kinds of refactorings performed with tools differ from the kinds performed manually. How could this research be extended? How could this research be applied in practice? For the toolsmith: Most kinds of refactorings will not be used as frequently as the toolsmiths hoped. Improving the under-used tools or their documentation may increase tool use. Programmers often do not configure refactoring tools. Configuration-less refactoring tools, which have recently seen increasing support in Eclipse and other environments, will suit the majority of, but not all, refactoring situations. 30 refactorings did not have tool support, the most popular of these was MODIFY ENTITY PROPERTY, performed 8 times, which would allow developers to safely modify properties such as static or final. For researchers: Questions still remain to answer. Why is the RENAME refactoring tool so much more popular than other refactoring tools? Why do some refactorings tend to be batched while others do not? Our experiments should be repeated in other projects and for other refactorings to validate our findings. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? Of particular interest to me is the inspiration for the hypothesis the authors verify - previous literature (frequently in other software engineering domains), personal experience, anecdotes from programmers, surveys. The benefit from this is twofold. First, it provides a source of inspiration for formulating hypotheses. Second, it endorses the validity of the hypotheses. We hypothesize refactoring behavior of refactoring tool developers differs from that of their users. Toleman and Welsh assume a variant of this hypothesis - that the designers of software tools erroneously consider themselves typical tool users - and argue that the usability of software tools should be objectively evaluated. We hypothesize that programmers typically perform refactoring in batches. Based on personal experience and anecdotes from programmers, we suspect that programmers often refactor several pieces of code because several related program elements may need to be refactored in order to perform a composite refactoring. In previous research, Murphy-Hill and Black built a refactoring tool that supported refactoring several program elements at once, on the assumption that this is common. We hypothesize that programmers do not often configure refactoring tools. We suspect this because tweaking code manually after the refactoring may be easier than configuring the tool. In the past, we have found some limited evidence that programmers perform only a small amount of configuration of refactoring tools. When we did a small survey in September 2007 at a Portland Java Users Group meeting, 8 programmers estimated that, on average, they supply configuration information only 25% of the time. In Xing and Stroulia's automated analysis of the Eclipse codebase, the authors conclude that \"indeed refactoring is a frequent practice\". Although flawed, this becomes one of the authors' hypotheses. Furthermore, some hypotheses are formed from a critique of previous literature, combined with domain expertise and/or other literature. Several researchers have used messages attached to commits into a version control as indicators of refactoring activity. However, we hypothesize that this assumption is false, because refactoring may be an unconscious activity, and because the programmer may consider it subordinate to some other activity, such as adding a feature. Past research has often drawn conclusions based on observations of high-level refactorings. We hypothesize that in practice programmers also perform many lower-level refactorings. We suspect this because lower-level refactorings will not change the program's interface and thus programmers may feel more free to perform them. Additionally, much of the methodology presented in this paper can be borrowed. The fourth dataset used by the authors is Eclipse CVS, the version history of the Eclipse and JUnit code bases extracted from their Concurrent Versioning System (CVS) repositories. CVS does not maintain records showing which file revisions were committed as a single transaction. The standard approach for recovering transactions is to find revisions committed by the same developer with the same commit message within a small time window; we use a 60 second time window. In our experiments, we randomly sampled from about 3400 source file commits that correspond to the same time period, the same projects, and the same developers represented in Toolsmiths. Using these data, two of the authors inferred which refactorings were performed by comparing adjacent commits manually. Ratzinger describes the most sophisticated strategy for finding refactoring messages: searching for the occurrence of keywords such as \"move\" and \"rename\", and excluding \"needs refactoring\". We replicated Ratzinger's experiment for the Eclipse code base to nullify Ratzinger's conclusions. In order for refactoring activity to be defined as frequent, we seek to apply criteria that require refactoring to be habitual and occurring at regular intervals. First, we examined the Toolsmiths data to determine how refactoring activity was spread throughout development. Second, we examined the Users data to determine how often refactoring occurred within a programming session and whether there was significant variation among the population. We hypothesize that programmers often do not use refactoring tools, because existing tools may not have a sufficiently usable user-interface. To validate this hypothesis, we correlated the refactorings that we observed by manually inspecting Eclipse CVS commits with the refactoring tool usages in the Toolsmiths data set.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: \"Cloning Considered Harmful: Considered Harmful","slug":"Paper-Reading-Cloning-Considered-Harmful-Considered-Harmful","date":"2022-10-05T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/10/05/Paper-Reading-Cloning-Considered-Harmful-Considered-Harmful/","link":"","permalink":"https://abbaswu.github.io/2022/10/05/Paper-Reading-Cloning-Considered-Harmful-Considered-Harmful/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How was the work validated? Current literature on the topic of duplicated code in software systems often considers duplication harmful to the system quality, and the reasons commonly cited for duplicating code often have a negative connotation. While these positions are sometimes correct, during our case studies we have found that this is not universally true, and we have found several situations where code duplication seems to be a reasonable or even beneficial design option. This paper introduces eight cloning patterns that we have uncovered during case studies on large software systems, and discusses the advantages and disadvantages associated with using them. Forking, cloning used to bootstrap development of similar solutions, with the expectation that evolution of the code will occur somewhat independently Hardware variation Platform variation Experimental variation Templating, directly copy behavior of existing code but appropriate abstraction mechanisms are unavailable Boiler-plating due to language in-expressiveness API/Library protocols General language or algorithmic idioms Customization, currently existing code does not adequately meet a new set of requirements Bug workarounds Replicate and specialize What were the main contributions of the paper as you (the reader) see it? How does this work move the research forward? How could this research be extended? This paper introduces the notion of categorizing high level patterns of cloning in a similar fashion to the cataloging of design patterns or anti-patterns. There are several benefits that can be gained from this characterization. It provides a flexible framework on top of which we can document our knowledge about how and why cloning occurs in software. This documentation crystallizes a vocabulary that researchers and practitioners can possibly use to communicate about cloning. This categorization is a first step towards formally defining these patterns to aid in automated detection and classification. These classifications can then be used to define metrics concerning code quality and maintenance efforts. Automatic classifications will also provide us with better measures of code cloning in software systems and severity of the problem in general. How could this research be applied in practice? In each uncovered cloning pattern, the author describes its advantages, disadvantages, how it can be managed, issues to be aware of when deciding to use it as a long-term solution, as well as real examples in large software systems. These provide practical guidelines when considering a trade-off between code cloning and formulating abstractions for code reuse, as well as how to manage code cloning should it be used, when developing a software project.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Go To Statement Considered Harmful","slug":"Paper-Reading-Go-To-Statement-Considered-Harmful","date":"2022-10-04T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/04/Paper-Reading-Go-To-Statement-Considered-Harmful/","link":"","permalink":"https://abbaswu.github.io/2022/10/04/Paper-Reading-Go-To-Statement-Considered-Harmful/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. The author has been familiar with the observation that the quality of programmers is a decreasing function of the density of go to statements in the programs they produce, and in this paper, he explains why the use of the go to statement has negative effects. He first remarks that the process taking place under control of the program, instead of the program itself, is the true subject matter of a programmer's activity, and it is this process whose behavior has to satisfy the desired specifications. He then argues that our intellectual powers can better master static relations than visualize processes evolving in time, for which reason we should shorten the conceptual gap between the static program and the dynamic progress. The author continues characterizing the progress of a progress, explaining that it can be uniquely characterized by a mixed sequence of textual and/or dynamic indices, when conditionals, procedures, and repetition clauses are considered. However, the unbridled use of the go to statement has an immediate consequence that it becomes terribly hard to find a meaningful set of coordinates in which to describe the process progress, which will in turn \"make a mess of one's program\". However, in my opinion, although the go to statement is considered harmful, abolishing the go to statement from all \"higher level\" programming languages is an overstatement. As the author himself stated: The exercise to translate an arbitrary flow diagram more or less mechanically into a jump-less one, is not to be recommended. Then the resulting flow diagram cannot be expected to be more transparent than the original one. There exist situations where an \"arbitrary flow diagram\" has to be implemented (especially when implementing Finite-State Machines in lexers, regex engines, and protocols), and in these situations, implementing the flow diagram using go to statements is much more direct, straightforward and easier to reason about (not to mention more efficient) than mashing up structured programming constructs.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: KLEE: unassisted and automatic generation of high-coverage tests for complex systems programs","slug":"Paper-Reading-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs","date":"2022-10-04T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/10/04/Paper-Reading-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs/","link":"","permalink":"https://abbaswu.github.io/2022/10/04/Paper-Reading-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? How was it addressed by prior work? Many classes of errors are difficult to find without executing a piece of code. The importance of such testing, combined with the difficulty and poor performance of random and manual approaches, has led to much work in using symbolic execution to automatically generate test inputs. It has been an open question whether the approach has any hope of consistently achieving high coverage on real applications, facing the challenges in handling code that interacts with the environment, and the exponential number of paths through code. Traditional symbolic execution systems either cannot handle programs interacting with the environment or require a complete working model. More recent work in test generation does allow external interactions, but forces them to use entirely concrete procedure call arguments, which limits the behaviors they can explore. For the path explosion problem, search strategies proposed in the past include Best First Search, Generational Search, and Hybrid Concolic Testing. Orthogonal to search heuristics, researchers have addressed the path explosion problem by testing paths compositionally, and by tracking the values read and written by the program. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you? KLEE interprets programs compiled to LLVM IR, and typically requires no source modification. It functions as a hybrid between an operating system for symbolic processes and an interpreter. Each symbolic process has a register file, stack, heap, program counter, and path condition. Unlike a normal process, storage locations for a symbolic process - registers, stack and heap objects - refer to expression trees instead of raw data values. The leaves of an expression are symbolic variables or constants, and the interior nodes come from LLVM IR operations. Conditional branches take a boolean expression and alter the instruction pointer of the symbolic process based on whether the condition is true or false. KLEE queries the constraint solver to determine if the branch condition is either provably true or false along the current path. If so, the instruction pointer is updated to the appropriate location. Otherwise, both branches are possible. KLEE forks the symbolic process so that it can explore both paths. The number of forked symbolic processs grows quite quickly in practice. KLEE implements the heap as an immutable map, and portions of the heap structure itself can also be shared amongst multiple symbolic processs. Additionally, this heap structure can be forked in constant time, which is important given the frequency of this operation. Potentially dangerous operations implicitly generate branches that check if any input value exists that could cause an error. For example, a division instruction generates a branch that checks for a zero divisor. If so, KLEE solves the current path's constraints to produce a test case that will follow the same path when rerun on an unmodified version of the checked program, and terminates the current symbolic process. KLEE will then continue execution on the false path, which adds the negation of the check as a constraint (e.g., making the divisor not zero). The core of KLEE is an interpreter loop which selects a symbolic process to run and then symbolically executes a single instruction in the context of that symbolic process. Given more than one symbolic process, KLEE must pick which one to execute first. KLEE selects the symbolic process to run at each instruction by uses each strategy in a round robin fashion. - Random Path Selection: Use a binary tree to record the program path followed for all active symbolic processs. A symbolic process is selected by traversing this tree from the root and randomly selecting the path to follow at branch points. This strategy has two important properties. - Favors symbolic processs high in the branch tree. They have less constraints on their symbolic inputs and have greater freedom to reach uncovered code. - Avoids starvation when some part of the program is rapidly creating new symbolic processs (\"fork bombing\") as it happens when a tight loop contains a symbolic condition. - Coverage-Optimized Search: Select symbolic processs likely to cover new code in the immediate future using heuristics. This loop continues until there are no symbolic processs remaining, or a user-defined timeout is reached. KLEE ensures that a symbolic process which frequently executes expensive instructions will not dominate execution time by running each symbolic process for a \"time slice\" defined by both a maximum number of instructions and a maximum amount of time. KLEE uses STP as its constraint solver. KLEE maps every memory object in the checked code to a distinct STP array. This representation dramatically improves performance since it lets STP ignore all arrays not referenced by a given expression. Furthermore, there are tricks to simplify expressions and ideally eliminate queries before they reach STP, including: Expression Rewriting Constraint Set Simplification Implied Value Concretization Constraint Independence Counter-example Cache: Redundant queries are frequent, and a simple cache is effective at eliminating a large number of them. However, it is possible to build a more sophisticated cache due to the particular structure of constraint sets. The counter-example cache maps sets of constraints to counter-examples (i.e., variable assignments), along with a special sentinel used when a set of constraints has no solution. This mapping is stored in a custom data structure  derived from the UBTree structure of Hoffmann and Hoehler, which allows efficient searching for cache entries for both subsets and supersets of a constraint set. By storing the cache in this fashion, the counter-example cache gains three additional ways to eliminate queries. When a subset of a constraint set has no solution, then neither does the original constraint set. When a superset of a constraint set has a solution, that solution also satisfies the original constraint set. When a subset of a constraint set has a solution, it is likely that this is also a solution for the original set. KLEE handles the environment by redirecting library calls to models that understand the semantics of the desired action well enough to generate the required constraints. The real environment can fail in unexpected ways. Such failures can often lead to unexpected and hard to diagnose bugs. To help catch such errors, KLEE will optionally simulate environmental failures by failing system calls in a controlled manner. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? Four sets of experiments are conducted. We do intensive runs to both get high coverage and find bugs on Coreutils and BusyBox tools, do a comparision with random tests and developer test suites, and discuss the bugs found. To demonstrate KLEE's applicability to bug finding, we used KLEE to check all 279 BusyBox tools and 84 MINIX tools in a series of short runs. Thus far, we have focused on finding generic errors that do not require knowledge of a program's intended behavior. We now show how to do much deeper checking, including verifying full functional correctness on a finite set of explored paths. We use KLEE to find deep correctness errors by cross-checking purportedly equivalent Coreutils and BusyBox tool implementations. We have also applied KLEE to checking non-application code by using it to check the HiStar kernel. We chose line coverage as reported by gcov as a conservative measure of KLEE-produced test case effectiveness, because it is widely-understood and uncontroversial. The results of the experiments are very positive, and convincingly prove the proposed problem statement. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? Coverage-Optimized Search tries to select symbolic processs likely to cover new code in the immediate future. It uses heuristics to compute a weight for each symbolic process and then randomly selects a symbolic process according to these weights. How these heuristics work, which is critical for performance, is not symbolic processd, and remains unclear. KLEE ensures that a symbolic process which frequently executes expensive instructions will not dominate execution time by running each symbolic process for a \"time slice\" defined by both a maximum number of instructions and a maximum amount of time. Precisely how this \"time slice\" is calculated is also unclear. KLEE handles the environment by redirecting library calls to models that understand the semantics of the desired action well enough to generate the required constraints. These models are written in normal C code which the user can readily customize, extend, or even replace without having to understand the internals of KLEE. However, what \"understand the semantics of the desired action well enough\" means is unclear. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? KLEE does not currently support symbolic floating point, longjmp, threads, and assembly code. Additionally, memory objects are required to have concrete sizes. These block KLEE's application towards floating point-heavy scientific computation and data science code, and may also limit KLEE to simple programming languages such as C, not supporting the numerous dynamics, including exception handling, within C++.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: CUTE: A Concolic Unit Testing Engine for C","slug":"Paper-Reading-CUTE-A-Concolic-Unit-Testing-Engine-for-C","date":"2022-10-02T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/10/02/Paper-Reading-CUTE-A-Concolic-Unit-Testing-Engine-for-C/","link":"","permalink":"https://abbaswu.github.io/2022/10/02/Paper-Reading-CUTE-A-Concolic-Unit-Testing-Engine-for-C/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. NOTE: I believe the paper to be written very obscurely, so I will explain the ideas of the paper in my own words. What is the problem being tackled? How was it addressed by prior work? Unit testing is a method for modular testing of a program's functional behavior. Such testing requires specification of values for the inputs (or test inputs) to the unit. Manual specification of such values is labor intensive and cannot guarantee that all possible behaviors of the unit will be observed during the testing. Several techniques have been proposed to automatically generate values for the inputs. Randomly choose the values over the domain of potential inputs Many values may lead to the same behavior and are redundant. The probability of selecting inputs causing buggy behavior may be astronomically small. Symbolic Exection Addresses the problem of redundant executions and increases test coverage For large or complex units, it is intractable to maintain and solve the constraints required for test generation Incrementally generating test inputs by combining concrete and symbolic execution During a concrete execution, a conjunction of symbolic constraints along the path of execution is generated. These constraints are modified and then solved to generate further test inputs to direct the program along alternative paths. If it is not feasible to solve, simply substitute random concrete values. This problem is particularly complex for programs with dynamic data structures using pointer operations. Pointers may have aliases. In this paper, we provide a method for representing and solving approximate pointer constraints to generate test inputs. Our method is thus applicable to a broad class of sequential programs. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you? We consider the execution of a function to be determined by all the stack variables, global variables, and heap objects it exercises. Only primitive types and pointer types are taken into consideration. For structures and arrays, each member is considered to be a separate variable. External OS services are not modelled. We associate the following properties with each stack variable, global variable, and heap object. Concrete Value Symbolic Value Concrete Address Symbolic Address The branches taken within an execution can be described with a predicate sequence called a path constraint. Each predicate is described using the aforementioned stack variables, global variables, and/or heap objects. Symbolic values are used when available, otherwise, concrete values are used. Predicates involving primitive types are of the form \\(a_1 x_1 + \\dots + a_n x_n + c~R~0, R \\in \\{&lt;, &gt;, \\le, \\ge, =, \\ne\\}\\), where \\(a_i, \\dots, a_n, c\\) are integer constants. (Essentially considers only linear combinations of primitive types) Predicates involving pointers are of the form \\(x~R~y\\) or \\(x~R~NULL\\), \\(R \\in \\{=, \\ne\\}\\). (Essentially considers only being able to assign to a pointer NULL or another previously known address, and does not allow converting integers to pointers) Running process of CUTE. while True: Execute, in the process: When allocating a stack variable, global variable, or heap object without initialization (incl. function parameters): Modify \"known stack variables, global variables, and heap objects\" if needed. If its concrete value has been stored, initialize it to its stored concrete value. Otherwise, generate a random concrete value for it. Record its concrete value and concrete address. When allocating a stack variable, global variable, or heap object with initialization: Modify \"known stack variables, global variables, and heap objects\" if needed. Record its concrete value and concrete address. Record its symbolic value and symbolic address. When assigning an existing stack variable, global variable, or heap object: Update its concrete value. Update its symbolic value. When taking a branch, add a new predicate to the path constraint. After execution, negate the last predicate within the path constraint, and solve for the concrete values of \"stack variables, global variables, and heap objects allocated without initialization\". Update their recorded concrete values. Solving optimizations: Check if the last predicate is syntactically the negation of any preceding predicate Identify and eliminate common arithmetic subconstraints. Identify dependencies between predicates and exploit them. The path constraints from two consecutive concolic executions, \\(C\\) and \\(C&#39;\\) differ only in a small number of predicates, and their respective solutions are similar. The solver collects all the predicates in C that are dependent on the negation of the last and solves for them. In practice, we have found that the size of this set is almost one eighth the size of \\(C\\) on average. Generated random concrete values: Primitive Type: random number Pointer Type: NULL We next consider testing of functions that take data structures as inputs. We want to test such functions with valid inputs only. There are two main approaches to obtaining valid inputs: Generating inputs with call sequences Use the functions that check if an input is a valid data structure by solving them, i.e., generating input for which they return true. Previous techniques include a search that uses purely concrete execution and a search that uses symbolic execution for primitive data but concrete values for pointers. CUTE, in contrast, uses symbolic execution for both primitive data and pointers. This allows it to solve these functions asymptotically faster than the fastest previous techniques. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? We illustrate two case studies that show how CUTE can detect errors. We applied CUTE to test its own data structures. Our goal in this case study was to detect memory leaks in addition to standard errors such as segmentation faults, assertion violation etc. We also applied CUTE to unit test SGLIB version 1.0.1, a popular, open-source C library for generic data structures. We chose SGLIB as a case study primarily to measure the efficiency of CUTE. We found two bugs in SGLIB using CUTE. The case studies showcase the power of CUTE's concolic unit testing approach, and match well with the proposed problem statement. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? After execution, negate the last predicate within the path constraint, and solve for the concrete values of \"stack variables, global variables, and heap objects allocated without initialization\". A solving optimization that the author proposed is \"identifing and eliminating common arithmetic subconstraints\". However, how this is done is not explained. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? For structures and arrays, each member is considered to be a separate variable. Although this facilicates analysis, this could incur significant overhead and impede scalability. External OS services are not modelled. Predicates involving primitive types are of the form \\(a_1 x_1 + \\dots + a_n x_n + c~R~0, R \\in \\{&lt;, &gt;, \\le, \\ge, =, \\ne\\}\\), where \\(a_i, \\dots, a_n, c\\) are integer constants. This essentially considers only linear combinations of primitive types. The author shows preference to using the technique of \"using the functions that check if an input is a valid data structure by solving them\" to solve the problem of testing of functions that take data structures as inputs. However, such an approach may be impossible for object-oriented languages such as C++, in which data structures are encapsulated in classes, and the logic of validness is enforced with the constructor and public methods of the classes.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Selection and Presentation Practices for Code Example Summarization","slug":"Paper-Reading-Selection-and-Presentation-Practices-for-Code-Example-Summarization","date":"2022-09-28T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/09/28/Paper-Reading-Selection-and-Presentation-Practices-for-Code-Example-Summarization/","link":"","permalink":"https://abbaswu.github.io/2022/09/28/Paper-Reading-Selection-and-Presentation-Practices-for-Code-Example-Summarization/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? How could this research be applied in practice? Code examples are important in modern software development. As part of the first steps toward automatic source-to-source summarization, the authors studied how humans summarize examples to understand how to automate the process, and propose empirically-supported hypotheses justifying the use of specific practices. Selection Practices - Practices Related to Language Constructs - Practices Based on Query Term - Practices Considering the Human Reader Presentation Practices - Trimming a Line When Needed - Compressing a Large Amount of Code - Truncating Code - Formatting Code for Readability - Improving Code The results provide a grounded basis for the development of code example summarization and presentation technology. How was the work validated? We chose a well-defined corpus of programming documents, The Official Android API Guides, which contains a mix of natural-language text and code fragments. We collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. We analyzed common practices behind these decisions across the hand-generated representations, as well as the rationale behind the practices. What were the main contributions of the paper as you (the reader) see it? In my opinion, aside from the obvious contributions of the paper presented by the author, there is a lot to learn from the study set-up and the conceptual framework for interpreting the results. To understand the rationale behind the practices, we instructed the participants to verbalize their thought process using the think-aloud protocol. We distinguished practices concerning the type of content selected and the way the content was presented in a summary, because even summaries with content associated with the same part of the original fragment could vary on how to present the summary. To make hypotheses justifying the use of different practices, we relied on a quantitative analysis of the distribution of each practice across code fragments and participants. In-lined histograms presents the distribution of observations of a given practice for the participants over the code fragments. This provides a convenient and compact assessment of the amount of evidence for a practice. Furthermore, the authors have borrowed a lot from related domains of research, including natural language generation, natural language summarization of code, etc. Some examples: The separation of content selection from presentation is typical in a natural language generation system. The comments demonstrated a number of different ways to abstract content, including aggregating lexically and aggregating semantically - natural language generation terminology. Seven participants injected additional natural language into the code summaries. This motivates a novel type of transformations that mix code and text. The only work we know of in this area is the natural summaries generated by Rastkar et al. This gives revelations on exploiting knowledge from related domains when doing our own research. How could this research be extended? The goal of the study was to inform the design of concise representations of source code and automatic summarization algorithms. A natural future direction is to implement these representations and algorithms, and conduct empirical studies assessing their usefulness in summarizing source code.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Finding and Understanding Bugs in C Compilers","slug":"Paper-Reading-Finding-and-Understanding-Bugs-in-C-Compilers","date":"2022-09-24T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/09/24/Paper-Reading-Finding-and-Understanding-Bugs-in-C-Compilers/","link":"","permalink":"https://abbaswu.github.io/2022/09/24/Paper-Reading-Finding-and-Understanding-Bugs-in-C-Compilers/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? Finding compiler bugs, especially bugs in the \"middle end\" of a compiler that performs transformations on an intermediate representation, to improve the quality of C compilers. How was it addressed by prior work? Compilers have been tested using randomized methods for nearly 50 years. In 1998, McKeeman coined the term \"differential testing\". His work resulted in DDT, a family of program generators that conform to the C standard at various levels. However, DDT avoided only a small subset of all undefined behaviors, and only then during test-case reduction, not during normal testing. Thus, it is not a suitable basis for automatic bug-finding. Lindig used randomly generated C programs to find several compiler bugs related to calling conventions. His tests are self-checking, but far less expressive than Csmith. Sheridan also used a random generator to find bugs in C compilers. Sheridan's tool produces self-checking tests. However, it is less expressive than Csmith and it fails to avoid undefined behavior such as signed overflow. Zhao et al. created an automated program generator for testing an embedded C++ compiler, which allows a general test requirement, such as which optimization to test, to be specified. What are the innovation(s) proposed in this paper? The paper proposes Csmith, a randomized test-case generation tool which generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. This advances the state of the art in compiler testing. Csmith supports compiler bug-hunting using differential testing. Csmith generates a C program, a test harness then compiles the program using several compilers, runs the executables, and compares the outputs. How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? The authors conducted five experiments. Finding and reporting bugs in a a variety of C compilers over a three-year period. They have found and reported more than 325 bugs in mainstream C compilers including GCC, LLVM, and commercial tools. Compiling and running one million random programs using several years' worth of versions of GCC and LLVM, to understand how their robustness is evolving over time. Evaluating Csmith's bug-finding power as a function of the size of the generated C programs. Comparing Csmith's bug-finding power to that of four previous random C program generators. Investigating the effect of testing random programs on branch, function, and line coverage of the GCC and LLVM source code. The experiments thoroughly evaluate and demonstrate Csmith's bug-finding power and provide guidelines for using Csmith to find bugs. Which technical innovations are most compelling to you? Csmith uses randomized differential testing. This has the advantage that no oracle for test results is needed. It exploits the idea that if one has multiple, deterministic implementations of the same specification, all implementations must produce the same result from the same valid input. When two implementations produce different outputs, one of them must be faulty. Given three or more implementations, a tester can use voting to heuristically determine which implementations are wrong. How Csmith designs the results used for differential testing is also worthwhile. A Csmith-generated program prints a value summarizing the computation performed by the program, which is implemented as a checksum of the program's non-pointer global variables at the end of the program's execution. Thus, if changing the compiler or compiler options causes the checksum emitted by a Csmith-generated program to change, a compiler bug has been found. Also compelling are the mechanisms that Csmith uses to avoid generating C programs that execute undefined behaviors or depend on unspecified behaviors, including performing incremental pointer and dataflow analysis in the process of generating programs. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? In the process of randomly generating programs, Csmith randomly selects an allowable production from its grammar for the current program point. To make the choice, it consults a probability table and a filter function specific to the current point: there is a table/filter pair for statements, another for expressions, and so on. The table assigns a probability to each of the alternatives, where the sum of the probabilities is one. However, how this probability table is constructed and maintained, which obviously is critical to generating high-quality random programs, is not stated in the paper, and requires clarification. Do you forsee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome? Which problems remain unsolved after this paper? The most important language features not currently supported by Csmith are strings, dynamic memory allocation, floating-point types, unions, recursion, and function pointers. These are language features that are ubiquitous in real-world programs, thus, not supporting them is a serious barrier to the applicability of Csmith. The authors plan to add some of these features to future versions of our tool. Although Csmith-generated programs allowed discovering bugs missed by compilers' standard test suites, branch, function, and line coverage of the GCC and LLVM source code did not significantly improve compared to the compilers' existing test suites. 'Coverage-guided' fuzzing may represent a future direction of research to discover more bugs lurking in unvisited sections of compiler source code.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges","slug":"Paper-Reading-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges","date":"2022-09-22T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/09/22/Paper-Reading-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges/","link":"","permalink":"https://abbaswu.github.io/2022/09/22/Paper-Reading-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How could this research be applied in practice? The paper conducts an empirical study of the effectiveness and challenges of automatically generated unit tests at finding real faults, and derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. Improving the obtained code coverage so that faulty statements are executed in the first instance. A high code coverage ratio does not necessarily indicate that the bug was covered. Improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, is also required. Improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time. How was the work validated? The authors applied three state-of-the art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. To account for randomness in test generation, we generated 10 test suites for each tool and fault. Tools may generate flaky tests, which may also fail on the fixed version. They are automatically removed. Even if a test is not flaky, it might still fail on the buggy version for reasons unrelated to the actual fault. Such false positives are identified. For each executed test, we collected information on whether it passed or failed, and the reason of failure. In order to study how code coverage relates to fault detection, we measured statement coverage, and also bug coverage - whether a fault was 1) fully covered (all modified statements covered), 2) partially covered (some modified statements covered), or 3) not covered. To gain insights on how to increase the fault detection rate of test generation tools, the authors did case studies on the challenges that prevent fault detection, and studied the root causes for flaky and false-positive tests. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? How could this research be extended? The revelations from the case studies supporting the primary contributions of the paper as the author sees it are particularly important, as they identify specific challenges and provide plausible solutions for increasing the fault detection rate of test generation tools. Creation of complex objects, such as a control flow graph, which often requires a certain sequence of prior method calls. Viable solutions include seeding objects observed at runtime, mining of common usage patterns of objects to guide object creation, or carving of complex object states from system tests. Complex strings satisfying a certain syntax. Search-based tools are capable in principle of generating string inputs, but doing so can take very long. Symbolic approaches using string solvers or dedicated solvers for regular expressions are generally restricted to fixed length strings. If an input grammar is known, this can be used to generate test data more efficiently. Complex conditions which randomly initialized inputs are unlikely to satisfy. Dynamic symbolic execution would not suffer from this problem. Errors are not propagated. To some extent, this is the result of focusing on simple structural criteria such as branch coverage, rather than aiming to exercise more complex intra-class data flow dependencies. Environmental dependencies and dependencies on the static state of the system under test resulting in flaky tests. Aggressive mocking, which monitors and asserts on the internal state (e.g. the order of method calls) of the class under test, rather than testing the class on what its public method returns, and its side effects.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: The Art of Testing Less without Sacrificing Quality","slug":"Paper-Reading-The-Art-of-Testing-Less-without-Sacrificing-Quality","date":"2022-09-21T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/09/21/Paper-Reading-The-Art-of-Testing-Less-without-Sacrificing-Quality/","link":"","permalink":"https://abbaswu.github.io/2022/09/21/Paper-Reading-The-Art-of-Testing-Less-without-Sacrificing-Quality/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? For large complex software products, there is a need to check that changes do not negatively impact other parts of the software and they comply with system constraints such as backward compatibility, performance, security etc. Ensuring these system constraints may require complex test procedures, but long tests conflict with strategic aims to shorten release cycles. To accelerate test processes without sacrificing product quality, the paper develops a cost model for test executions based on historic test execution results that causes no test execution runtime overhead. The paper then presents a novel cost based test selection strategy, THEO, which skips test executions where the expected cost of running the test exceeds the expected cost of not running it, while ensuring that all tests will execute on all code changes at least once. How was the work validated? The paper replayed past development periods of Microsoft Windows, Office, and Dynamics with THEO. THEO would have reduced the number of test executions by up to 50%, cutting down test time by up to 47%. At the same time, product quality was not sacrificed as the process ensures that all tests are ran at least once on all code changes. Simulation shows that THEO produced an overall cost reduction of up to $2 million per development year, per product. Furthermore, this paper have convinced an increasing number of Microsoft product teams to explore ways to integrate THEO into their actual live production test environments. This further endorses THEO's effectiveness. How could this research be extended? The paper stated that through reducing the overall test time, THEO would also have other impacts on the product development process, such as increasing code velocity and developer satisfaction. An empirical study on the effects of cost based test selection strategies on these aspects would be a direction for extending this research. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? How could this research be applied in practice? In my opinion, the main contribution of this paper, and the aspect most able to be used as a reference in other projects, is the cost model where each test execution is considered an investment and the expected test result considered as return of investment. Several factors are considered in the cost model, with their values easily derived from past observations. \\(p_{TP}\\), the probability the combination of test and execution context will detect a defect (true positive). \\(p_{FP}\\), the probability the combination of test and execution context will report a false alarm (false positive). \\(engineers\\), the number of engineers whose code changes passed the current code branch. \\(time_{delay}\\), the average time span required to fix historic defects on the corresponding code branch. When a test is executed: \\(cost_{machine}\\): the per-minute infrastructure cost of test execution. \\(cost_{inspect}\\): the average cost per test inspection, equal to inspection time times the salary of the engineer. For simplicity reasons, an average cost of test inspection is used. When a test is skipped: \\(cost_{escaped}\\): the average cost of an escaped defect, per developer and hour of delay. Defect severity is not modeled, as it cannot be determined beforehand, and all defects causing development activity to freeze on the corresponding branch must be considered severe. After collecting these data, two cost functions are calculated: the expected cost of executing a test \\(cost_{exec} = cost_{machine} + p_{FP} \\times cost_{inspect}\\), and the expected cost for not executing a test \\(cost_{skip} = p_{TP} \\times cost_{escaped} \\times time_{delay} \\times engineers\\). Through a reasonable and tested quantization like this, objective decisions can be made, boosting the efficiency of software development.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Boosting Fuzzer Eficiency: An Information Theoretic Perspective","slug":"Paper-Reading-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective","date":"2022-09-20T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/09/20/Paper-Reading-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective/","link":"","permalink":"https://abbaswu.github.io/2022/09/20/Paper-Reading-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? Finding a solid theoretical foundation for fuzzing and using it to boost fuzzing efficiency is a direction of research of great practical value. How was it addressed by prior work? Previous works have proposed various heuristics to boost fuzzing efficiency, such as assigning more energy to seeds that have previously been observed to crash, that maximize execution counts to discover algorithmic complexity vulnerabilities, that exercise low-probability paths, etc. Furthermore, there has also been prior research in theoretical aspects of fuzzing, such as conducting a probabilistic analysis on the efficiency of blackbox versus whitebox fuzzing, empirically investigating the scalability of non-deterministic black- and greybox fuzzing, etc. What are the innovation(s) proposed in this paper? First, the paper develops an information-theoretic foundation for non-deterministic fuzzing. Assumptions Fuzzing Heuristics remain constant throughout the fuzzing process. Concepts Neighborhood All inputs generated from mutating a seed. Species A branch within a program. Species Discovery Program execution traverses a previously untraversed branch when some input is provided to the program. Incidence Frequency The number of times a species is covered. Energy The probability the fuzzer chooses a seed for mutation. Power Schedule The procedure of assigning energy to a seed. Local Species Distribution of a Seed Given a seed, the probability of each species being covered, when an input generated by mutation from the seed is fed to the program. Entropy in the Context of Fuzzing Using the metaphor of a \"language\" with \"words\" of varying frequencies, entropy in the context of fuzzing can be understood as: \"Sentences\" of the \"language\": Program executions resulting from generated inputs. \"Words\" of the \"language\": Species. Frequencies of the \"words\": The frequencies of each species being traversed. Entropy can be calculated using the frequencies of the \"words\", and represents the frequency distribution of the \"words\". As high entropy implies that the species of the program have all been well covered, it can be used as a proxy for fuzzing efficiency. Local Entropy of a Seed Still using the metaphor of a \"language\" with \"words\" of varying frequencies, local entropy of a seed can be understood as: \"Sentences\" of the \"language\": Program executions resulting from inputs within the seed's neighborhood. \"Words\" of the \"language\": Species. Frequencies of the \"words\": The frequencies of each species being traversed. The local entropy of a seed quantifies the information that feeding the inputs within the seed's neighborhood into the program reveals about the species. Second, the paper presents the first entropy-based power schedule to boost the efficiency of greybox fuzzers. More energy is assigned to seeds that elicit more information about the program's species. Thus, every time when randomly choosing a seed for mutation, each seed is assigned an energy proportional to its local entropy. However, a new seed that has never been fuzzed will always be assigned zero energy, and they will never be chosen for mutation. To solve this problem, add-one smoothing is used for the frequency of the species. Specifically, the frequency of species \\(i\\) used to calculate local entropy of seed \\(t\\): \\(p_i^t = \\frac{Y_i^t + 1}{S + Y_1^t + \\dots + Y_S^t}\\) Where: \\(Y_i^t\\) is the number of times species \\(i\\) has been traversed by the neighborhood of \\(t\\). \\(S\\) is the total number of species at the time of calculation. Furthermore, in the experiments, the authors noticed that the local entropies for different seeds were almost the same, because a small number of very abundant species had a huge impact on the local entropies. Thus, the authors defined an abundance threshold \\(\\theta\\) which is an upper bound for \\(Y_i^t\\). How are those innovations evaluated? How does the paper's evaluation match with the proposed problem statement? The paper provides an open-source implementation, Entropic, within LLVM libFuzzer, and presents a substantial empirical evaluation on over 250 widely-used, open-source C/C++ programs producing over 2 CPU years worth of data. Four research questions were asked to evaluate the hypothesis that increasing information per generated input increases fuzzer efficiency. What is the empirical coverage improvement over the baseline? How much faster are bugs detected compared to the baseline? How does the choice of abundance threshold influence the performance of our technique? What is the cost of maintaining incidence frequencies? The answers to these research strongly support the hypothesis, thus the evaluation matches well with the proposed problem statement. Your opinion of the paper Which technical innovations are most compelling to you? Developing an information-theoric foundation for non-deterministic fuzzing, in which entropy in the context of fuzzing is calculated using the probability distribution of species (branches). This is both intuitive and allows us to effectively use entropy, which has \"really nice properties, and a principled origin\" as a \"convenient proxy\" for fuzzing efficiency. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? Which problems remain unsolved after this paper? The paper develops an information-theoretic foundation for non-deterministic fuzzing, before presenting the first entropy-based power schedule to boost the efficiency of greybox fuzzers. I have questions regarding both aspects. Entropy is calculated using the probability distribution of species, which are branches. Is is possible to utilize a different definition of \"species\"? The entropy-based power schedule assigns each seed with energy proportional to its local entropy. However, the authors noticed that the local entropies for different seeds were almost the same, because a small number of very abundant species had a huge impact on the local entropies. Thus, the authors defined an abundance threshold \\(\\theta\\) for \\(Y_i^t\\), a task-relevant hyperparameter. Is there a better approach for calculating the local entropies? Do you forsee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome? As stated above, regarding the entropy-based power schedule.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Reading: How Effective Developers Investigate Source Code: An Exploratory Study","slug":"Paper-Reading-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study","date":"2022-09-19T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/09/19/Paper-Reading-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study/","link":"","permalink":"https://abbaswu.github.io/2022/09/19/Paper-Reading-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? The paper provides a set of detailed observations about the characteristics of effective program investigation. These observations are accompanies by hypotheses that can be validated by additional research and practical experience. The paper's results support the intuitive notion that developers should follow a general plan, perform focused searches in the context of this plan, and keep some form of record of their findings when investigating a program. The paper describes a methodology and analysis technique for studying the behavior of software developers. How was the work validated? The authors conducted a study of five developers undertaking an identical software change task on a medium-sized system, where understanding the existing software is a precursor to modification and validation. They did a detailed qualitative analysis of a few replicated cases, rather than a statistical analysis of causality between dependent variables. Many previous studies were based on heavily abstracted characterizations of both developer behavior and success level. It involved a detailed study of the examined code, the methods used to navigate between different locations in the code, and the modified source code. They contrasted the program investigation behavior of successful and unsuccessful developers, and isolated the factors associated with the behavior of a developer, rather than external factors (such as the influence of the workplace, the programming environment, etc.) How could this research be applied in practice? Ensuring that developers in charge of modifying software systems investigate the code of the system effectively can yield important benefits such as decreasing the cost of performing software changes and increasing the quality of the change. Understanding the nature of program investigation behavior that is associated with successful software modification tasks can help us improve the tool support and training programs offered to software developers. How could this research be extended? Researchers can reuse the authors' strategy (stated in \"How was the work validated?\") to help validate the hypotheses the authors' proposed, or to study other aspects of programmer behavior. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? In my opinion, the main contributions of the paper include the primary contributions of the paper as the author sees it, how the work was validated, and how this research could be applied in practice. However, what is most meaningful for me is how the work was validated. Such methodology is of great reference value for conducting studies on other aspects of programmer behavior. There are many technical details within that have left a deep impression on me. Each phase was described entirely through written instructions, and the subjects were given an Eclipse training phase and an investigation phase before the modification phrase. To record the actions of a developer in the investigation and modification phases, they recorded the developers' screens, and transcribed the recordings into a structured list of events. Each event contains the properties time, method, navigation, and modification. To analyze the quality of change, the authors analyzed the source code to determine the characteristics of an ideal solution, and divided the task into five subtasks. The authors examined how each subject had implemented each subtask, and characterized its quality.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Understanding the Formulation of Information Entropy","slug":"Understanding-the-Formulation-of-Information-Entropy","date":"2022-09-16T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/09/16/Understanding-the-Formulation-of-Information-Entropy/","link":"","permalink":"https://abbaswu.github.io/2022/09/16/Understanding-the-Formulation-of-Information-Entropy/","excerpt":"","text":"NOTE: The terms \"language\" and \"word\" are used metaphorically in this document. A \"language\" often has many \"words\", and the frequency of each \"word\" varies. If a \"language\" \\(X\\) has a total of \\(n\\) \"words\", then we can encode a word with \\(\\log_{2}{n}\\) binary bits. But when transmitting the words, we want to keep the encoding of each \"word\" as short as possible. A common practice is that for those high-frequency \"words\", we can use shorter encodings, and for those \"words\" that we use less frequently, we can allow longer encodings. An example is the Morse code encoding for a \"language\" consisting of 36 \"words\" - 26 Latin letters and 10 Arabic numerals. Morse code. The \"word\" \"e\" occurs frequently, hence a short code So, under some optimal encoding, what limit can the weighted average encoding length of all \"words\" achieve? Suppose our \"language\" has \\(n\\) \"words\", \\(x_1, x_2, \\dots, x_n\\), and their probability of occurrence is \\(p(x_1), p(x_2), \\dots, p(x_n)\\) (known quantities). Assuming that the lengths of the encodings of these \"words\" are \\(L(x_1), L(x_2), \\dots, L(x_n)\\) respectively, the weighted average encoding length of each \"word\" is: \\(\\bar{L} = p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)\\) How do we find the minimum value of \\(\\bar{L}\\)? Constraints Obviously, the encoded length of all \"words\" is greater than 0. But beyond that, there is a hidden constraint. We do not allow one encoding to be a prefix of another encoding, otherwise there will be ambiguity during decoding. For example, assuming that the three \"words\" of \"A\", \"B\", and \"C\" in the alphabet are encoded as \"0\", \"1\", and \"01\" respectively, then for For a code like \"001\", should we decode it as \"AAB\" or \"AC\"? We call a type of code which requires that there is no whole code word in the system that is a prefix of any other code word in the system as a prefix code. This means that if we assign a shorter encoding to a \"word\", it will squeeze a lot of resources out of the encoding space. For example, suppose the \"word\" \"A\" is encoded as \"0\", then it would \"squeeze out\" \"00\", \"01\", etc. from the codewords. Suppose the maximum value in \\(L(x_1), L(x_2), \\dots, L(x_n)\\) is \\(L_{max}\\). Then the encoding of all \"words\" are nodes on a full binary tree with a height of \\(L_{max}\\), and the full binary subtrees below each node have no intersection (otherwise violating the properties of the prefix code), as shown below. The encoding of all \"words\" are nodes on a full binary tree with a height of \\(L_{max}\\), and the full binary subtrees below each node have no intersection It is obvious that, all the full binary subtrees below each node, at most cover all the leaves of the full binary tree with height \\(L_{max}\\). For a \"word\" \\(x_i, i \\in \\{1, 2, \\dots, n\\}\\), the height of the full binary subtree below it is \\(L_{max} - L(x_i)\\), and it covers \\(2^{L_{max} - L(x_i)}\\) leaves. As the full binary tree with height \\(L_{max}\\) has a total of \\(2^{L_{max}}\\), we have: \\(2^{L_{max} - L(x_1)} + 2^{L_{max} - L(x_2)} + \\dots + 2^{L_{max} - L(x_n)} \\le 2^{L_{max}}\\) This simplifies to: \\(2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1\\) This is the Kraft-McMillan inequality. Optimization Therefore, our overall optimization objective is: \\(\\bar{L} = p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)\\) Subject to: \\(p(x_i) \\in (0, 1), i \\in \\{1, 2, \\dots, n\\}\\) are constants \\(p(x_1) + p(x_2) + \\dots + p(x_n) = 1\\) \\(L(x_i) &gt; 0, i \\in \\{1, 2, \\dots, n\\}\\) are independent variables \\(2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1\\) We can analyze the problem for the case where there are only two words \\(x_1, x_2\\). At this point, we have: \\(\\bar{L} = p(x_1) L(x_1) + p(x_2) L(x_2)\\) Equivalently: \\(\\bar{L} = p(x_1) L(x_1) + (1 - p(x_1)) L(x_2)\\) Subject to: \\(p(x_1) \\in (0, 1)\\) is a constant \\(L(x_i) &gt; 0, i \\in \\{1, 2\\}\\) are independent variables \\(2^{- L(x_1)} + 2^{- L(x_2)} \\le 1\\) Define \\(a_1 = 2^{- L(x_1)}, a_2 = 2^{- L(x_2)}\\). Now we have: \\(\\bar{L} = - p(x_1) \\log_2{a_1} - (1 - p(x_1)) \\log_2{a_2}\\) Subject to: \\(p(x_1) \\in (0, 1)\\) is a constant \\(0 &lt; a_i &lt;1, i \\in \\{1, 2\\}\\) are independent variables \\(a_1 + a_2 \\le 1\\) At this point, \\(\\bar{L}\\) can be regarded as a binary function whose independent variables are \\(a_1, a_2\\), and the value ranges of the independent variables \\(a_1, a_2\\) are as follows: Value ranges of \\(a_1, a_2\\) We want to find the minimum value of \\(\\bar{L}(a_1, a_2)\\) within this range of values. The gradient of \\(\\bar{L}(a_1, a_2)\\) is as follows: \\(\\nabla\\bar{L}(a_1, a_2) = {(-p(x_1) \\log{2} \\frac{1}{a_1}, -(1 - p(x_1)) \\log{2} \\frac{1}{a_2})}^T\\) Within the value range of the independent variables \\(a_1, a_2\\), \\(\\nabla\\bar{L}(a_1, a_2)\\) is always less than 0, which means that with the growth of \\(a_1, a_2\\), \\(\\bar{L}(a_1, a_2)\\) decreases. Therefore, the maximum value of \\(\\bar{L}(a_1, a_2)\\) must occur when \\((a_1, a_2)\\) is on the boundary line \\(a_1 + a_2 = 1\\). Substituting the boundary line \\(a_1 + a_2 = 1\\) into \\(\\bar{L}(a_1, a_2)\\), you can get a unary function: \\(\\bar{L}(a_1) = - p(x_1) \\log_2{a_1} - (1 - p(x_1)) \\log_2{(1 - a_1)}\\) The constraints include: \\(p(x_1) \\in (0, 1)\\), constant \\(0 &lt; a_1 &lt; 1\\) The derivative of \\(\\bar{L}(a_1)\\) is as follows: \\(\\frac{d \\bar{L}(a_1)}{d a_1} = \\frac{\\log{2} (a_1 - p(x_1))}{a_1 (1 - a_1)}\\) The constraints include: \\(p(x_1) \\in (0, 1)\\) is a constant \\(0 &lt; a_1 &lt; 1\\) When \\(0 &lt; a_1 &lt; p(x_1)\\), \\(\\frac{d \\bar{L}(a_1)}{d a_1} &lt; 0\\), \\(\\bar{L}(a_1)\\) monotonically decreases, and when \\(p(x_1) &lt; a_1 &lt; 1\\), \\(\\frac{d \\bar{L}(a_1)}{d a_1} &gt; 0\\), \\(\\bar{L}(a_1)\\) monotonically increases. Therefore, when \\(a_1 = p(x_1)\\), \\(\\bar{L}(a_1)\\) obtains the minimum value. As \\(a_1 = 2^{- L(x_1)}, a_2 = 2^{- L(x_2)}\\), this means that, for: \\(\\bar{L} = p(x_1) L(x_1) + (1 - p(x_1)) L(x_2)\\) Subject to: \\(p(x_1) \\in (0, 1)\\) is a constant \\(L(x_i) &gt; 0, i \\in \\{1, 2\\}\\) are independent variables \\(2^{- L(x_1)} + 2^{- L(x_2)} \\le 1\\) \\(\\bar{L}\\)'s minima occurs when \\(L(x_1) = -\\log_2{p(x_1)}, L(x_2) = -\\log_2{p(x_2)}\\), and the minima is \\(- p(x_1) \\log_2{p(x_1)} - p(x_2) \\log_2{p(x_2)}\\). Going back to the multivariate optimization problem: \\(\\bar{L} = p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)\\) Subject to: \\(p(x_i) \\in (0, 1), i \\in \\{1, 2, \\dots, n\\}\\) are constants \\(p(x_1) + p(x_2) + \\dots + p(x_n) = 1\\) \\(L(x_i) &gt; 0, i \\in \\{1, 2, \\dots, n\\}\\) are independent variables \\(2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1\\) \\(\\bar{L}\\)'s minima occurs when \\(L(x_i) = -\\log_2{p(x_i)}, i \\in \\{1, 2, \\dots, n\\}\\), and the minima is \\(- p(x_1) \\log_2{p(x_1)} - \\dots - p(x_n) \\log_2{p(x_n)}\\). Definition of Information Entropy If a language \"language\" \\(X\\) has \\(n\\) \"words\", \\(x_1, x_2, \\dots, x_n\\), the probability of their occurrence is \\(p(x_1), p(x_2), \\dots, p(x_n)\\), then all \"words\" under a certain optimal encoding, the previously calculated minimum weighted average encoding length, \\(- p(x_1) \\log_2{p(x_1)} - \\dots - p (x_n) \\log_2{p(x_n)}\\), is called the information entropy of the \"language\", denoted as \\(H(X)\\). The reason why it is called \"information entropy\" is mainly due to the following reasons: From von Neumann's naming suggestion for Shannon: My greatest concern was what to call it. I thought of calling it 'information,' but the word was overly used, so I decided to call it 'uncertainty.' When I discussed it with John von Neumann, he had a better idea. Von Neumann told me, 'You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage. In a sense, it does reflect the frequency distribution of the \"words\" of \"language\" \\(X\\), just as entropy in thermodynamics reflects the distribution of microscopic particles. The lower \\(H(X)\\) is, the more the case that only a few words are used frequently in \\(X\\); the higher \\(H(X)\\) is, the more the case that all words in \\(X\\) are used frequency. Links to Explanations of Related Concepts Cross Entropy Joint Entropy Mutual Information How These Concept are Applied in Practice https://colah.github.io/posts/2015-09-Visual-Information/#conclusion References https://colah.github.io/posts/2015-09-Visual-Information/ https://mbernste.github.io/posts/sourcecoding/ https://en.wikipedia.org/wiki/KraftMcMillan_inequality https://mathoverflow.net/questions/403036/john-von-neumanns-remark-on-entropy","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://abbaswu.github.io/categories/Mathematics/"}],"tags":[]},{"title":"Paper Reading: Asking and Answering Questions during a Programming Change Task","slug":"Paper-Reading-Asking-and-Answering-Questions-during-a-Programming-Change-Task","date":"2022-09-14T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/09/14/Paper-Reading-Asking-and-Answering-Questions-during-a-Programming-Change-Task/","link":"","permalink":"https://abbaswu.github.io/2022/09/14/Paper-Reading-Asking-and-Answering-Questions-during-a-Programming-Change-Task/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? A catalog of 44 types of questions programmers ask during software evaluation tasks, organized into four categories based on the kind and scope of information needed to answer a question. Finding a focus point Expanding a focus point Understanding a subgraph Over groups of subgraphs A description of the observed behavior around answering these questions. A description of how existing deployed and proposed tools do, and do not, support answering programmers' questions. How was the work validated? The author interviewed participants in two studies. 9 participants in academia worked on a code base that was new to them. 16 participants in industry worked on a code base for which they had responsibility. The two studies have allowed us to observe programmers in situations that vary along several dimensions: - the programming tools - the type of change task - the system - paired versus individual programming - prior knowledge of the code base The differences have increased the authors' ability to generate an extensive set of questions programmers ask. They build rather than test theory and the specific result of this process is a theoretical understanding of the situation of interest grounded in the data collected. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? In my opinion, aside from the final results, three important considerations learned from this paper are: Interviewing participants in two very different groups. Developing generic versions of the questions participants asked, which slightly abstract from the specifics of a particular situation and code base. Compared the generic questions and categorized those questions into four categories based on the kind and scope of information needed to answer a question. This is an example of extracting generalized knowledge from specific case studies, which makes it a great example to study for conducting empirical studies. How could this research be extended? How could this research be applied in practice? The research identified clear gaps of tool support in answering programmers' questions. Support for more refined or precise questions. Some questions can he seen as more refined versions of other questions. A programmer's questions also often have an explicit or implicit scope. Due to limited tool support, programmers end up asking questions more globally than they intend, and, the result sets will include many irrelevant items. Support for maintaining context. A particular question is often part of a larger process involving multiple questions. There are missed opportunities for tools to make use of the larger context to help programmers more efficiently scope their questions and to determine what is relevant to their higher level questions. Support for piecing information together. Many questions require considering multiple entities and relationships. In these situations, the burden is on the programmer to assemble the information needed to answer their intended question. Tool support is missing for bringing information together and building toward an answer. Improved tools and an assessment of these tools in answering these questions present directions for future research.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Qualitative Methods in Empirical Studies of Software Engineering","slug":"Paper-Reading-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering","date":"2022-09-14T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/09/14/Paper-Reading-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering/","link":"","permalink":"https://abbaswu.github.io/2022/09/14/Paper-Reading-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? With empirical studies of software engineering beginning to address the human aspects of software development, the author presents and reviews a number of different methods for the collection and analysis of qualitative data, and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combines with quantitative methods. Collecting Qualitative Data Participant Observation Interviewing Extracting Quantitative Values from Qualitative Data for Quantitative Analysis (Coding) Analyzing Qualitative Data Theory Generation: extract from a set of field notes a statement or preposition that is supported in multiple ways by the data. Theory Confirmation: confirming a preposition after it has been generated from the data. What were the main contributions of the paper as you (the reader) see it? How could this research be applied in practice? Aside from the primary contributions of the paper as the author sees it, in my opinion, another major contribution of the paper is identifying the four main categories of empirical studies, and explaining in detail how combinations of quantitative and qualitative methods can be designed for each category. The four main categories of empirical studies: - Blocked subject-project study: - Several projects, several subjects. - Reduces bias, but increases the cost of the experiment. - Replicated project study: - One project, several subjects. - Isolates the effect of differences between subjects. - Multiproject variation: - Several projects, one subject. - Observes the performance of the subject on a project before some treatment is applied, and on a different project after that treatment is applied. - Single project study: - One project, one subject. - Similar to a case study. - Certain attributes are examined and possibly compared to some baseline. How combinations of quantitative and qualitative methods can be designed for each category: - Blocked subject-project study, Replicated project study: - When testing hypotheses and finding casual relationships between variables, use qualitative data to illuminate the statistical results. - Multiproject variation study: - Qualitative analysis: revealing new issues and tracking changes relative to other issues. - Quantitative analysis: looking more closely at the issues suggested by the qualitative analysis. - Single project study: - First, data is collected qualitatively through interviews. - A taxonomy of the question under research is generated. - Part of the interview data is coded to yield quantitative variables. - Any relationships found between quantitative variables are checked against qualitative data. How was the work validated? Examples, interviews, quotes from experts, and paper citations are used to validate the points presented when reviewing a number of different methods for the collection and analysis of qualitative data, identifying the four main categories of empirical studies, and explaining in detail how combinations of quantitative and qualitative methods can be designed for each category. How could this research be extended? In the last paragraph, the author points out that \"we must exploit to the fullest every opportunity we do have, by collecting and analyzing as much data of as many different types as possible\". Aside from the examples presented in the paper, what other types of data can be collected, and how they can be analyzed, is a future direction of research.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: No Silver Bullet Essence and Accidents of Software Engineering","slug":"Paper-Reading-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering","date":"2022-09-12T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/09/12/Paper-Reading-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering/","link":"","permalink":"https://abbaswu.github.io/2022/09/12/Paper-Reading-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. How does this work move the research forward? What were the primary contributions of the paper as the author sees it? The author concludes that there is no elixir or \"silver bullet\" to the problems software engineering is facing. Furthermore, the author also examines encouraging innovations, and shows that a disciplined, consistent effort to develop, propagate, and exploit them should alleviate the problem. What were the main contributions of the paper as you (the reader) see it? In my opinion, what the paper is most remarkable at is shedding light upon the nature of the software problem and its implications. The essence of a software entity is a construct of interlocking concepts that cannot be accurately visualized. The complexity of software is an essential property, and it increases non-linearly with size. This has many implications. Difficulty of design. Hindrance of communication among team members, which leads to product flaws, cost overruns, schedule delays. Hard to use programs. Difficulty of extending to new functions without creating side effects. Security trapdoors. Personnel turnover incurs tremendous learning and understanding burden. Software is constantly subject to pressure for change. The aforementioned points clarified by the paper illuminates research directions in software engineering aimed at ameliorating the software problem. How could this research be extended? In the last section of the paper, the author examines promising attacks on the essence of the software problem. Buying off-the-shelf software instead of building in-house software. Rapid prototyping and iterative specification of requirements with client feedback. Incremental development of software from a simple and incomplete, yet running, system. Growing great designers who are the core of the development team. The effectiveness of these and other approaches in mitigating the software problem could be assessed in subsequent works. How was the work validated? First, the author examines the nature of the software problem and its implications. Further on, the author recalls the three steps in software technology that have been most fruitful in the past - high-level languages, time-sharing, and unified programming environments, concluding that they have their limits and the difficulties that they attacked are accidental, not essential. The author continues to consider the technical developments that are most often advanced as potential silver bullets - high-level language advances, object-oriented programming, artificial intelligence, automatic programming, graphical programming, program verification, environments and tools, workstations - analyzing the problems they assess, their advantages, and their disadvantages. Finally, the author presents promising attacks on the conceptual essence, explaining why they would be useful. How could this research be applied in practice? The lessons learned from this research are of great practical value. In shedding light upon the nature of the software problem and its implications, the author provides criteria for organizations to assess the effectiveness of their development practices. In considering the technical developments that are most often advanced as potential silver bullets, the author examines their advantages, and their disadvantages, and provide insights into whether to, and how to adequately use them. In presenting promising attacks on the conceptual essence, the author provides meaningful suggestions for organizations to improve their software development processes, and provides convincing rationale for doing so. As this is a classic paper, many promising attacks on the conceptual essence have already materialized and become mainstream. Rapid prototyping and incremental development have been manifested as \"agile development\" and have been widely adopted. With the advent of the open-source revolution and code-hosting platforms such as GitHub, reusing off-the-shelf software instead of building in-house software has become ubiquitous. However, the call for organizations to \"grow great designers who are the core of the development team\" incurs significant requirements on corporate management competency, and sadly, hasn't fully become reality. How does the work apply to you? It sheds light upon the nature of the software problem and its implications, illuminates research directions in software engineering aimed at ameliorating the software problem, and provides a reference research methodology for problems within software engineering.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: Software's Chronic Crisis","slug":"Paper-Reading-Software-s-Chronic-Crisis","date":"2022-09-12T07:00:00.000Z","updated":"2023-11-06T07:31:36.169Z","comments":true,"path":"2022/09/12/Paper-Reading-Software-s-Chronic-Crisis/","link":"","permalink":"https://abbaswu.github.io/2022/09/12/Paper-Reading-Software-s-Chronic-Crisis/","excerpt":"","text":"NOTE: This is a Paper Reading for Advanced Software Engineering. The original paper can be found here. How does this work move the research forward? What were the primary contributions of the paper as the author sees it? The author identifies software's chronic crisis and how it is exacerbated by current trends in software engineering. The vast majority of code is handcrafted by artisans using techniques they neither measure nor are able to repeat consistently. The software industry remains short of the mature engineering discipline needed to meet the demands of an information-age society, including getting software right the first time in embedded environments, distributed systems and systems integration, rapid increasing system sizes, and systems becoming so complex that no manager can comprehend the entirety. Later, the author analyzes proposed remedies to the aforementioned problems and points out directions for future work. Remedies: Capability Maturity Model, which quantifies a developer's software engineering and management excellence. Consistent and quantitative measurement of development. Strategies to avoid bugs or attack them early. Recognizing changing requirements Growing software from rapid prototypes and customer feedback Formal verification when necessary Clean-room process Cautious approach to technological innovations such as object-oriented analysis and programming Directions for Future Work: An experimental branch of computer science to separate the general results from the accidental Standard unit of measurement of developer productivity Codified proven solutions for novices Academic-industrial collaboration to gather data and try things Generalized, reusable software components Certifying software engineers Outsourcing More software development-oriented computer science curricula What were the main contributions of the paper as you (the reader) see it? How could this research be applied in practice? Aside for the primary contributions of the paper as the author sees it, in my opinion, a major contribution of the paper in a practical sense are revelations for improving the culture within software developing organizations. For example, Focus on interchangeability. Follow best practices. Fix not just the bug but also the flaw in the testing process that allowed it to slip through. Value verification in addition to innovation. Pay attention to the difference in competence between employees. Furthermore, as a historical paper, many of its proposals have already materialized. For example, the open-source revolution and collaboration platforms such as GitHub have greatly facilitated gathering data and trying things for research, and has provided a wealth of generalized, reusable software components. How could this research be extended? Implementing and assessing the proposed directions for future work represents a natural extension of this research. How was the work validated? The authors validate their arguments on software's chronic crisis and base their proposals for remedies and future work by analyzing real cases in software engineering, as well as compiling the opinions of experts in the field, including university professors and corporate managers. How does the work apply to you? From a theoretical perspective, as a milestone paper in the domain of software engineering, this paper provides a model research methodology for practical problems within software engineering - analyzing real cases and compiling the opinions of experts. From a practical perspective, this paper identifies core values and skills that us, as practitioners of software engineering, should firmly grasp.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Reading: An empirical study of the reliability of UNIX utilities","slug":"Paper-Reading-An-empirical-study-of-the-reliability-of-UNIX-utilities","date":"2022-09-10T07:00:00.000Z","updated":"2023-11-06T07:31:36.165Z","comments":true,"path":"2022/09/10/Paper-Reading-An-empirical-study-of-the-reliability-of-UNIX-utilities/","link":"","permalink":"https://abbaswu.github.io/2022/09/10/Paper-Reading-An-empirical-study-of-the-reliability-of-UNIX-utilities/","excerpt":"","text":"NOTE: This is a Paper Reading for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. \"An empirical study of the reliability of UNIX utilities\" is the work that spawned research into the domain of software fuzzing. It proposes a technique later known as random fuzzing, testing the reliability of UNIX utilities by feeding them a stream of randomly generated characters and checking whether the program crashed with a core dump or hangs. Although the technique is simple and is not a substitute for formal verification or testing, it is inexpensive and easy to apply. Its effectiveness in identifying bugs and increasing overall system reliability has been proven in many ways. It crashed 25-33% of the utility programs considered to be \"reliable\" on each platform. It was able to find recurring security bugs resulting from bad programming practices that even the best static analysis tools have limited success in detecting, including: Accessing outside the bounds of a buffer Dereferencing a null pointer Unintentionally overwriting data or code Ignoring return codes, especially error-indicating return codes Faulty communication with subprocesses Unintended interaction between modules Improper error handling Signed characters Race conditions during signal handling Its relevance has remained strong over the years. Subsequent studies using the same technique showed that similar problems also existed within other operating systems, such as Microsoft Windows. Even after thirty years, the utility programs in the modern Unix distributions of Linux, macOS, and FreeBSD are still crashing at a noticeable rate and not getting better, as evidenced in \"The Relevance of Classic Fuzz Testing: Have We Solved This One?\" The contributions of this work is multi-fold. As mentioned before, it proposed random fuzzing, an inexpensive, easy to apply, and time-proven way of finding security bugs which is complimentary with formal verification and testing. It spawned research into the domain of software fuzzing. New fuzz tools usually take a gray- or white-box approach, diving deeper into a program's control flow, and they have been applied to many new contexts. However, they often require more advanced specification of the input and/or long execution times to explore the input and program control-flow space. It provides revelations for software engineering: good design, good education, ongoing training, testing integrated into the development cycle, and most importantly, a culture that promotes and rewards reliability. Some personal thoughts after reading the paper. Given the source code of a program and an input, what is the mechanism through which the researchers determine the position where the program crashes and hangs when given the input? This is mentioned in neither \"An empirical study of the reliability of UNIX utilities\" nor its sequel \"The Relevance of Classic Fuzz Testing: Have We Solved This One?\", but is of great practical value. There is a surprising number of security bugs stemming from language defects such as not checking array bounds and dereferencing null pointers, as well as ad-hoc, hacky solutions to recurring problems such as lexical analysis, syntax analysis, structured error handling, as well as graph algorithms including cycle detection, topological sort, etc. Personally, this is not my style of coding. I make extensive a lot of \"safe\" language constructs such as null coalescing, heavily exploit performant and well-tested algorithms within standard libraries and widely-adapted third-party libraries (such as boost in C++ and networkx in Python), and use theoretically sound tools (such as automatically generated LALR parsers for syntax analysis) in software projects. The efficiency, effectiveness, and practical value of these and other solutions, as well as how they can be improved, is an interesting question that comes to my mind after reading this paper.","categories":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]}],"categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://abbaswu.github.io/categories/Mathematics/"},{"name":"Paper Reading","slug":"Paper-Reading","permalink":"https://abbaswu.github.io/categories/Paper-Reading/"},{"name":"Research Programming","slug":"Paper-Reading/Research-Programming","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Research-Programming/"},{"name":"Talks","slug":"Talks","permalink":"https://abbaswu.github.io/categories/Talks/"},{"name":"Meeting Minutes","slug":"Meeting-Minutes","permalink":"https://abbaswu.github.io/categories/Meeting-Minutes/"},{"name":"Conferences","slug":"Conferences","permalink":"https://abbaswu.github.io/categories/Conferences/"},{"name":"Reflections","slug":"Reflections","permalink":"https://abbaswu.github.io/categories/Reflections/"},{"name":"Reference","slug":"Reference","permalink":"https://abbaswu.github.io/categories/Reference/"},{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"},{"name":"Unix","slug":"Code/Unix","permalink":"https://abbaswu.github.io/categories/Code/Unix/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Reading/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Type-Systems/"},{"name":"C++","slug":"Code/C","permalink":"https://abbaswu.github.io/categories/Code/C/"},{"name":"PySide6","slug":"Code/Python/PySide6","permalink":"https://abbaswu.github.io/categories/Code/Python/PySide6/"},{"name":"Planning","slug":"Planning","permalink":"https://abbaswu.github.io/categories/Planning/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"},{"name":"Advanced Software Engineering","slug":"Paper-Reading/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Reading/Advanced-Software-Engineering/"}],"tags":[]}