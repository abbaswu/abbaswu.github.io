{"meta":{"title":"Jifeng Wu's Personal Website","subtitle":"Jifeng Wu's Personal Website","description":"Jifeng Wu's Personal Website","author":"Jifeng Wu","url":"https://abbaswu.github.io","root":"/"},"pages":[{"title":"","date":"2022-10-20T19:50:30.506Z","updated":"2022-10-20T19:50:30.506Z","comments":false,"path":"categories/index.html","permalink":"https://abbaswu.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2022-10-20T19:50:14.156Z","updated":"2022-10-20T19:50:14.156Z","comments":false,"path":"projects/index.html","permalink":"https://abbaswu.github.io/projects/index.html","excerpt":"","text":""},{"title":"","date":"2022-10-20T19:50:53.140Z","updated":"2022-10-20T19:50:53.140Z","comments":false,"path":"tags/index.html","permalink":"https://abbaswu.github.io/tags/index.html","excerpt":"","text":""},{"title":"About Me","date":"2022-10-20T04:00:00.000Z","updated":"2023-05-25T13:12:42.717Z","comments":false,"path":"index.html","permalink":"https://abbaswu.github.io/index.html","excerpt":"","text":"Hello everyone! My name is Jifeng Wu and you can call me Abbas. I am a Computer Science Masters’ student at UBC working with Alexander J. Summers and Caroline Lemieux in the Software Practices Lab. I am interested in Programming Languages and Software Engineering. Before that, I obtained my Bachelor of Engineering from School of Computer Science, Wuhan University. My bachelor’s thesis, “Effective Stack Wear Leveling for NVM” has been advised by Qingan Li, and I have also done research in Graph Data Mining, advised by Yuanyuan Zhu. I enjoy computer programming (my favorite programming languages are C++, Python, Shell, and I am an avid Unix fan), photography, hiking, cycling, cooking, and language learning in my free time. If you are interested, you can find my blog here, my CV here, and feel free to contact me through my social media links!"}],"posts":[{"title":"Syncing a Local Directory With a Remote Directory via rsync","slug":"Syncing-a-Local-Directory-With-a-Remote-Directory-via-rsync","date":"2023-07-11T07:00:00.000Z","updated":"2023-07-11T05:49:44.133Z","comments":true,"path":"2023/07/11/Syncing-a-Local-Directory-With-a-Remote-Directory-via-rsync/","link":"","permalink":"https://abbaswu.github.io/2023/07/11/Syncing-a-Local-Directory-With-a-Remote-Directory-via-rsync/","excerpt":"","text":"Syncing a Local Directory With a Remote Directory via rsyncrsync is a powerful utility for efficiently synchronizing directories between a local computer and a remote server. It achieves this synchronization by comparing file modification times and sizes, utilizing delta encoding, and optionally employing data compression to minimize network usage. To successfully synchronize directories between two systems using rsync, you need to have rsync installed on both the local and remote machines. Additionally, the remote machine should be accessible from the local machine via SSH, enabling the local machine to invoke the remote machine’s rsync and determine which parts of local files need to be transferred. The rsync command-line syntax is similar to that of cp and scp. The following command-line options are commonly used: -e: Specifies the command to establish an SSH connection before the username@domain section. This option allows you to customize SSH behavior, such as using specific ports or providing a PEM file (e.g., ssh -i SSH-key.pem). -r (recursive): Used for syncing directories, similar to cp and scp. -v (verbose): Lists files being transferred during synchronization. -z: Enables additional data compression for improved network usage. When specifying the source directory and the destination directory, keep the following points in mind: A local directory can be represented using either a relative or absolute path. A remote directory is represented using the username@domain:&lt;absolute path on the remote machine&gt; notation, similar to scp. A source directory must end with a trailing slash. A destination directory must not end with a trailing slash. Here’s an example that illustrates how to sync the local directory TypeWriter_dataset to the remote directory /home/ubuntu/TypeWriter_dataset with additional data compression. This assumes connecting to ubuntu@104.171.203.254 via the command ssh -i SSH-key.pem: 12345rsync \\-e &#x27;ssh -i SSH-key.pem&#x27; \\-r -v -z \\TypeWriter_dataset/ \\ubuntu@104.171.203.254:/home/ubuntu/TypeWriter_dataset Conversely, to sync the remote directory /home/ubuntu/TypeWriter_dataset to the local directory TypeWriter_dataset: 12345rsync \\-e &#x27;ssh -i SSH-key.pem&#x27; \\-r -v -z \\ubuntu@104.171.203.254:/home/ubuntu/TypeWriter_dataset/ \\TypeWriter_dataset References: https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories https://en.wikipedia.org/wiki/Rsync","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"}],"tags":[]},{"title":"Task Parallelism and Data Parallelism Thread Pools","slug":"Task-Parallelism-and-Data-Parallelism-Thread-Pools","date":"2023-07-11T07:00:00.000Z","updated":"2023-07-11T16:29:59.348Z","comments":true,"path":"2023/07/11/Task-Parallelism-and-Data-Parallelism-Thread-Pools/","link":"","permalink":"https://abbaswu.github.io/2023/07/11/Task-Parallelism-and-Data-Parallelism-Thread-Pools/","excerpt":"","text":"Task Parallelism and Data Parallelism Thread PoolsParallel computing environments often involve distributing code across multiple processors for efficient execution. Two common paradigms of parallelization are task parallelism and data parallelism. Task parallelism focuses on distributing encapsulated tasks that can execute the same or different code on the same or different data across different processors. On the other hand, data parallelism involves performing the same operations on different subsets of the same data on multiple processors. Both task parallelism and data parallelism can be implemented using thread pools. This article explores simple implementations of thread pools for task parallelism and data parallelism scenarios. Task Parallelism Thread PoolThe following code demonstrates a simple thread pool for task parallelism. It utilizes a task queue to distribute tasks, which are represented as Callable[[], None] objects (callables accepting no parameters and returning None) across multiple threads. Each thread continuously fetches a task from the task queue and executes it. If an exception occurs during task execution, a traceback is printed, and a new task is obtained from the task queue. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import tracebackfrom collections.abc import Callable, Iterable, Generatorfrom queue import Queuefrom threading import Threadfrom typing import Anyclass TaskParallelismThreadPoolThread(Thread): def __init__(self, task_queue: Queue[Callable[[], None] | None]): super().__init__() self.task_queue: Queue[Callable[[], None] | None] = task_queue def run(self) -&gt; None: while True: task: Callable[[], None] | None = self.task_queue.get() if task is None: break try: task() except: traceback.print_exc()def run_simple_task_parallelism_thread_pool(tasks: Iterable[Callable[[], None]], num_threads: int) -&gt; None: # Create task queue. task_queue: Queue[Callable[[], None] | None] = Queue() # Create all threads which share a task queue. threads: list[Thread] = [] for _ in range(num_threads): thread: Thread = TaskParallelismThreadPoolThread(task_queue) thread.start() threads.append(thread) # Enqueue all tasks. for task in tasks: task_queue.put(task) # Enqueue sentinel values for all threads to stop once all tasks are finished. for _ in range(num_threads): task_queue.put(None) # Wait for all threads to stop. for thread in threads: thread.join() To use this task parallelism thread pool, provide it with a collection of tasks (represented as Callable[[], None] objects) and the desired number of threads. The tasks will be executed in parallel by the thread pool until all tasks have finished. Data Parallelism Thread PoolThe following code showcases a simple thread pool for data parallelism. It distributes data as argument tuples across multiple threads using an argument tuple queue. Each thread is assigned an operation created using an operation factory. After executing an operation on an argument tuple, the resulting return value is passed to a return value callback. As with the task parallelism thread pool, all threads in the thread pool are always busy by getting a new argument tuple whenever its operation finishes execution on a previous argument tuple. Should an exception be raised when executing an operation on an argument tuple, a traceback is printed, and a new argument tuple is taken from the shared argument tuple queue. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import tracebackimport multiprocessingfrom collections.abc import Callable, Iterable, Generatorfrom queue import Queuefrom threading import Threadfrom typing import Anyclass DataParallelismThreadPoolThread(Thread): def __init__(self, operation: Callable[[...], Any], argument_tuple_queue: Queue[tuple[Any, ...] | None], return_value_callback: Callable[[Any], None]): super().__init__() self.operation: Callable[[...], Any] = operation self.argument_tuple_queue: Queue[tuple[Any, ...] | None] = argument_tuple_queue self.return_value_callback: Callable[[Any], None] = return_value_callback def run(self) -&gt; None: while True: argument_tuple: tuple[Any, ...] | None = self.argument_tuple_queue.get() if argument_tuple is None: break try: self.return_value_callback(self.operation(*argument_tuple)) except: traceback.print_exc()def run_simple_data_parallelism_thread_pool( operation_factory: Callable[[], Callable[[...], Any]], argument_tuples: Iterable[tuple[Any, ...]], return_value_callback: Callable[[Any], None] = lambda return_value: None, num_threads: int = multiprocessing.cpu_count()) -&gt; None: # Create argument tuple queue. argument_tuple_queue: Queue[tuple[Any, ...] | None] = Queue() # Create all threads which share an argument tuple queue. threads: list[Thread] = [] for _ in range(num_threads): thread: Thread = DataParallelismThreadPoolThread(operation_factory(), argument_tuple_queue, return_value_callback) thread.start() threads.append(thread) # Enqueue all argument tuples. for argument_tuple in argument_tuples: argument_tuple_queue.put(argument_tuple) # Enqueue sentinel values for all threads to stop once execution on all argument tuples have finished. for _ in range(num_threads): argument_tuple_queue.put(None) # Wait for all threads to stop. for thread in threads: thread.join() To utilize the data parallelism thread pool, provide an operation factory, a collection of argument tuples, and a return value callback. The operation factory, when called, creates operations represented as Callable[[...], Any] objects, accepting arguments from an argument tuple and returning a value. Each thread in the thread pool will execute these operations on the provided argument tuples. Any return values will be passed to the return value callback, which can be customized according to your needs. The data parallelism thread pool will process the argument tuples in parallel until all tuples have been processed. ExampleSay that we want to sleep for $0, 1, 2, \\dots, N - 1$ seconds before printing the number of seconds slept in parallel, where $N$ is the number of threads in our thread pool. We can adopt a task parallelism approach, where we create tasks which encapsulate how long they sleep, and add them to a task parallelism thread pool: 1234567891011121314151617181920import timeimport randomN = 8def create_task(sleep_time: int) -&gt; Callable[[], None]: def task(): nonlocal sleep_time time.sleep(sleep_time) print(f&#x27;Slept for &#123;sleep_time&#125;&#x27;) return taskrun_simple_task_parallelism_thread_pool( (create_task(i) for i in range(N)), N) As an alternative, we can also use a data parallelism approach, in which operations accept the number of seconds they sleep from argument tuples, sleep for those time durations, and return those time durations before return value callbacks operate on the return values and print those time durations: 12345678910111213141516171819202122232425262728import timeimport randomfrom collections.abc import CallableN = 8def operation(sleep_time: int) -&gt; int: time.sleep(sleep_time) return sleep_timedef operation_factory() -&gt; Callable[[int], int]: return operationdef return_value_callback(sleep_time: int) -&gt; None: print(f&#x27;Slept for &#123;sleep_time&#125;&#x27;)run_simple_data_parallelism_thread_pool( operation_factory, ((i,) for i in range(N)), return_value_callback, N) Running both thread pools takes the same time and produces the same output. References https://en.m.wikipedia.org/wiki/Task_parallelism https://en.wikipedia.org/wiki/Data_parallelism","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"}],"tags":[]},{"title":"Batch Killing Processes Looked up Through ps -aux | grep <process_name>","slug":"Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name","date":"2023-06-10T04:00:00.000Z","updated":"2023-06-10T18:33:14.919Z","comments":true,"path":"2023/06/09/Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name/","link":"","permalink":"https://abbaswu.github.io/2023/06/09/Batch-Killing-Processes-Looked-up-Through-ps-aux-grep-process_name/","excerpt":"","text":"Sometimes we acidentally spawn a series of processes, and we want to kill them. We can look up their pid’s through ps -aux | grep &lt;process_name&gt; (as shown below) and manually run the kill command to kill each process by providing its pid, but how can we automate this tedious task? 12345678910$ ps aux | grep pipreqsjifengwu 58180 0.0 0.1 46440 27332 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/spectria/tildes/tildes --mode no-pinjifengwu 58205 0.1 0.1 48140 29392 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/PnX-SI/GeoNature --mode no-pinjifengwu 58224 5.7 0.2 51856 33108 pts/0 T 13:38 0:23 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/fabiandevia/home --mode no-pinjifengwu 58267 4.4 0.2 57880 38204 pts/0 T 13:39 0:17 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/377312117/gitproject --mode no-pinjifengwu 58272 2.3 0.2 53756 34252 pts/0 T 13:39 0:08 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/crazyfish1111/home --mode no-pinjifengwu 58282 0.1 0.1 47840 28132 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/Piratenpartei/ekklesia-portal --mode no-pinjifengwu 58295 0.1 0.1 48220 28492 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/jauhararifin/ugrade/server --mode no-pinjifengwu 58659 0.3 0.1 48608 29324 pts/0 T 13:41 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/klen/pylama --mode no-pinjifengwu 59564 0.0 0.0 19612 2516 pts/2 S+ 13:45 0:00 grep --color=auto pipreqs First, we can add grep -v grep to the pipe to hide the grep processes from the output: 123456789$ ps aux | grep pipreqs | grep -v grepjifengwu 58180 0.0 0.1 46440 27332 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/spectria/tildes/tildes --mode no-pinjifengwu 58205 0.1 0.1 48140 29392 pts/0 T 13:38 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/PnX-SI/GeoNature --mode no-pinjifengwu 58224 5.6 0.2 51856 33108 pts/0 T 13:38 0:23 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/fabiandevia/home --mode no-pinjifengwu 58267 4.4 0.2 57880 38204 pts/0 T 13:39 0:17 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/377312117/gitproject --mode no-pinjifengwu 58272 2.2 0.2 53756 34252 pts/0 T 13:39 0:08 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/crazyfish1111/home --mode no-pinjifengwu 58282 0.1 0.1 47840 28132 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/Piratenpartei/ekklesia-portal --mode no-pinjifengwu 58295 0.1 0.1 48220 28492 pts/0 T 13:39 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/jauhararifin/ugrade/server --mode no-pinjifengwu 58659 0.3 0.1 48608 29324 pts/0 T 13:41 0:00 /home/jifengwu/miniconda3/envs/dataset_investigation/bin/python /home/jifengwu/miniconda3/envs/dataset_investigation/bin/pipreqs repos/klen/pylama --mode no-pin Then, we can add awk &#39;&#123;print $2&#125;&#39; to the pipe to invoke awk to trim the second space-delimited component (which in this case is the pid). Now we have a list of the pid’s of the processes we want to kill: 123456789$ ps aux | grep pipreqs | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;5818058205582245826758272582825829558659 Finally, we can iterate over the pid’s in a for-loop to kill them. 1234$ for pid in $(ps aux | grep pipreqs | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;)&gt; do&gt; kill -15 $pid&gt; done References https://www.baeldung.com/linux/grep-exclude-ps-results https://stackoverflow.com/questions/46008880/how-to-always-cut-the-pid-from-ps-aux-command","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Unix","slug":"Code/Unix","permalink":"https://abbaswu.github.io/categories/Code/Unix/"}],"tags":[]},{"title":"Parsing Command-line Options in Shell Scripts Using `getopts`","slug":"Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts","date":"2023-06-09T04:00:00.000Z","updated":"2023-06-09T19:00:27.302Z","comments":true,"path":"2023/06/08/Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts/","link":"","permalink":"https://abbaswu.github.io/2023/06/08/Parsing-Command-line-Options-in-Shell-Scripts-Using-getopts/","excerpt":"","text":"What is getoptsgetopts is a built-in Unix shell command for parsing command-line options. It is a wrapper around getopt, a POSIX C library function used to parse command-line options of the Unix&#x2F;POSIX style. Specifically: Options are single-character alphanumerics preceded by a - (hyphen-minus) character, i.e. -a. -b, -c. Options can take an argument or none. Multiple options can be chained together, as long as the non-last ones are not argument-taking. If -a and -b take no arguments while -c takes an argument, -abc foo is the same as -a -c -e foo, but -bca is not the same as -b -c a due to the preceding rule. When an option takes an argument, this can be in the same token or in the next one. In other words, if -c takes an argument, -cfoo is the same as -c foo. optstring‘sBoth getopt and getopts specifies specify options using a optstring. Specifically: Begin an optstring with :. To specify an option that does not take an argument, append its name to the optstring. To specify an option that takes an argument, append its name and : to the optstring. For example, the optstring that specifies two options -a, -b that do not take arguments and two options -c, -d that take arguments is :abc:d:. Using getopts in a Shell ScriptIn Shell scripts, getopts invoked with an optstring is used with a while-loop to parse command-line options. Say that our Shell script test_getopts.sh accepts two options -a, -b that do not take arguments and two options -c, -d that take arguments. Our Shell script can look like this: 12345678910111213141516171819202122232425#!/bin/shwhile getopts &#x27;:abc:d:&#x27; namedo case $name in a) echo &quot;You provided option -a&quot; ;; b) echo &quot;You provided option -b&quot; ;; c) echo &quot;You provided option -c with argument $OPTARG&quot; ;; d) echo &quot;You provided option -d with argument $OPTARG&quot; ;; :) echo &quot;Option -$OPTARG requires an argument&quot; ;; ?) echo &quot;You provided an invalid option -$OPTARG&quot; ;; esacdone Here, getopts is invoked with the optstring for specifying our options, :abc:d:. In each iteration of the while-loop, the next option is parsed and the Shell variables name and OPTARG are set to different values based on different conditions we may encounter. If a valid option is detected and that option does not take an argument, the Shell variable name is set to the name of the option. If a valid option is detected and that option takes an argument: If we have provided an argument, the Shell variable name is set to the name of the option, and the Shell variable OPTARG is set to the value of the argument. If we haven’t provided an argument, the Shell variable name is set to :, and the Shell variable OPTARG is set to the name of the argument. If an invalid option is detected, the Shell variable name is set to ?, and the Shell variable OPTARG is set to the name of the argument. We can see getopts at work by providing different command-line options when invoking our Shell script. Providing no command-line options: 1$ sh test_getopts.sh Providing option -a that do not take arguments: 12$ sh test_getopts.sh -aYou provided option -a Providing option -a that do not take arguments twice: 123456$ sh test_getopts.sh -a -aYou provided option -aYou provided option -a$ sh test_getopts.sh -aaYou provided option -aYou provided option -a Providing option -c that takes an argument with an argument foo: 12$ sh test_getopts.sh -c fooYou provided option -c with argument foo Providing option -c that takes an argument with an argument foo twice: 123$ sh test_getopts.sh -c foo -c barYou provided option -c with argument fooYou provided option -c with argument bar Providing option -c that takes an argument without an argument: 12$ sh test_getopts.sh -cOption -c requires an argument Providing an invalid argument -e: 12$ sh test_getopts.sh -eYou provided an invalid option -e References https://en.wikipedia.org/wiki/Getopts https://pubs.opengroup.org/onlinepubs/9699919799/utilities/getopts.html https://en.wikipedia.org/wiki/Getopt https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap12.html","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Unix","slug":"Code/Unix","permalink":"https://abbaswu.github.io/categories/Code/Unix/"}],"tags":[]},{"title":"Paper Review: Sized Types","slug":"Paper-Review-Sized-Types","date":"2023-03-25T07:00:00.000Z","updated":"2023-04-11T22:20:17.304Z","comments":true,"path":"2023/03/25/Paper-Review-Sized-Types/","link":"","permalink":"https://abbaswu.github.io/2023/03/25/Paper-Review-Sized-Types/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Type Systems. The original paper can be found here. SummaryYou can check the presentation that I made for this paper in this GitHub repository. CritiqueAlthough I liked the idea of Sized Types proposed in the motivation, this paper was difficult for me to grasp, and after spending days reading it, there are still sections which I am confused about. I have summarized my understanding of this paper in the uploaded PDF, and I will discuss my thoughts here. I really like the idea of Sized Types that they can be used to prove data computations terminate and codata computations are productive using the same formalization. Apparently, the requirement that size indexes in Sized Types be natural number size variables, the special index $\\omega$, or linear functions of the size variables facilitates generating constraints in the type checking algorithm that can be solved by a constraint solver (e.g. an SMT solver). Although this may lead to overapproximation in certain scenarions (for example, representing the type of the factorial function), over all, I consider it to be a good balance point between expressiveness and usability. 3.2 Semantics of Expressions, 3.3 The Universe of Types, 3.4 Continuity and Ordinals, 3.5 Semantics of Types, and 3.7 $\\omega$-Types used a lot of concepts before properly introducing them, and I couldn’t understand this part. The example presented to demonstrate the type checking algorithm involves generating constraints. However, only the generated constraints are presented, while how the constraints are generated and what each symbol in the constraints stand for with regards to the aforementioned AST nodes is unknown.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Review: Refinement Types","slug":"Paper-Review-Refinement-Types","date":"2023-03-19T07:00:00.000Z","updated":"2023-03-20T03:56:54.982Z","comments":true,"path":"2023/03/19/Paper-Review-Refinement-Types/","link":"","permalink":"https://abbaswu.github.io/2023/03/19/Paper-Review-Refinement-Types/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Type Systems. The original paper can be found here. SummaryThis paper presents a clear and organized guide to refinement type systems by condensing the extensive literature on the topic and demonstrating the implementation of a refinement type checker. It first states the motivation for requirement types, a history of requirement types, and refinement logic, which is a logic system used in the proposed refinement type checker. The rest of the paper shows the implementation of a refinement type checker through a series of programming languages, beginning with simply-typed lambda calculus and incrementally adding additional features. This approach is influenced by the nanopass framework, which is used to teach compilation. CritiqueHonestly, I have found the section on implementing a refinement type checker through a series of programming languages challenging to understand. Still, I have understood much of the paper before that. Therefore, I will summarize my gained insights and state questions that I have in mind. InsightsRefinement Types as SubtypesType systems are the most commonly employed technique for ensuring the correct behavior of software. However, even well-typed programs can contain various bugs, such as buffer overflows, divisions by zero, logic bugs, and out-of-bounds array accesses. One approach to address this issue is to enhance a language’s types with subtypes that limit the range of valid values with predicates, such as ‘non-negative integer’ from ‘integer.’ These subtypes are known as ‘refinement types.’ They enable developers to write precise contracts for valid inputs and outputs of functions and specify the correctness properties. This brings formal verification into mainstream software development. Refinement Logic and How it Maps to SMT ExpressionsI was partically impressed by refinement logic, the logic system used in the proposed refinement type checker, as it is both expressive and easy to be verified using an SMT solver. Refinement logic consists of two parts: predicates and constraints. Predicates are drawn from the quantifier-free fragment of linear arithmetic and uninterpreted functions (commonly used in SMT solvers), and may include boolean and integer literals, boolean and integer variables, arithmetic operators, boolean operators, comparisons, the ‘if-then-else’ expression, and uninterpreted functions (resembling those in z3). Predicates are the building block of constraints, which are generated from refinement type checking. A constraint is either a predicate, an implication $\\forall t: T : p \\Rightarrow c$ which states that for each term $t$ of type $T$, if the predicate $p$ holds then another constraint $c$ must be true, or a conjunction of two other constraints. Constraints can be verified by checking whether there is no satisfying assignment for the negated constraint. In this process, they can be converted into SMT expressions in a straightforward way. For example, the constraint presented in the paper $$c &#x3D; \\forall x: array : 0 \\le length(x) \\Rightarrow \\forall n: int : n &#x3D; length(x) \\Rightarrow \\forall i: int : i &#x3D; n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x)$$ can be negated as follows: $$\\neg c$$ $$\\neg (\\forall x: array : 0 \\le length(x) \\Rightarrow \\forall n: int : n &#x3D; length(x) \\Rightarrow \\forall i: int : i &#x3D; n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))$$ $$\\exists x: array : 0 \\le length(x) \\land \\neg (\\forall n: int : n &#x3D; length(x) \\Rightarrow \\forall i: int : i &#x3D; n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))$$ $$\\exists x: array : 0 \\le length(x) \\land \\exists n: int : n &#x3D; length(x) \\land \\neg (\\forall i: int : i &#x3D; n - 1 \\Rightarrow 0 \\le i \\land i &lt; length(x))$$ $$\\exists x: array : 0 \\le length(x) \\land \\exists n: int : n &#x3D; length(x) \\land \\exists i: int : i &#x3D; n - 1 \\land \\neg (0 \\le i \\land i &lt; length(x))$$ $$\\exists x: array : 0 \\le length(x) \\land \\exists n: int : n &#x3D; length(x) \\land \\exists i: int : i &#x3D; n - 1 \\land (0 &gt; i \\lor i \\ge length(x))$$ We can verify the negated constraint using an SMT solver: 123456789101112131415161718192021222324252627In [1]: import z3In [2]: L = z3.Int(&#x27;L&#x27;)In [3]: n = z3.Int(&#x27;n&#x27;)In [4]: i = z3.Int(&#x27;i&#x27;)In [5]: solver = z3.Solver()In [6]: solver.add(0 &lt;= L)In [7]: solver.add(n == L)In [8]: solver.add(i == n - 1)In [9]: solver.add(z3.Or(i &lt; 0, i &gt; L))In [10]: check_sat_result = solver.check()In [11]: check_sat_resultOut[11]: satIn [12]: model_ref = solver.model()In [13]: model_refOut[13]: [i = -1, L = 0, n = 0] Note that check_sat_result is sat and we can find a satisfying assignment for $\\neg c$: $i &#x3D; -1, length(x) &#x3D; 0, n &#x3D; 0$. This means that the original constraint $c$ is invalid. QuestionsAlthough the concept of refinement types is neat, what is the burden on programmers of writing refinement types that describe legal inputs and outputs of functions? This is a critical aspect to determine whether refinement types can bring formal verification into mainstream software development. Furthermore, constraints in the proposed refinement logic generated by the refinement type checker can be negated and converted into SMT expressions. However, what is the feasibility of doing such checking for large-scale programs? Would it become unscalable?","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Rapidly Prototyping Presentations Using Markdown with Marp","slug":"Rapidly-Prototyping-Presentations-Using-Markdown-with-Marp","date":"2023-03-17T07:00:00.000Z","updated":"2023-04-11T22:24:04.224Z","comments":true,"path":"2023/03/17/Rapidly-Prototyping-Presentations-Using-Markdown-with-Marp/","link":"","permalink":"https://abbaswu.github.io/2023/03/17/Rapidly-Prototyping-Presentations-Using-Markdown-with-Marp/","excerpt":"","text":"In the need to create a decent, academic presentation fast? LaTeX overly verbose? No time to spend on adjusting style? Have notes written in Markdown? The solution: create presentations with Markdown using Marp! Source code and compiled PDF of the presentation for “Rapidly Prototyping Presentations Using Markdown with Marp” presented at the SPL Workshop 2023W2 is available in this GitHub repository.","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"}],"tags":[]},{"title":"Paper Review: How to make ad-hoc polymorphism less ad-hoc","slug":"Paper-Review-How-to-make-ad-hoc-polymorphism-less-ad-hoc","date":"2023-03-06T08:00:00.000Z","updated":"2023-03-06T18:35:38.150Z","comments":true,"path":"2023/03/06/Paper-Review-How-to-make-ad-hoc-polymorphism-less-ad-hoc/","link":"","permalink":"https://abbaswu.github.io/2023/03/06/Paper-Review-How-to-make-ad-hoc-polymorphism-less-ad-hoc/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Type Systems. The original paper can be found here. SummaryThe paper first defines and compares parametric and ad-hoc polymorphism and points out the limitations of existing implementations of ad-hoc polymorphism. It then presents type classes that extend the Hindley&#x2F;Milner type system to support ad-hoc polymorphism as a remedy to these limitations and explains how to translate a program using type classes into an equivalent program without them at compile-time. Furthermore, it showcases the power of type classes and the translation mechanism using the example of a polymorphic equality operation. Finally, it explores subclassing of type classes. CritiqueThe paper is easy to follow as it is written in a lucid manner and gives an informal introduction to type classes and its translation rules. Furthermore, the motivation for type classes and how it connects to object-oriented programming languages is explicitly stated in the paper. I have further looked up some material following these lines. I will summarize them before presenting some questions and comments. My TakeawaysDifferent Types of PolymorphismSee my Paper Review for “Types and Programming Languages” Chapter 15 and Chapter 16. Type Classes and Protocols&#x2F;Interfaces in Smalltalk&#x2F;Objective-C&#x2F;Java&#x2F;C#An interface is an abstract type used to provide a collection of methods compliant classes must implement in the Java (and C#) programming languages. Java is mostly influenced by Objective-C, and Java’s interfaces are adaptations of the protocols in Objective-C and Smalltalk, which in turn is based on protocols in networking, notably the ARPANet. Although Type Classes and Interfaces do not share a common lineage, it is straightforward to implement Type Classes with Generic Interfaces whose Generic Parameters should be Classes that comply with the Interface. For instance, the Type Class below specifies the equal (&#x3D;&#x3D;) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented in Java using the following Generic Interface: 123interface Eq&lt;T&gt; &#123; boolean isEqual(T other);&#125; Type Classes and Concepts in C++Although Java’s syntax resembles C++’s, its semantics of late-binding, single inheritance, class objects, and an extensive runtime system are in the lineage of Smalltalk and Objective-C, far away from that of C++’s. However, in C++’s Template Metaprogramming world, Concepts, added in C++20, resembles Type Classes. Template Metaprogramming in C++ had been untyped, with template parameters being generic type variables substituted at template instantiation. In C++20, a type system has been added to this untyped template language through concepts. They are Boolean predicates on template parameters evaluated at the point of, not after, template instantiation. The compiler will produce a clear error immediately if a programmer tries to use a template parameter that doesn’t meet the requirements of a concept. This starkly contrasts the challenging-to-grasp errors reported after an invalid type substitutes a generic type variable emanating from the implementation context rather than the template instantiation itself. For instance, the first two arguments to std::sort must be random-access iterators. If an argument is not a random-access iterator, an error will occur when std::sort attempts to use it as a bidirectional iterator. 12std::list&lt;int&gt; l = &#123;2, 1, 3&#125;;std::sort(l.begin(), l.end()); Without concepts, compilers may produce large amounts of error information, starting with an equation that failed to compile when it tried to subtract two non-random-access iterators: 123In instantiation of &#x27;void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = std::_List_iterator&lt;int&gt;; _Compare = __gnu_cxx::__ops::_Iter_less_iter]&#x27;: error: no match for &#x27;operator-&#x27; (operand types are &#x27;std::_List_iterator&lt;int&gt;&#x27; and &#x27;std::_List_iterator&lt;int&gt;&#x27;) std::__lg(__last - __first) * 2, However, if concepts are used, the problem can be found and reported at template instantiation: 12error: cannot call function &#x27;void std::sort(_RAIter, _RAIter) [with _RAIter = std::_List_iterator&lt;int&gt;]&#x27;note: concept &#x27;RandomAccessIterator()&#x27; was not satisfied It is straightforward to implement Type Classes with concepts. For instance, the Type Class below specifies the equal (&#x3D;&#x3D;) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented using the following C++ concept: 123456789#include &lt;concepts&gt;// Declaration of the concept &quot;Eq&quot;, which is satisfied by any type &#x27;T&#x27;// such that for values &#x27;t&#x27; of type &#x27;T&#x27;, the expression t == t compiles// and its type satisfies the concept std::same_as&lt;bool&gt;template &lt;typename T&gt; concept Eq = requires (T t) &#123; &#123; t == t &#125; -&gt; std::same_as&lt;bool&gt;;&#125;; Afterwards, such a concept can be specified when template parameters are being introduced in a template definition, to indicate that the corresponding template parameter must satisfy the concept. 123template&lt;Eq T&gt; void f(const T&amp; t) &#123; // ...&#125; References https://stackoverflow.com/questions/6948166/javas-interface-and-haskells-type-class-differences-and-similarities https://cs.gmu.edu/~sean/stuff/java-objc.html https://functionalcpp.wordpress.com/2013/08/16/type-classes/ https://stackoverflow.com/questions/32124627/how-are-c-concepts-different-to-haskell-typeclasses https://wiki.haskell.org/OOP_vs_type_classes https://doi.org/10.1145/1411318.1411324 https://www.foonathan.net/2021/07/concepts-structural-nominal/ https://www.reddit.com/r/haskell/comments/1e9f49/concepts_in_c_template_programming_and_type/ Questions and Comments The translation mechanism (pre-processor) proposed in this paper translates a program using type classes into an equivalent program without them at compile-time so that an existing Hindley&#x2F;Milner type system can be used afterward instead of having to develop a new, complex type system to support type classes. This is indeed a very clever mechanism. Can this be viewed as an example of desugaring?","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Understanding Lambda Calculus Through C++","slug":"Understanding-Lambda-Calculus-Through-CXX","date":"2023-03-01T08:00:00.000Z","updated":"2023-03-22T20:14:45.241Z","comments":true,"path":"2023/03/01/Understanding-Lambda-Calculus-Through-CXX/","link":"","permalink":"https://abbaswu.github.io/2023/03/01/Understanding-Lambda-Calculus-Through-CXX/","excerpt":"","text":"Fixed-point Combinators and Recursive Lambda ExpressionsIn Lambda Calculus, we cannot refer to the Lambda Abstraction itself within a Lambda Abstraction. Thus, we cannot straightforwardly implement recursion. A workaround for this is Fixed-point Combinators, in which we add another parameter to the Lambda Abstraction, which is passed the Lambda Abstraction itself when the Lambda Abstraction is being Applied. Similarly, C++ does not allow defining a recursive lambda expression. A workaround is to add another parameter to the lambda expression, which would be used for recursive calling and is passed the value of the lambda expression itself when the lambda expression is being called. We can intuitively name this parameter “itself.” The type of itself is the type of the lambda expression, which has to be inferred. Thus, we should use auto to represent its type. Using auto in a lambda expression’s parameter list requires C++14 or above. For example, we can write the following lambda expression to calculate the nth Fibonacci Number recursively: 1234567const auto fib = [](const auto itself, const unsigned int n) -&gt; unsigned long &#123; if (n == 0) return 0UL; else &#123; if (n == 1) return 1UL; else return itself(itself, n - 1) + itself(itself, n - 2); &#125;&#125;; This lambda expression can be called like this: 1unsigned long result = fib(fib, input); C++ compilers can optimize for such usages. For example, the following program: 123456789101112131415161718192021#include &lt;stdio.h&gt;const auto fib = [](const auto itself, const unsigned int n) -&gt; unsigned long &#123; if (n == 0) return 0UL; else &#123; if (n == 1) return 1UL; else return itself(itself, n - 1) + itself(itself, n - 2); &#125;&#125;;int main() &#123; unsigned int input; scanf(&quot;%u&quot;, &amp;input); unsigned long result = fib(fib, input); printf(&quot;%lu\\n&quot;, result); return 0;&#125; compiles to the following LLVM IR with clang++ -std=c++14 -O1 -S -emit-llvm, where calling fib within main has been transformed to calling a recursive function _ZNK3$_0clIS_EEmT_j, with the first parameter itself optimized. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@.str = private unnamed_addr constant [3 x i8] c&quot;%u\\00&quot;, align 1@.str.1 = private unnamed_addr constant [5 x i8] c&quot;%lu\\0A\\00&quot;, align 1; Function Attrs: norecurse nounwind uwtabledefine dso_local i32 @main() local_unnamed_addr #0 &#123; %1 = alloca i32, align 4 %2 = bitcast i32* %1 to i8* call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4 %3 = call i32 (i8*, ...) @__isoc99_scanf(i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str, i64 0, i64 0), i32* nonnull %1) %4 = load i32, i32* %1, align 4, !tbaa !2 %5 = call fastcc i64 @&quot;_ZNK3$_0clIS_EEmT_j&quot;(i32 %4) %6 = call i32 (i8*, ...) @printf(i8* nonnull dereferenceable(1) getelementptr inbounds ([5 x i8], [5 x i8]* @.str.1, i64 0, i64 0), i64 %5) call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #4 ret i32 0&#125;; Function Attrs: argmemonly nounwind willreturndeclare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1; Function Attrs: nofree nounwinddeclare dso_local i32 @__isoc99_scanf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: inlinehint nounwind readnone uwtabledefine internal fastcc i64 @&quot;_ZNK3$_0clIS_EEmT_j&quot;(i32 %0) unnamed_addr #3 align 2 &#123; switch i32 %0, label %3 [ i32 0, label %9 i32 1, label %2 ]2: ; preds = %1 br label %93: ; preds = %1 %4 = add i32 %0, -1 %5 = call fastcc i64 @&quot;_ZNK3$_0clIS_EEmT_j&quot;(i32 %4) %6 = add i32 %0, -2 %7 = call fastcc i64 @&quot;_ZNK3$_0clIS_EEmT_j&quot;(i32 %6) %8 = add i64 %7, %5 br label %99: ; preds = %1, %3, %2 %10 = phi i64 [ 1, %2 ], [ %8, %3 ], [ 0, %1 ] ret i64 %10&#125;; Function Attrs: nofree nounwinddeclare dso_local i32 @printf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: argmemonly nounwind willreturndeclare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1 Type Abstractions and Template FunctionsPolymorphic Lambda Calculus (also known as Second Order Lambda Calculus or System F) introduces Type Abstractions and Type Applications. A Type Abstraction, written as λ X . t, represents a Term (often a Lambda Abstraction) t containing a Type Variable X. A Type Application, written as t [T], uses a Concrete Type T to replace all instances of the Type Variable in the Term of the Type Abstraction. This can be used to implement Polymorphic Lambda Abstractions. For example, the following Type Abstraction representing a Polymorphic Identity Function: 1id = λ X . λ x: X . x can be instantiated to yield any concrete identity function that may be required, such as id [Nat]: Nat -&gt; Nat. Such Type Abstractions can be implemented in C++ using template functions: 123template &lt;typename X&gt; auto id(X x) &#123; return x;&#125; while Type Applications correspond to template instantiations: 1id&lt;int&gt; Should the template function be passed a callable, we usually want to use a template typename to support functions, function pointers, functors, and lambda expressions. Alternatively, we can also use auto to represent its type in the template function’s parameter list. Note that using auto in a (non-lambda expression) function’s parameter list requires C++20 or above. For example, the following Type Abstraction: 1double = λ X . λ f: X -&gt; X . λ a: X . f(f a) can be represented using the following template function: 12345template &lt;typename X, typename F&gt; auto double_(const F f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125; or in C++20 or above: 12345template &lt;typename X&gt; auto double_(const auto f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125; C++ compilers support aggressive inlining optimizations when lambda expressions are used. For example, the call to const auto g = double_&lt;int&gt;([](int x) &#123; return 2 * x; &#125;); in the following source code: 1234567891011121314151617181920#include &lt;stdio.h&gt;template &lt;typename X&gt; auto double_(const auto f) &#123; return [f](X a) &#123; return f(f(a)); &#125;;&#125;int main() &#123; const auto g = double_&lt;int&gt;([](int x) &#123; return 2 * x; &#125;); int input; scanf(&quot;%d&quot;, &amp;input); printf(&quot;%d\\n&quot;, g(input)); return 0;&#125; has been completely inlined to %5 = shl i32 %4, 2 in the LLVM IR generated with clang++ -std=c++20 -O2 -S -emit-llvm: 123456789101112131415161718192021222324252627@.str = private unnamed_addr constant [3 x i8] c&quot;%d\\00&quot;, align 1@.str.1 = private unnamed_addr constant [4 x i8] c&quot;%d\\0A\\00&quot;, align 1; Function Attrs: norecurse nounwind uwtabledefine dso_local i32 @main() local_unnamed_addr #0 &#123; %1 = alloca i32, align 4 %2 = bitcast i32* %1 to i8* call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #3 %3 = call i32 (i8*, ...) @__isoc99_scanf(i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str, i64 0, i64 0), i32* nonnull %1) %4 = load i32, i32* %1, align 4, !tbaa !2 %5 = shl i32 %4, 2 %6 = call i32 (i8*, ...) @printf(i8* nonnull dereferenceable(1) getelementptr inbounds ([4 x i8], [4 x i8]* @.str.1, i64 0, i64 0), i32 %5) call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #3 ret i32 0&#125;; Function Attrs: argmemonly nounwind willreturndeclare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1; Function Attrs: nofree nounwinddeclare dso_local i32 @__isoc99_scanf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: nofree nounwinddeclare dso_local i32 @printf(i8* nocapture readonly, ...) local_unnamed_addr #2; Function Attrs: argmemonly nounwind willreturndeclare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1 Type Classes and Concepts in C++Template Metaprogramming in C++ had been untyped, with template parameters being generic type variables substituted at template instantiation. In C++20, a type system has been added to this untyped template language through concepts. They are Boolean predicates on template parameters evaluated at the point of, not after, template instantiation. The compiler will produce a clear error immediately if a programmer tries to use a template parameter that doesn’t meet the requirements of a concept. This starkly contrasts the challenging-to-grasp errors reported after an invalid type substitutes a generic type variable emanating from the implementation context rather than the template instantiation itself. For instance, the first two arguments to std::sort must be random-access iterators. If an argument is not a random-access iterator, an error will occur when std::sort attempts to use it as a bidirectional iterator. 12std::list&lt;int&gt; l = &#123;2, 1, 3&#125;;std::sort(l.begin(), l.end()); Without concepts, compilers may produce large amounts of error information, starting with an equation that failed to compile when it tried to subtract two non-random-access iterators: 123In instantiation of &#x27;void std::__sort(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = std::_List_iterator&lt;int&gt;; _Compare = __gnu_cxx::__ops::_Iter_less_iter]&#x27;: error: no match for &#x27;operator-&#x27; (operand types are &#x27;std::_List_iterator&lt;int&gt;&#x27; and &#x27;std::_List_iterator&lt;int&gt;&#x27;) std::__lg(__last - __first) * 2, However, if concepts are used, the problem can be found and reported at template instantiation: 12error: cannot call function &#x27;void std::sort(_RAIter, _RAIter) [with _RAIter = std::_List_iterator&lt;int&gt;]&#x27;note: concept &#x27;RandomAccessIterator()&#x27; was not satisfied It is straightforward to implement Type Classes with concepts. For instance, the Type Class below specifies the equal (&#x3D;&#x3D;) operations for Type Constructors that are its instances: 12class Eq a where (==) :: a -&gt; a -&gt; Bool This can be implemented using the following C++ concept: 123456789#include &lt;concepts&gt;// Declaration of the concept &quot;Eq&quot;,// which is satisfied by any type &#x27;T&#x27; such that for values &#x27;t&#x27; of type &#x27;T&#x27;, the expression t == t compiles and its type satisfies the concept std::same_as&lt;bool&gt;// This is represented using a &quot;requires expression&quot; which returns a booltemplate &lt;typename T&gt; concept Eq = requires (T t) &#123; &#123; t == t &#125; -&gt; std::same_as&lt;bool&gt;;&#125;; Afterwards, such a concept can be specified when template parameters are being introduced in a template definition, to indicate that the corresponding template parameter must satisfy the concept. 123template&lt;Eq T&gt; void f(const T&amp; t) &#123; // ...&#125; or (using a “requires clause”): 123template&lt;typename T&gt; requires Eq&lt;T&gt; void f(const T&amp; t) &#123; // ...&#125; References https://stackoverflow.com/questions/6948166/javas-interface-and-haskells-type-class-differences-and-similarities https://cs.gmu.edu/~sean/stuff/java-objc.html https://functionalcpp.wordpress.com/2013/08/16/type-classes/ https://stackoverflow.com/questions/32124627/how-are-c-concepts-different-to-haskell-typeclasses https://wiki.haskell.org/OOP_vs_type_classes https://doi.org/10.1145/1411318.1411324 https://www.foonathan.net/2021/07/concepts-structural-nominal/ https://www.reddit.com/r/haskell/comments/1e9f49/concepts_in_c_template_programming_and_type/","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"C++","slug":"Code/C","permalink":"https://abbaswu.github.io/categories/Code/C/"}],"tags":[]},{"title":"Paper Review: \"Types and Programming Languages\" Chapter 22","slug":"Paper-Review-Types-and-Programming-Languages-Chapter-22","date":"2023-02-26T08:00:00.000Z","updated":"2023-02-26T21:54:43.983Z","comments":true,"path":"2023/02/26/Paper-Review-Types-and-Programming-Languages-Chapter-22/","link":"","permalink":"https://abbaswu.github.io/2023/02/26/Paper-Review-Types-and-Programming-Languages-Chapter-22/","excerpt":"","text":"SummaryChapter 22 of “Types and Programming Languages” explores the problem of Type Reconstruction (Type Inference) or deriving Types for Unannotated Arguments of Lambda Abstractions. It first introduces Type Variables and Substitutions before formalizing the Type Reconstruction problem. Then, it points out that Type Reconstruction can be implemented using a Constraint Typing Algorithm or an Algorithm that calculates a Set of Constraints between Types involving Type Variables and records them for later consideration, and proves the Completeness and Soundness of Constraint Typing. Moreover, it introduces a Unification Algorithm to calculate Principle Solutions (most general solutions) to Constraint Sets. Finally, the Chapter presents how the Typing Rules for Let Expressions can be modified to support Let Polymorphism - allowing an Untyped Function to generate different Constraints, thus be able to be Reconstructed to Different Types when applied to Terms of different Types. CritiqueOverall, Chapter 22 is clearly written, and several sections intrigued me (such as that Parametric Polymorphism and Type Reconstruction can result from two different interpretations of Dependent Types containing Type Variables). Moreover, this Chapter provides essential inspiration for my Class Project, “Inferring Feasible Types for the Parameters and Return Values of Python Functions.” However, the Chapter also used some Concepts without introducing them (such as the Unification Problem), and I had to look into them to understand parts of the Chapter. Background KnowledgeCompleteness and Soundness of a TheoryUsing $TRUE$ and $PROVABLE$ to represent the Set of Facts that are True and Provable under a Theory, respectively: Completeness: $TRUE \\subseteq PROVABLE$ or every Fact that is True is also Provable (but there may be some Facts that are Provable but are not True). Soundness: $PROVABLE \\subseteq TRUE$ or every Fact that is Provable is also True (but there may be some True Facts that are not Provable). Completeness and Soundness: $TRUE &#x3D; PROVABLE$. An ideal Theory should be both Complete and Sound. Unification ProblemGiven two Terms containing some Variables, find a Substitution (an Assignment of Terms to Variables) that makes the two Terms equal. For example, given $f(x_1, h(x_1), x_2) &#x3D; f(g(x_3), x_4, x_3)$, a valid Substitution is $\\sigma &#x3D; {g(x_3): x_1, x_3: x_2, h(g(x_3)): x_4}$. Takeaways From This PaperParametric Polymorphism and Type ReconstructionGiven Dependent Types containing Type Variables (often the result of the Programmer leaving out Type Annotations in Source Code), we can make one of the following assumptions. All Substitution Instances are well-typed. Thus, it is possible for Type Variables to be held abstract during Type Checking and only be Substituted for Concrete Types later on. This is the basis of Parametric Polymorphism. Not all Substitution Instances are well-typed. In this case, we want to look for valid Substitutions. This leads us to the problem of Type Reconstruction. Deriving Constraint Sets and Calculating Solutions to ThemTo explore valid ways that Concrete Types can substitute Type Variables, we can calculate a Set of Constraints between Types involving Type Variables. This is similar to an ordinary Type Checking Algorithm checking Requirements in the Premise but records these Requirements as Constraints for later consideration instead of checking them immediately. After we have generated a Constraint Set, we can use a Unification Algorithm to calculate Solutions to it. The Unification Algorithm proposed in the Chapter removes a Constraint from the Constraint Set, processes it, and recursively processes the remaining Constraint Set. There is a most general way to instantiate the Type Variables. This is known as a Principle Solution, which contains Principle Types, or the most general types, for Type Variables. Inspirations From This PaperThis Paper points out a viable way to implement my Class Project “Inferring Feasible Types for the Parameters and Return Values of Python Functions.” Propose Typing Rules for Python Expressions. Implement an Algorithm similar to an ordinary Type Checking Algorithm checking Requirements in the Premise, but which records these Requirements as Constraints for later consideration instead of checking them immediately. Questions What are the specific types of Constraints that are recorded when deriving Constraint Sets? What do the derived Constraint Sets look like? Implementing the Unification Algorithm proposed to calculate Solutions to the Constraint Set seems non-trivial. Are there any implementations of it for more “real-world” (imperative, non-ML Family) Programming Languages? What adjustments have to be made to accomplish such an implementation?","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Displaying Information for Thrown and Caught Exceptions to the User in Python","slug":"Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python","date":"2023-02-20T08:00:00.000Z","updated":"2023-02-20T19:51:07.739Z","comments":true,"path":"2023/02/20/Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python/","link":"","permalink":"https://abbaswu.github.io/2023/02/20/Displaying-Information-for-Thrown-and-Caught-Exceptions-to-the-User-in-Python/","excerpt":"","text":"Exception Semantics in PythonException handling refers to how a program reacts when unexpected events, known as exceptions, occur throughout the program’s execution. Exception semantics varies considerably among programming languages. Based on this, we can divide programming languages into two groups: Programming languages that only employ exceptions to address exceptional, unforeseen, or incorrect circumstances, such as C++, Java, and C#. Programming languages that use exceptions as standard flow control structures, such as Ada, ML, OCaml, Python, and Ruby. For example, in Python, when an iterator has exhausted its output, and no more items can be generated, an exception of type StopIteration is thrown. As a result, exceptions are pervasive in Python, and exception catching and handling is a must for writing robust Python code. Displaying Information for Thrown and Caught Exceptions to the UserIn many situations, it is beneficial to handle the exception and give a user a “loud and clear” message of what has happened as feedback. This is also particularly useful in investigating the root cause of the exception and whether it is the tip of the iceberg of a more significant latent bug. This can be simplified by the fact that exceptions thrown by built-in functions, standard library functions, and functions in many well-tested third-party libraries all contain rich semantics in: The class of the exception. Given an exception e, it is accessible via type(e), and type(e).__name__ gives a str representation. The message of the exception. Given an exception e, str(e) generates a representation of the argument(s) to the instance. In command-line programs, we can write both of them to stderr, as shown in the example below: 12345678from sys import stderrtry: # Do some potentially erroneous operationexcept Exception as e: # Write the class of the exception and the message of the exception to stderr print(type(e).__name__, str(e), file=stderr) In GUI programs, we can display them in a message box, with the class of the exception being the title of the message box and the message of the exception being the message of the message box, as shown in the example below: 12345678910111213141516171819202122232425262728from PySide6.QtCore import Slotfrom PySide6.QtWidgets import QDialog, QMessageBoxfrom .ui import Ui_ConnectToServerDialogclass ConnectToServerDialog(QDialog): def __init__(self, parent=None): super().__init__(parent) self.ui=Ui_ConnectToServerDialog() self.ui.setupUi(self) self.ui.connectPushButton.clicked.connect(self.accept) self.server=None @Slot() def accept(self): try: # Do some operation that involves potentially erroneous user input except Exception as e: # Display the exception thrown in a QMessageBox # The type of the exception is the title of the QMessageBox # The message of the exception is the message of the QMessageBox QMessageBox.about(self, type(e).__name__, str(e)) return super().accept() References https://en.wikipedia.org/wiki/Exception_handling#Exception_support_in_programming_languages https://docs.python.org/3/library/exceptions.html#bltin-exceptions","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"}],"tags":[]},{"title":"Using ffmpeg to Manipulate Video Files","slug":"Using-ffmpeg-to-Manipulate-Video-Files","date":"2023-02-18T08:00:00.000Z","updated":"2023-02-18T22:56:52.940Z","comments":true,"path":"2023/02/18/Using-ffmpeg-to-Manipulate-Video-Files/","link":"","permalink":"https://abbaswu.github.io/2023/02/18/Using-ffmpeg-to-Manipulate-Video-Files/","excerpt":"","text":"FFmpeg is a collection of libraries and tools for manipulating video, audio, and other multimedia files and streams. It is frequently used for basic editing (cutting and joining), video scaling, post-production video effects, and standard compliance (SMPTE, ITU). The main component of FFmpeg is the command-line tool ffmpeg, which reads an input file, applies transformations, and writes to an output file. The basic usage pattern of ffmpeg is ffmpeg -i &lt;input file&gt; [options] &lt;output file&gt;. ffmpeg automatically selects the decoder and encoder based on the extension of &lt;input file&gt; and &lt;output file&gt;, and we specify what transformations we apply to the original video file in [options]. We now explain what to write in [options] to address typical video manipulation demands. Cut a VideoAdd the option -ss &lt;start time in hours&gt;:&lt;start time in minutes&gt;:&lt;start time in seconds&gt; -to &lt;end time in hours&gt;:&lt;end time in minutes&gt;:&lt;end time in seconds&gt;. If there is no need to transcode the video format or apply other transformations, add -c:v copy -c:a copy for increased speed. For example, ffmpeg -i input.mp4 -ss 00:05:10 -to 00:15:30 -c:v copy -c:a copy output.mp4 saves input.mp4 from 00:05:10 to 00:15:30 to output.mp4 with no transcoding or other transformations, resulting in a 10 minutes and 20 seconds video. Change Frame RateAdd the option -r &lt;frame rate&gt;. For example, ffmpeg -i input.mp4 -r 12 output.gif controls the frame rate of output.gif generated by transcoding to 12FPS. Change Output ResolutionAdd the option -s &lt;length&gt; x &lt;width&gt;. The units of &lt;length&gt; and &lt;width&gt; are in pixels. It is usually required that the length and width be scaled equally from the original length and width to avoid distortion of the picture. For example, ffmpeg -i input.mp4 -s 320x180 output.gif controls the resolution of the output.gif generated by transcoding to 320x180. Change Playback Speed To speed up the output video to k times its original size: add the option -filter:v &#39;setpts=PTS/&lt;k&gt;&#39;. To slow down the output video to k times the original: add the option -filter:v &#39;setpts=&lt;k&gt;*PTS&#39;. For example, ffmpeg -i input.mp4 -filter:v &#39;setpts=PTS/2&#39; output.gif (or ffmpeg -i input.mp4 -filter:v &#39;setpts=0.5*PTS&#39; output.gif) will transcode the resulting output.gif to speed up to 2 times the original. Extract AudioBefore we extract audio, we need to first look up media information using ffprobe, a command-line tool installed with ffmpeg: 12345678910$ ffprobe video.mkv...Input #0, matroska,webm, from &#x27;video.mkv&#x27;: Metadata: encoder : no_variable_data creation_time : 1970-01-01T00:00:00.000000Z Duration: 00:23:30.07, start: 0.000000, bitrate: 1392 kb/s Stream #0:0: Audio: aac (LC), 48000 Hz, stereo, fltp (default) Metadata:... We can see that the format of the audio stream is aac. Now, we can add the option -map 0:a -acodec copy to copy the audio stream. Note that the extension of output file should correspond with the format of the audio stream. For example, we run the following command to save the audio stream of video.mkv to audio.mp4: ffmpeg -i video.mkv -map 0:a -acodec copy audio.mp4. Note that .mp4 is a valid extensions of aac audio streams. References https://en.wikipedia.org/wiki/FFmpeg https://shotstack.io/learn/use-ffmpeg-to-trim-video/ https://homehack.nl/create-animated-gifs-from-mp4-with-ffmpeg/ https://superuser.com/questions/1261678/how-do-i-speed-up-a-video-by-60x-in-ffmpeg/1261681 https://www.baeldung.com/linux/ffmpeg-audio-from-video","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Unix","slug":"Code/Unix","permalink":"https://abbaswu.github.io/categories/Code/Unix/"}],"tags":[]},{"title":"Syncing a Local Directory With a Remote Directory via FTP","slug":"Syncing-a-Local-Directory-With-a-Remote-Directory-via-FTP","date":"2023-02-16T08:00:00.000Z","updated":"2023-07-11T05:05:09.246Z","comments":true,"path":"2023/02/16/Syncing-a-Local-Directory-With-a-Remote-Directory-via-FTP/","link":"","permalink":"https://abbaswu.github.io/2023/02/16/Syncing-a-Local-Directory-With-a-Remote-Directory-via-FTP/","excerpt":"","text":"Syncing a Local Directory With a Remote Directory via FTPOverviewWe can use the following Sequence Diagram to depict the process of modifying the Files within a Local Directory and a Remote Directory before syncing the two Directories via the File Transfer Protocol (FTP), a standard communication protocol used on computer networks to send files between a Client and a Server. sequenceDiagram loop Machine With the FTP Client -&gt;&gt; Machine With the FTP Client: Modify Files Machine With the FTP Server -&gt;&gt; Machine With the FTP Server: Modify Files Machine With the FTP Server -&gt;&gt; Machine With the FTP Server: Start FTP Server Machine With the FTP Client -&gt;&gt; Machine With the FTP Client: Start FTP Client Machine With the FTP Client -&gt;&gt; Machine With the FTP Server: Connect to FTP Server Machine With the FTP Client -&gt;&gt; Machine With the FTP Server: Mirror Remote Directory -&gt; Local Directory Machine With the FTP Server --&gt;&gt; Machine With the FTP Client: Update Local Directory With Changed Files in Remote Directory Machine With the FTP Client -&gt;&gt; Machine With the FTP Server: Reverse Mirror Local Directory -&gt; Remote Directory Machine With the FTP Client --&gt;&gt; Machine With the FTP Server: Update Remote Directory With Changed Files in Local Directory end Setting Up the Machine With the FTP ClientObviously, we need to install an FTP Client on this Machine. Personally, I recommend installing lftp, a “sophisticated” file transfer program. Unlike a standard FTP Client, which only enables you to upload or download files, lftp additionally enables you to maintain file synchronisation using its built-in mirror command. Setting Up the Machine With the FTP ServerWe would also need an FTP Server on the other Machine. Although there are many existing UNIX FTP servers, such as proftpd and vsftpd, they are usually tricky to compile, configure, and set up and require Root Privileges to run, which is tedious, if not impossible, in many situations. As an alternative, we write our own FTP Server using pyftpdlib, a pure Python FTP server library written which offers a high-level interface to creating portable and efficient FTP servers. Such a solution requires us to have a Python environment running on the Machine With the FTP Server and install pyftpdlib, which is very simple in today’s world where the Python ecosystem is ubiquitous. After setting up a Python environment and installing pyftpdlib, we can write a script for an FTP server. Below is the script that I am using. By default, it sets up a user user with password 12345 and listens on port 2121 of the Machine With the FTP Server’s Outbound IP Address, but these settings can all be tweaked by providing command-line arguments. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#!/usr/bin/env python3import argparseimport osimport socketfrom pyftpdlib.authorizers import DummyAuthorizerfrom pyftpdlib.handlers import FTPHandlerfrom pyftpdlib.servers import FTPServerdef get_outbound_ip_address(): s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) try: # doesn&#x27;t even have to be reachable s.connect((&#x27;10.255.255.255&#x27;, 1)) IP = s.getsockname()[0] except Exception: IP = &#x27;127.0.0.1&#x27; finally: s.close() return IPdef parse_command_line_arguments(): parser = argparse.ArgumentParser() parser.add_argument(&#x27;--user&#x27;, type=str, required=False, default=&#x27;user&#x27;, help=&#x27;Define a user having full read/write permissions&#x27;) parser.add_argument(&#x27;--password&#x27;, type=str, required=False, default=&#x27;12345&#x27;, help=&#x27;The password of the user having full read/write permissions&#x27;) parser.add_argument(&#x27;--anonymous&#x27;, action=&#x27;store_true&#x27;, required=False, default=False, help=&#x27;Add read-only anonymous user&#x27;) parser.add_argument(&#x27;--root&#x27;, type=str, required=False, default=os.getcwd(), help=&#x27;Root directory in FTP server&#x27;) parser.add_argument(&#x27;--host&#x27;, type=str, required=False, default=get_outbound_ip_address(), help=&#x27;Host to listen on&#x27;) parser.add_argument(&#x27;--port&#x27;, type=int, required=False, default=2121, help=&#x27;Port to listen on&#x27;) args = parser.parse_args() return args.user, args.password, args.anonymous, args.root, args.host, args.portdef main(): # Parse command line arguments user, password, anonymous, root, host, port = parse_command_line_arguments() # Instantiate a dummy authorizer for managing &#x27;virtual&#x27; users authorizer = DummyAuthorizer() # Define a new user having full r/w permissions authorizer.add_user(user, password, root, perm=&#x27;elradfmwMT&#x27;) # Add anonymous user if anonymous: authorizer.add_anonymous(root) # Instantiate FTP handler class handler = FTPHandler handler.authorizer = authorizer # Define a customized banner (string returned when client connects) handler.banner = &quot;pyftpdlib based ftpd ready.&quot; # Instantiate FTP server class and listen on &lt;host&gt;:&lt;port&gt; server = FTPServer((host, port), handler) # Start FTP server server.serve_forever()if __name__ == &#x27;__main__&#x27;: main() Save this to a file, and run chmod +x on the file to make it executable. 12345678910111213141516abbas@abbas-ThinkPad-X1-Carbon-Gen-9:~$ ./pyftpd -husage: pyftpd [-h] [--user USER] [--password PASSWORD] [--anonymous] [--root ROOT] [--host HOST] [--port PORT]optional arguments: -h, --help show this help message and exit --user USER Define a user having full read/write permissions --password PASSWORD The password of the user having full read/write permissions --anonymous Add read-only anonymous user --root ROOT Root directory in FTP server --host HOST Host to listen on --port PORT Port to listen onabbas@abbas-ThinkPad-X1-Carbon-Gen-9:~$ ./pyftpd [I 2023-02-16 15:47:39] &gt;&gt;&gt; starting FTP server on 10.43.111.144:2121, pid=11969 &lt;&lt;&lt;[I 2023-02-16 15:47:39] concurrency model: async[I 2023-02-16 15:47:39] masquerade (NAT) address: None[I 2023-02-16 15:47:39] passive ports: None DemonstrationLast but not least, we will present a demonstration of syncing a Local Directory With a Remote Directory via FTP. Our Local Directory is a directory named mirror_ubuntu with two files mirror_ubuntu_1.txt and mirror_ubuntu_2.txt on the Machine With the FTP Client. Our Remote Directory is a directory named mirror_ipad with two files mirror_ipad_1.txt and mirror_ipad_2.txt on the Machine With the FTP Server, which is the iSH app running within an iPad. We start the FTP Server on the Machine With the FTP Server, and start the FTP Client on the Machine With the FTP Client. As depicted in Overview, we first Mirror Remote Directory to Local Directory, which can be accomplished by running mirror --continue --no-perms &lt;Remote Directory&gt; &lt;Local Directory&gt; within lftp. The Local Directory will now contain Files that were modified the Remote Directory. Afterwards, we Reverse Mirror Local Directory to Remote Directory, which can be accomplished by running mirror --continue --no-perms --reverse &lt;Local Directory&gt; &lt;Remote Directory&gt; within lftp. The Remote Directory will now contain Files that were modified the Local Directory. At this point, the Local Directory has been successfully synced with the Remote Directory. References https://www.geekbitzone.com/posts/lftp/lftp-mirror-remote-folders/ https://github.com/giampaolo/pyftpdlib","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"}],"tags":[]},{"title":"Paper Review: \"Types and Programming Languages\" Chapter 15 and Chapter 16","slug":"Paper-Review-Types-and-Programming-Languages-Chapter-15-and-Chapter-16","date":"2023-02-10T08:00:00.000Z","updated":"2023-04-11T21:56:48.795Z","comments":true,"path":"2023/02/10/Paper-Review-Types-and-Programming-Languages-Chapter-15-and-Chapter-16/","link":"","permalink":"https://abbaswu.github.io/2023/02/10/Paper-Review-Types-and-Programming-Languages-Chapter-15-and-Chapter-16/","excerpt":"","text":"SummaryChapter 15, “Subtyping,” describes adding Subtyping with Functions and Records into Simply Typed Lambda Calculus. It formalizes the Subtype Relation as a collection of Inference Rules, verifies that verify that the Preservation and Progress Theorems of Simply Typed Lambda Calculus still apply, examines Ascription (or Casting) in the context of Subtyping, and proposes Subtyping Rules for Variants, Lists, References, and Arrays. Finally, it presents alternative Coercion Semantics for Subtyping. Chapter 16, “Metatheory of Subtyping,” observes that the Subtyping Rules presented in the previous chapter are not syntax-directed and have overlapping conclusions, which impedes implementing a Typechecking Algorithm, and develops the Algorithmic Subtype Relation and the Algorithmic Typing Relation to address these problems. CritiqueAcquired InsightsI will first summarize the insights that I gained while reading these Chapters. An empty Bottom Type is useful, both as a way of expressing that a Function is not intended to return and telling the Typechecker that the Term can be associated with any Type. Implementing Ascription (Casting) in Subtyping is non-trivial, especially for Downcasting. As blindly following Type Assertions may lead to potentially serious consequences, the Compiler would need to insert a Runtime Type Check, essentially adding the Machinery for Typechecking to the Runtime System. This might incur a significant performance overhead. Different from an Inheritance Based Class Hierarchy, which is a physical relationship between Types, Subtyping generally is more of a logical relationship between Types. For example, in the alternative Coercion Semantics for Subtyping, we can consider that int and float, two Types that do not inherit from one another, have a Subtyping Relation, as they can be converted to one another. In this case, the Subtyping Relation is compiled to Coercions at runtime (instructions physically converting an int to a float, or vice versa), which are much more efficient than virtual function calls frequently seen in an Inheritance Based Class Hierarchy. Background KnowledgeThere is no doubt that the Chapters are written in great detail. However, I find some of the content, especially the terminology, a little difficult to understand, and I have looked into background knowledge concerning the topic. Below summarizes what I have read. PolymorphismPolymorphism describes that a single Interface can work with Terms of Different Types in Programming Languages. There are different kinds of Polymorphism in the context of Programming Languages, including: Parametric PolymorphismAlso known as “Generic Programming”. Using Abstract Symbols that can substitute for any Type instead of specifying Concrete Types in Interfaces. C++’s Template Metaprogramming comes close to Parametric Polymorphism (except for Template Specializations). Ad Hoc PolymorphismDefining a Common Interface for a Set of Individually Specified Types. Includes Function Overloading, Operator Overloading, and C++’s Template Metaprogramming with Template Specializations. SubtypingIt is a form of Polymorphism in which the Terms of a Subtype T, which is related to another Type known as the Supertype T&#39; in some way, can be safely used in any Context where the Terms of T&#39; are used. The Concept of Subtyping has gained visibility with the advent of Object Oriented Programming Languages, where it is frequently the case that an Inheritance Based Class Hierarchy forms the basis of Subtyping, and such Safe Substitution is known as the Liskov Substitution Principle. However, stepping out of this specific and widely known context, there are several different Schemes of Subtyping. They can be broadly classified along two dimensions: Nominal Subtyping vs. Structural Subtyping and Inclusive Implementations vs. Coercive Implementations. Nominal Subtyping requires the Subtyping Relation to be explicitly declared among the two Types. This is the case with the Subtyping based on an Inheritance Based Class Hierarchy frequently encountered in Object Oriented Programming Languages. In contrast, in Structural Subtyping, a Type T is implicitly the Subtype of another Type T&#39; if Terms of T has all the Properties of Terms of T&#39; and can handle all the Messages Terms of T&#39; can handle. This is closely related to Row Polymorphism or the so-called Duck Typing in Dynamically Typed Programming Languages. On another dimension, Implementations of Subtyping can be divided into Inclusive Implementations and Coercive Implementations. In Inclusive Implementations, any Term of a Subtype, left unchanged, is automatically a Term of a Supertype. This is often the case with the Subtyping based on an Inheritance Based Class Hierarchy frequently encountered in Object Oriented Programming Languages. A Term can have multiple Types in this situation. In contrast, Coercive Implementations are defined by Type Conversion Functions from Subtype to Supertype and allow a Term of a Subtype to be converted to a Term of a Supertype, such as the case for int‘s, float‘s, and str‘s. It is also worth noticing that applying the Type Coercion Function from A to B and then from B to C might have a different result from directly applying the Type Coercion Function from A to C. For example, str(float(2)) returns a value different from str(2). Based on the concept of Subtyping, the concept of Variance reference to how the Subtyping Relations between more complex Types relates to the Subtyping Relations between the simpler Types they include. For example, given that Cat is a Subtype of Animal, should a List of Cat‘s be a Subtype of a List of Animal‘s? What about a Function that takes a Term of Type Cat as an Arugument and a Function that takes a Term of Type Animal as an Arugument? Different Programming Languages have different implementations, but most Programming Languages respect the following patterns. If the Complex Types are Read Only and&#x2F;or capable of returning Terms of the Simple Types, they should have the same Subtyping Relations as the Simple Types. This is known as Covariance. For example, A read-only List of Cat‘s can be used whenever a read-only List of Animal‘s is required, as each Term read from the read-only List of Cat‘s is of Type Cat, which is a Subtype of Animal. In other words, const List&lt;Cat&gt; is a Subtype of const List&lt;Animal&gt;. It is not safe to use a const List&lt;Animal&gt; where a const List&lt;Cat&gt; is required, as a Term read from a const List&lt;Animal&gt; may not be of Type Cat. In other words, const List&lt;Animal&gt; is not a Subtype of const List&lt;Cat&gt;. If the Complex Types are Write Only and&#x2F;or capable of accepting Terms of the Simple Types as Parameters, they should have the opposite Subtyping Relations as the Simple Types. This is known as Contravariance. For example, A Function that takes a Term of Type Animal as a Parameter may be used where a Function that takes a Term of Type Cat as a Parameter is used, as each Term of Type Cat can also be passed as a Parameter of Type Animal. In other words, Animal -&gt; T is a Subtype of Cat -&gt; T. It is not safe to use a Cat -&gt; T where an Animal -&gt; T is required, as a Term of Type Animal may not be passed as a Parameter of Type Cat. In other words, Cat -&gt; T is not a Subtype of Animal -&gt; T. If the Complex Types are Read&#x2F;Write, they should have no Subtying Relations. This is known as Invariance. For example, A Term written into a List&lt;Animal&gt; need not be of Type Cat, but a Term written into a (non-constant) List&lt;Cat&gt; must be of Type Cat. Thus, it is not safe to use a List&lt;Cat&gt; where a List&lt;Animal&gt; is required. In other words, List&lt;Cat&gt; is not a Subtype of List&lt;Animal&gt;. A Term read from a (non-constant) List&lt;Animal&gt; may not be of Type Cat. Thus it is not safe to use a List&lt;Animal&gt; where a List&lt;Cat&gt;is required. In other words, List&lt;Animal&gt; is not a Subtype of List&lt;Cat&gt;. References https://en.wikipedia.org/wiki/Polymorphism_(computer_science) https://stackoverflow.com/questions/36948205/why-is-c-said-not-to-support-parametric-polymorphism https://en.wikipedia.org/wiki/Subtyping https://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science) Having acquired such Background Knowledge, I will also summarize the insights that I gained while reading these Chapters. Acquired InsightsAn empty Bottom Type is useful, both as a way of expressing that a Function is not intended to return and telling the Typechecker that the Term can be associated with any Type. Implementing Ascription (Casting) in Subtyping is non-trivial, especially for Downcasting. As blindly following Type Assertions may lead to potentially serious consequences, the Compiler would need to insert a Runtime Type Check, essentially adding the Machinery for Typechecking to the Runtime System. This might incur a significant performance overhead. Different from an Inheritance Based Class Hierarchy, which is a physical relationship between Types, Subtyping generally is more of a logical relationship between Types. For example, in the alternative Coercion Semantics for Subtyping, we can consider that int and float, two Types that do not inherit from one another, have a Subtyping Relation, as they can be converted to one another. In this case, the Subtyping Relation is compiled to Coercions at runtime (instructions physically converting an int to a float, or vice versa), which are much more efficient than virtual function calls frequently seen in an Inheritance Based Class Hierarchy.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Review: \"Types and Programming Languages\" Chapter 13 and Chapter 14","slug":"Paper-Review-Types-and-Programming-Languages-Chapter-13-and-Chapter-14","date":"2023-02-05T08:00:00.000Z","updated":"2023-02-05T19:48:55.389Z","comments":true,"path":"2023/02/05/Paper-Review-Types-and-Programming-Languages-Chapter-13-and-Chapter-14/","link":"","permalink":"https://abbaswu.github.io/2023/02/05/Paper-Review-Types-and-Programming-Languages-Chapter-13-and-Chapter-14/","excerpt":"","text":"SummaryChapters 13 and 14 of “Types and Programming Languages” discuss adding Impure Features, also known as Computational Effects, into Simply Typed Lambda Calculus. Specifically, Chapter 13 discusses adding References to Mutable Cells that can be Allocated, Dereferenced, and Assigned and formalizes their Operational Behavior. Chapter 14 gradually adds Raising and Handling Exceptions, starting from a Term error of any Type that completely aborts Evaluation when applied as a Function or passed as an Argument to a Function, before supporting Exception Handling, as well as Raising a Value (potentially containing information about what unusual thing happened) as an Exception. CritiqueOverall, I believe these two Chapters are written very well, as they progressively add realistic features to Simply Typed Lambda Calculus. I will summarize takeaways from this paper before presenting some questions and comments. Takeaways From This PaperReferences to Mutable CellsThe Formalization of the Operational Behavior of References to Mutable Cells encompasses Allocations (providing an initial value to a Mutable Cell), Dereferences (reading the current value of the referenced Cell), and Assignments (changing the value stored in the referenced Cell), but not Deallocations. Explicit Deallocations lead to the Dangling Reference Problem, which undermines Type Safety. Instead, References to Mutable Cells that are no longer needed should be Garbage Collected. An interpretation of how Aliasing makes Program Analysis tricky is that Aliasing essentially sets up “Implicit Communication Channels in the form of Shared State” between different parts of a Program. To formalize the Operational Behavior of References to Mutable Cells, we can consider a Reference $l \\in L$, where $L$ is the set of Locations of the Program’s Store (a.k.a. Heap Memory) $\\mu$. As the result of Evaluating an Expression depends on the current contents of the Store and may cause Side Effects for the Store, Evaluation Rules should, in addition to Terms and Types, take the Store as an Argument and return a new Store as part of the result of Evaluating an Expression. Furthermore, in a naive implementation of Typing Rules for References to Mutable Cells, the Type of the Reference depends on the Type of the Mutable Cell, e.g., $\\frac{\\Gamma \\vdash \\mu(l): T}{\\Gamma \\vdash l: \\text{Ref} : T}$. However, this is inefficient where there are multiple levels of Indirection and is problematic where there are Cyclic References. To solve this problem, the Chapter proposes extending Typing Rules with a Store Typing $\\Sigma$, which maps every Location $l \\in L$ to a fixed, definite Type. In this case, the Typing Rule is written as $\\frac{\\Gamma | \\Sigma \\vdash \\Sigma(l): T}{\\Gamma | \\Sigma \\vdash l: \\text{Ref} : T}$. Raising and Handling ExceptionsThe first (and most straightforward) Approach to Raising and Handling Exceptions, a Term error that completely aborts Evaluation when applied as a Function or passed as an Argument to a Function, effectively simulates Unwinding the Call Stack when it propagates error to the top level. The final approach that supports both Exception Handling and Raising a Value as an Exception considers an Exception to be a Value $t_{exp}$ of Type $T_{exp}$ (instead of a Term error). It proposes a Term Constructor raise t_&#123;exp&#125; that describes Raising a Value as an Exception, and models Exception Handling with try t_1 with t_2: T_1, in which $t_1: T_1$ and $t_2: T_{exp} \\rightarrow T_1$ (i.e., $t_2$ is a function, called when an Exception is Raised, taking a Raised Exception as Input and Returning a Value of the same Type as $t_1$ as Output). Questions and CommentsAfter reading these two Chapters, the power of Functions as a Universal Abstraction has left a deep impression on me. For example: Arrays containing Terms of Type $T$ can be modeled as References to Functions of type $Nat \\rightarrow T$. The Referenced Function looks up the Element given an Index. Exception Handling is modeled with try t_1 with t_2, in which $t_2$ is a function called when an Exception is Raised, taking a Raised Exception as Input and Returning a Value of the same Type as $t_1$ as Output). This describes complex Side Effects in a realistic Programming Language in a Side Effect Free manner that is clean and easy to reason about while not sacrificing Expressiveness. Are there any other complex Side Effects that can be modeled like this using Functions?","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Review: Bi-directional type checking","slug":"Paper-Review-Bi-directional-type-checking","date":"2023-01-30T08:00:00.000Z","updated":"2023-02-01T20:51:56.304Z","comments":true,"path":"2023/01/30/Paper-Review-Bi-directional-type-checking/","link":"","permalink":"https://abbaswu.github.io/2023/01/30/Paper-Review-Bi-directional-type-checking/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Type Systems. The original paper can be found here. SummaryThe paper first explains that except Syntax Directed Systems, Typing Rules cannot be directly translated into Algorithms for Type Checking and Type Inference. It presents a motivating example of this using a Simply Typed Lambda Calculus having Bool and Function as Types and Bool Constants, Variables, Function Abstractions, Function Applications, and Conditional Expressions as Terms, in which the Typing Rule for Function Abstractions cannot be directly translated into a Function for Type Inference. It then presents Bidirectional Typing as a remedy to this problem. It explains what Bidirectional Typing is, discusses its advantage, and adds Bidirectional Typing into the previously presented Simply Typed Lambda Calculus, presenting how Bidirectional Typing works during the process. Finally, it discusses the limitations of Bidirectional Typing and presents academic literature for further reading. CritiqueOverall, I believe this paper is written very well, as I can grasp most of it after reading it. I will summarize my takeaways from this paper before presenting some questions and comments. My Takeaways From This PaperWhat Bidirectional Typing IsBidirectional Typing splits each Typing Rule $\\Gamma \\vdash t: T$ into: An Inference Rule $\\Gamma \\vdash t \\Rightarrow T$, which infers $t$’s type to be $T$ in Context $\\Gamma$. A Type Checking Rule $\\Gamma \\vdash t \\Leftarrow T$, which checks $t$’s type to be $T$ in Context $\\Gamma$. The Inference Rules and Type Checking Rules would work together and call each other. Advantages of Bidirectional Typing Makes general Typing Rules more Syntax Directed, thus, simplifying implementing Algorithms for Type Checking and Type Inference. Requires relatively few additional Type Annotations. Produces good error messages that report where the error occurs. Limitations of Bidirectional Typing Variables in a Derivation can no longer be replaced by the Derivation for a Term of the same Type. This is because Bidirectional Typing uses Inference Mode to check Variables but uses Checking Mode to check many other Terms. In some situations, explicit Type Annotations may need to be written within complex Terms, such as a direct Application of a Function Abstraction, like (λ b . if b then false else true) true: Bool Questions and Comments Page 8 mentions, “remember that the derivation, like the bidirectional typing rules, should be read bottom-to-top and left-to-right.” However, Inference Rules have the form of $\\frac{Premise}{Conclusion}$. So, why should the derivation be read from Conclusion to Premise? What are the meanings of the small-step rule $\\frac{}{t : T \\rightarrow t}$ and the large-step rule $\\frac{t \\Downarrow t’}{t : T \\Downarrow t’}$ on Page 8? I believe explicit Type Annotations should be enforced for the Parameters within Function Abstractions, such as (λ b: Bool . if b then false else true) instead of (λ b . if b then false else true). This aligns with real-world programming languages (C++, Java, Rust, Swift, Haskell, etc.) This increases readability. This simplifies both the Typing Rules and the Inference Rules and Type Checking Rules of Bidirectional Typing. Feedback from the Class DiscussionSmall Step Semantics, represented using $\\rightarrow$’s, depict one step in Evaluation. For example, if $e$ is $true$ itself, $\\text{if}: e : \\text{then} : e_1 : \\text{else} : e_2$ can be Evaluated in one step to $e_1$. This can be represented using $\\frac{e \\rightarrow true}{\\text{if}: e : \\text{then} : e_1 : \\text{else} : e_2 \\rightarrow e_1}$ Big Step Semantics, represented using $\\Downarrow$’s, depict Reducing a Subexpression to a Value through several Small Steps. For example, if $e$ is a Subexpression that can be Reduced to $true$ after several Small Steps, $\\text{if}: e : \\text{then} : e_1 : \\text{else} : e_2$ can be Reduced to $e_1$ after several Small Steps. This can be represented using $\\frac{e \\Downarrow true}{\\text{if}: e : \\text{then} : e_1 : \\text{else} : e_2 \\Downarrow e_1}$. Syntax Directed means a one-to-one correspondence between the Type of the Term and the Syntax (Derivation of the Grammar Rules) of the Term. There is no precise definition for Bidirectional Typing. Instead, Bidirectional Typing points a direction toward implementing a Type Inference&#x2F;Type Checking Algorithm. In Bidirectional Typing, we prefer to start from Inference Mode because if we can Infer the Type of a Term, we can Check the Type of the Term, while Checking falls back on Inference. Why is there only a Checking Rule and no Inference Rule for if t then t else t? This gives better error messages. Should the Terms in the then branch and the else branch have different types, it is possible to give an error message directly stating this information. If an Inference Rule had been proposed instead, it would blame one branch for having a wrong type, which may be confusing and go against programmer intent. We can read Typing Rules either from top to bottom or from bottom to top, with slightly different interpretations. Reading from top to bottom describes how to use the information for Type Checking. Reading from bottom to top describes how a Type Inference Algorithm works, e.g., what needs to be Checked to Infer the Type of a Term. From a historical perspective, there are two Design Philosophies for Type Systems. The first is to augment a Programming Language with more information, such as C which uses it to determine how much space a variable would take up in memory. The second is to express programmer intent. Type Annotations (Ascriptions) for Parameters are required for Functions that are not immediately used, such as Top Level Functions. However, it is helpful to omit Type Annotations (Ascriptions) for Parameters for immediately used Lambda Terms within Higher Order Functions.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Paper Review: \"Types and Programming Languages\" Chapter 9 and Chapter 11","slug":"Paper-Review-Types-and-Programming-Languages-Chapter-9-and-Chapter-11","date":"2023-01-25T08:00:00.000Z","updated":"2023-02-05T04:06:45.613Z","comments":true,"path":"2023/01/25/Paper-Review-Types-and-Programming-Languages-Chapter-9-and-Chapter-11/","link":"","permalink":"https://abbaswu.github.io/2023/01/25/Paper-Review-Types-and-Programming-Languages-Chapter-9-and-Chapter-11/","excerpt":"","text":"SummaryChapter 9 of “Types and Programming Languages” presents the simply typed lambda calculus, which constructs a type system for pure lambda calculus, explaining theoretical aspects such as the typing relation and the Curry-Howard Correspondence along the way. Chapter 11 introduces simple extensions to the simply typed lambda calculus presented in Chapter 9, such as base types, derived forms, type ascriptions, let bindings, and some compound data structures (pairs, tuples, records, sums, variants, and lists), making it better resemble a real-world programming language. CritiqueForewordI have found the textbook hard to follow in many places. Thus, I have followed the textbook and looked into many online resources to grasp the content. Below summarizes my understanding after studying the material. Basic Concepts in Type TheoryTerms and TypesIn Type Theory, every Term has a Type, often written together as &lt;Term&gt;: &lt;Type&gt;. Types include Natural Numbers (nat) and Boolean Logic Values (bool). For example (assuming x: nat and y: nat): 0: nat x: nat 1 + 1: nat x + y: nat true: bool x + y: nat FunctionsFunctions are also Terms with Types, represented as Lambda Terms. A Lambda Term looks like (λ &lt;First Parameter Name&gt;: &lt;First Parameter Type&gt; &lt;Second Parameter Name&gt;: &lt;Second Parameter Type&gt; ... . &lt;Term to Return&gt;). It has type &lt;First Parameter Type&gt; → &lt;Second Parameter Type&gt; → ... → &lt;Type of Term to Return&gt;. This indicates that the Lambda Term is a function that takes Parameters of &lt;First Parameter Type&gt;, &lt;Second Parameter Type&gt;, etc., and returns a Term of &lt;Type of Term to Return&gt;. Examples of Lambda Terms: (λ x: nat . (x + x)): nat → nat: a Function which takes in a Parameter x of Type nat and returns the doubled Parameter. (λ x: nat y: nat . (x + y)): nat → nat → nat: a Function which takes in two Parameters x, y all of Type nat and returns their sum. A Lambda Term is often called an Anonymous Function because it has no Name. We can use the notion to give a Name to a Lambda Term: add: nat → nat → nat ::= (λ x: nat y: nat . (x + y)) Function ApplicationsIn Type Theory, a Function Call is called a Function Application, which “takes a Term of a Type and results in a Term of another Type.” Function Application is written as &lt;Function&gt; &lt;Argument&gt; &lt;Argument&gt; ... (akin to Function Calls in Haskell and Commands in Unix Shell) instead of the conventional &lt;Function&gt;(&lt;Argument&gt;, &lt;Argument&gt;, ...) in Programming Languages. If we define a Function add that takes two nat‘s and returns a nat, the following are valid Terms: add 0 0: nat add 2 3: nat add 1 (add 1 (add 1 0)): nat Dependent TypingSometimes, the Type returned by a Function depends on the Value of its Argument. This is known as Dependent Typing. For example, a function if takes three arguments, with if true b c returning b, and if false b c returning c. If b and c have different Types, then the type of if depends on the value of a. Dependent Typing is a reasonably complicated subject that is an active domain of research. Zero Type, Unit Type, and Universal TypeZero TypeIn some programming languages, there is a Zero Type or Bottom Type - a Type whose Set of Terms is the empty set and a Subtype of all other Types. In these programming languages, denoting the Zero Type as a Function’s Return Type frequently indicates that the Function never returns (never completes computation) - instead, it may loop forever, throw an exception, or terminate the process. As a real-world example, in Rust, the Zero Type is called the Never Type and is denoted by !. It is the kind of calculation that never returns any result. For example, the exit function fn exit(code: i32) -&gt; ! terminates the process without returning. Unit TypeIn some programming languages, the Unit Type is a Type whose Set of Terms is a singleton set, i.e., the type allows only one value. It is typically used to describe the Argument Type of a Function that doesn’t need arguments or the Return Type of a Function whose only goal is to have a side effect. For example: In Haskell, Rust, and Elm, the Unit Type is the Type of the 0-tuple (). In Python, the Unit Type is NoneType, which only has a single instance None. In JavaScript, both Null (which only has a single instance null) and Undefined (which only has a single instance undefined) are Unit Types. In languages such as C, C++, Java, and C#, void, which designates that a Function accepts no Arguments or does not return anything, plays a similar role to the Unit Type. However, there are also key differences: There are no Terms (Instances) of void. A proper Unit Type may always be the Type of an Argument to a Function, but void cannot be the Type of an Argument. Universal TypeMost object-oriented programming languages include a universal base class. In Type Theory, this is known as a Universal Type or a Top Type. Its Set of Terms encompasses any valid Term in the programming language, and all other types in the programming language are subtypes. For example: Object in Smalltalk and JavaScript java.lang.Object in Java System.Object in C#, Visual Basic .NET, and other .NET Framework languages object in Python (can also be type-annotated as typing.Any) Any in Scala and Julia Some object-oriented programming languages, such as C++, Objective-C, and Swift, do not have a universal base class. In these languages, some constructs function similarly to the Universal Type. In C++, void * can accept any non-function pointer (even though void itself is more akin to the Unit Type). In Objective-C, id can accept pointers to any object. In Swift, the protocol Any can accept any type. Languages that are not object-oriented usually do not have a Universal Type. Typing ContextA Typing Context (or Typing Environment) $\\Gamma$ is a Mapping from Terms to Types (or a collection of Term - Type Pairs). The judgement $\\Gamma \\vdash e: \\tau$ is read as “$e$ has type $\\tau$ in Context $\\Gamma$”. In Statically Typed Programming Languages, these Typing Contexts are used and maintained by Typing Rules to Type Check a given Program or Expression. Type InhabitationGiven a Typing Environment, a Type is inhabitated if an existing Term of the Type is available or a Term of the Type can be readily obtained (i.e., via Function Application). Derived FormsIn Type Theory, Syntactic Sugar is known as Derived Forms, while replacing a Derived Form with its lower-level definition (usually during compile time) is known as desugaring. For example: In C, a[i] and *(a + 1), a-&gt;x and (*a).x. In the tidyverse collection of R packages, x %&gt;% f(y) is equivalent to f(x, y). A programming language is typically divided into a compact core language, a rich set of syntax defined in terms of that core (Derived Forms), and a comprehensive standard library. This makes the language maintainable for engineers while making it convenient for users. Type AscriptionType Ascription is an assertion within source code that a term has a particular type. This can lead to cleaner, easier-to-understand code documentation. Important Derived Forms Tuple Record (Struct, Rows in a Database) - a collection of Fields, possibly of different Types Variant (Datatype, Tagged Union, Discriminated Union, Disjoint Union) A data structure to hold a Term that could take on “several different, but fixed Types.” Contains a Value field and a Tag field Widely used for defining recursive data structures (e.g. Trees containing Leaves and Internal Nodes) List Curry-Howard CorrespondenceThe Curry-Howard Correspondence, independently discovered by logicians Haskell Curry in 1958 and William Howard in 1969, states that “proofs in a given subset of mathematics are exactly programs from a particular programming language”. Specifically, Types correspond to logical formulas. A Term having a Type can be understood as evidence that the Type is inhabited. For example, 3110: int is evidence that int is inhabited. Logical Atoms $a$, $b$ correspond to whether Types A, B are inhabited. true corresponds to a Type that is always inhabited. The simplest of them all is the Unit Type. false corresponds to a Type that is never inhabited - the Zero Type. Conjunction $a \\land b$ corresponds to a Type inhabited when both Types A and B are inhabited - Tuple[A, B]. Disjunction $a \\lor b$ with the added condition that you know which one of $a$, $b$ is true when $a \\lor b$ is true corresponds to a Type that is inhabited when one of A, B is inhabited, and you know which one is inhabited - Variant[A, B]. Implication $a \\rightarrow b$ corresponds to a Type that, when inhibited, ensures B must be inhabited when A is inhabited - a Function Type, A -&gt; B. Programs correspond to proofs. Analyzing the types of expressions evaluated during the execution of a program corresponds to simplifying a proof. References https://en.wikipedia.org/wiki/Type_theory https://en.wikipedia.org/wiki/Bottom_type https://en.wikipedia.org/wiki/Typing_environment https://softwareengineering.stackexchange.com/questions/277197/is-there-a-reason-to-have-a-bottom-type-in-a-programming-language https://stackoverflow.com/questions/32505911/what-is-the-role-of-bottom-%E2%8A%A5-in-haskell-function-definitions https://doc.rust-lang.org/std/primitive.never.html https://en.wikipedia.org/wiki/Unit_type https://en.wikipedia.org/wiki/Top_type https://cs3110.github.io/textbook/chapters/adv/curry-howard.html#types-correspond-to-propositions https://wiki.haskell.org/Curry-Howard-Lambek_correspondence https://www.pédrot.fr/slides/inria-junior-02-15.pdf https://math.stackexchange.com/questions/2686280/what-do-logicians-mean-by-type https://homepages.inf.ed.ac.uk/stg/NOTES/node35.html https://cs.wellesley.edu/~cs251/s02/scheme-intro.pdf https://cs.brown.edu/~sk/Publications/Papers/Published/pk-resuarging-types/paper.pdf https://en.wikipedia.org/wiki/Syntactic_sugar https://www.wikidata.org/wiki/Q73072308 https://stackoverflow.com/questions/36389974/what-is-type-ascription https://github.com/rust-lang/rfcs/blob/master/text/0803-type-ascription.md https://medium.com/@andrew_lucker/things-you-cant-do-in-rust-type-ascription-5253951c7427 https://docs.scala-lang.org/style/types.html https://futhark-lang.org/examples/type-ascriptions.html https://en.wikipedia.org/wiki/Record_(computer_science) https://en.m.wikipedia.org/wiki/List_(abstract_data_type) Feedback from the Class DiscussionAn Introduction Rule describes how Elements of the Type can be Created, and is akin to a description of a Constructor. Similarly, an Elimination Rule describes how Elements of the Type can be used in an Expression, and is akin to a description of an Overloaded Operator. A lot of papers propose Typing Rules that don’t make much sense in isolation, but can be plugged into other Type Systems to add a Feature (i.e., allow the non-intrusive addition of other Typing Rules). Well-designed Type Systems provide guarantees on a program’s behavior (i.e., guarantee predictable runtime behavior). C introduced types, not for verification, but to determine how much space a variable would take up in memory. Uniqueness of Typing (i.e., a Term can only have one Type) doesn’t hold when there is Subtyping. Curry Style allows representing errors explicitly and describing the type of errors, which is suitable for languages where things can go wrong. In comparision, Church Style does not allow errors The Erasure Property is built upon the assumption that the Execution of the Program doesn’t rely on Types. Type Ascription woule be beneficial for giving hints to the Type Inference&#x2F;Type Checking Algorithm. Usually, Desugaring happens before Type Checking, as the Type System does not directly handle the Syntactic Sugar. Tuples are also called Sum Types, and Variants are also called Product Types. This is based on how many possible values the Tuple or Variant Type has. For example, std::pair&lt;char, bool&gt; has 256 * 2 = 512 values, std::variant&lt;char, bool&gt; has 256 + 2 = 258 values, and std::optional&lt;char&gt; has 256 + 1 = 257 values. Enums can be seen as Variants where each value is associated with the Unit Type. Tuples and Records are distinct Types because Compilers implement them differently Programming in Dynamically Typed Programming is akin to programming with variables which are Variants of all possible types.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"}],"tags":[]},{"title":"Modules Within PySide6 for Cross-platform GUI Building and System API Calling","slug":"Modules-Within-PySide6-for-Cross-platform-GUI-Building-and-System-API-Calling","date":"2023-01-02T08:00:00.000Z","updated":"2023-01-03T04:53:45.957Z","comments":true,"path":"2023/01/02/Modules-Within-PySide6-for-Cross-platform-GUI-Building-and-System-API-Calling/","link":"","permalink":"https://abbaswu.github.io/2023/01/02/Modules-Within-PySide6-for-Cross-platform-GUI-Building-and-System-API-Calling/","excerpt":"","text":"A Brief Introduction to QtQt is a C++ library with official Python bindings that allows creating GUIs and cross-platform applications targeting different software and hardware platforms, such as Linux, Windows, macOS, Android, or embedded systems, with little to no change to the underlying codebase, while still having native capabilities, speed, and “look and feel”. There are several advantages of using Qt, including: Well-designed object-oriented framework. Excellent documentation. Large user base. Widely endorsed. Many industrial-grade cross-platform software use Qt, including Autodesk Maya, Autodesk 3ds Max, Google Earth, Mathematica, OBS Studio, QGIS, Sibelius, Teamviewer, VirtualBox, VLC media player, Wireshark, and WPS Office. Modules Within PySide6To aid use, we have summarized the relevant modules of PySide6, Qt’s official Python bindings. PySide6.QtCore: Qt’s essential classes, including platform-independent command-line parsing, multithreading, date and time utilities, object serialization, etc. Most of these features have already been introduced into the C++ and Python standard libraries. However, we should pay attention to Qt’s event handling mechanism - a mechanism for emitting events in the form of objects across threads (known as signals), and handling these events in designated functions (known as slots). Modules Related to Building GUIsThe modules of PySide6 directly related to building GUIs are as follows: PySide6.QtGui: Classes used internally by Qt’s user interface technologies, including classes for windowing system integration, event handling, OpenGL and OpenGL ES integration, 2D graphics, basic imaging, fonts, and text. PySide6.QtWidgets: Provides a set of UI elements to create classic desktop-style user interfaces. All UI elements, including user-defined ones, inherit from PySide6.QtWidgets.QtWidget. PySide6.QtMultimediaWidgets: Provides multimedia-related widgets and controls. PySide6.QtOpenGL, PySide6.QtOpenGLWidgets: Used to support the OpenGL widget class, which operates similarly to other Qt widgets with the exception that it opens an OpenGL display buffer whose contents can be rendered using the OpenGL API. PySide6.QtPdf, PySide6.QtPdfWidgets: Classes for rendering pages from PDF documents. PySide6.QtWebEngineCore, PySide6.QtWebEngineWidgets: Provides a Chromium web browser engine as well as C++ classes to render web content and interact with it. Modules Related to System API CallingQt is more than a GUI framework. It also provides a cross-platform way of doing a lot of stuff that desktop applications often need to do - calling into system APIs, especially platform-specific multimedia APIs not covered by the POSIX standard and&#x2F;or the C++ and Python standard libraries. To better document this crucial yet often overlooked use case, we have summarized the relevant modules of PySide6. PySide6.QtBluetooth: Enables connectivity between Bluetooth enabled devices. Currently the API is supported to different degrees on Android, iOS, macOS, Linux, and Windows. PySide6.QtMultimedia: Provides APIs for rendering audio and video files on screen and playing them back, as well as a thorough API for recording audio and video via system cameras and microphones. PySide6.QtNfc: Provides APIs for dealing with NFC Forum Tags and NFC Forum Devices, including target identification and loss, the registration of NDEF message handlers, the reading and writing of NDEF messages on NFC Forum Tags, and the sending of tag-specific commands. PySide6.QtPositioning: Allows developers to locate themselves using a variety of sources, such as satellite, wifi, text files, and so on. The position on a map, for instance, can be determined using that information. Additionally, it is possible to retrieve satellite data and carry out area-based monitoring. Currently the API is supported on Android, iOS, macOS, Linux, and Windows (with GPS receivers exposed as a serial port providing NMEA sentences or using Windows.Devices.Geolocation). PySide6.QtPrintSupport: Offers broad cross-platform printing capability, including printing to attached printers, printing to remote printers over networks, and creating PDF files. PySide6.QtSensors: Provides access to sensor hardware. Currently the API is supported on Android, iOS, and Windows (MSVC). References: https://en.wikipedia.org/wiki/Qt_(software) https://www.quora.com/What-are-the-Pros-and-Cons-of-using-QT-framework-for-cross-platform-programming-Win-Mac https://wiki.qt.io/Qt_for_Python","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"},{"name":"PySide6","slug":"Code/Python/PySide6","permalink":"https://abbaswu.github.io/categories/Code/Python/PySide6/"}],"tags":[]},{"title":"Date and Time Types in Python","slug":"Date-and-Time-Types-in-Python","date":"2022-12-31T08:00:00.000Z","updated":"2022-12-31T16:32:37.116Z","comments":true,"path":"2022/12/31/Date-and-Time-Types-in-Python/","link":"","permalink":"https://abbaswu.github.io/2022/12/31/Date-and-Time-Types-in-Python/","excerpt":"","text":"There are many types in Python which can store date and time information. These types can be broadly divided into two categories: JSON Serializable Formats UNIX Timestamp (e.g. 0) ISO 8601 String (e.g. &#39;1970-01-01T00:00:00&#39;) UNIX Timestamp has its roots in the system time of Unix operating systems. It is now widely used in databases, programming languages, file systems, and other computer operating systems. It counts the number of seconds that have passed since the Unix epoch began on January 1, 1970 at 00:00:00 UTC, minus any modifications made for leap seconds. ISO 8601 is an international standard for the transmission and interchange of time- and date-related information on a global scale. Dates in the Gregorian calendar, hours based on the 24-hour timekeeping system, with an optional UTC offset, time intervals, and combinations of these are covered by ISO 8601. The standard offers a clear, unambiguous manner of expressing calendar dates and times in international communications, notably to prevent numeric dates and times from being misinterpreted when such data is sent between nations. As the categorization suggests, these formats can be used in JSON serialization, and are widely adopted in data exchange formats and APIs. For example, Stripe APIs use UNIX Timestamps, while Twitter and Dropbox APIs use ISO 8601 Strings. UNIX Timestamps are easier and more efficient to handle, while ISO 8601 Strings have the virtue of being human-readable. Widely Used In Memory Data Structures datetime.datetime (e.g. datetime.datetime(1970, 1, 1, 0, 0)) datetime.date (e.g. datetime.date(1970, 1, 1)) pandas.Timestamp (e.g. Timestamp(&#39;1970-01-01 00:00:00&#39;)) As the categorization suggests, these formats are in-memory, structured representations of date and time information. datetime.datetime and datetime.date are types implemented (and widely used) in the Python Standard Library. datetime.date represents a date (year, month, day) in an idealized calendar, which is the existing Gregorian calendar infinitely stretched in both directions, while datetime.datetime also combines the data from a time object (hour, minute, second, microsecond). pandas.Timestamp is implemented in pandas. It is the pandas replacement for datetime.datetime, and is the type used for the entries that make up a pandas.DatetimeIndex, and other time series-oriented data structures in pandas. Furthermore, it is also widely used across the Python Ecosystem for Data Science, such as being used by matplotlib as the xticks for plotting a pandas.Series with a pandas.DatetimeIndex, as shown below. Converting Between These TypesWith so many types in Python which can store date and time information, it is important to know how to convert between them. The following State Diagram depicts how we should perform the conversions. stateDiagram state &quot;UNIX Timestamp&quot; as UNIXTimestamp: 0 state &quot;ISO 8601 String&quot; as ISO8601String: &#x27;1970-01-01T00:00:00&#x27; state &quot;datetime.datetime&quot; as DatetimeDatetime: datetime.datetime(1970, 1, 1, 0, 0) state &quot;datetime.date&quot; as DatetimeDate: datetime.date(1970, 1, 1) state &quot;pandas.Timestamp&quot; as PandasTimestamp: Timestamp(&#x27;1970-01-01 00:00:00&#x27;) UNIXTimestamp --&gt; DatetimeDatetime: datetime.datetime.fromtimestamp function UNIXTimestamp --&gt; PandasTimestamp: constructor ISO8601String --&gt; DatetimeDatetime: isoformat method ISO8601String --&gt; PandasTimestamp: constructor DatetimeDatetime --&gt; UNIXTimestamp: datetime.datetime.timestamp function DatetimeDatetime --&gt; ISO8601String: datetime.datetime.fromiso function DatetimeDatetime --&gt; DatetimeDate: date method DatetimeDatetime --&gt; PandasTimestamp: constructor DatetimeDate --&gt; PandasTimestamp: pandas.Timestamp constructor PandasTimestamp --&gt; UNIXTimestamp: timestamp method PandasTimestamp --&gt; ISO8601String: isoformat method PandasTimestamp --&gt; DatetimeDatetime: to_pydatetime method References: https://en.wikipedia.org/wiki/Unix_time https://en.wikipedia.org/wiki/ISO_8601 https://dev.to/xngwng/do-you-prefer-unix-epoch-a-number-or-iso-8601-a-string-for-timestamps--28ll https://stackoverflow.com/questions/15554586/timestamps-iso8601-vs-unix-timestamp https://www.dataquest.io/blog/tutorial-time-series-analysis-with-pandas/ https://www.programiz.com/python-programming/datetime/timestamp-datetime https://stackoverflow.com/questions/3743222/how-do-i-convert-a-datetime-to-date https://stackoverflow.com/questions/969285/how-do-i-translate-an-iso-8601-datetime-string-into-a-python-datetime-object https://www.programiz.com/python-programming/datetime/timestamp-datetime https://pynative.com/python-iso-8601-datetime/ https://docs.python.org/3/library/datetime.html https://stackoverflow.com/questions/1937622/convert-date-to-datetime-in-python https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html https://stackoverflow.com/questions/993358/creating-a-range-of-dates-in-python https://stackoverflow.com/questions/41046630/set-time-formatting-on-a-datetime-index-when-plotting-pandas-series","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"}],"tags":[]},{"title":"Social Media Strategy (tentative)","slug":"Social-Media-Strategy-tentative","date":"2022-12-31T08:00:00.000Z","updated":"2022-12-31T19:34:02.650Z","comments":true,"path":"2022/12/31/Social-Media-Strategy-tentative/","link":"","permalink":"https://abbaswu.github.io/2022/12/31/Social-Media-Strategy-tentative/","excerpt":"","text":"Social media accounts are windows that exhibit our image to the public, and we must pay attention to them. However, on the one hand, the use of social media should complement, and not negatively affect, our routine work, study, and life patterns; on the other hand, different social media platforms generally differ in terms of appropriate content to display. After a period of observation and reflection, we have developed the following social media strategy to address these issues, as depicted in the Bipartite Graph below. stateDiagram state &quot;Life Moments&quot; as LifeMoments state &quot;Reflections on Life&quot; as ReflectionsOnLife state &quot;Work Moments&quot; as WorkMoments state &quot;Reflections on Work&quot; as ReflectionsOnWork state &quot;Reflections on Development&quot; as ReflectionsOnDevelopment state &quot;Detailed Explanations&quot; as DetailedExplanations state &quot;Planning&quot; as Planning state &quot;Instagram&quot; as Instagram state &quot;小红书&quot; as XiaoHongShu state &quot;WeChat&quot; as WeChat state &quot;QQ&quot; as QQ state &quot;LinkedIn&quot; as LinkedIn state &quot;Blog&quot; as Blog LifeMoments --&gt; Instagram LifeMoments --&gt; XiaoHongShu LifeMoments --&gt; WeChat LifeMoments --&gt; QQ ReflectionsOnLife --&gt; XiaoHongShu ReflectionsOnLife --&gt; Twitter ReflectionsOnLife --&gt; WeChat ReflectionsOnLife --&gt; QQ WorkMoments --&gt; Twitter WorkMoments --&gt; WeChat WorkMoments --&gt; QQ ReflectionsOnWork --&gt; Twitter ReflectionsOnWork --&gt; WeChat ReflectionsOnWork --&gt; QQ ReflectionsOnDevelopment --&gt; LinkedIn DetailedExplanations --&gt; Blog Planning --&gt; Blog In addition, a pain point we will encounter is that we will post some content to both Chinese and English social media platforms. To overcome this problem, we can first write a Chinese (or English) version and use automated tools such as Google Translate and DeepL before manually touching up the machine-translated version. Furthermore, to boost the following of our social media accounts, when we need to share the content posted on social media with others via private chat, we can share the link of the content posted to social media instead of copying and pasting the content itself.","categories":[{"name":"Planning","slug":"Planning","permalink":"https://abbaswu.github.io/categories/Planning/"}],"tags":[]},{"title":"Command-line HTTP Servers for Rapid File Sharing","slug":"Command-line-HTTP-Servers-for-Rapid-File-Sharing","date":"2022-12-26T08:00:00.000Z","updated":"2023-02-18T22:55:42.597Z","comments":true,"path":"2022/12/26/Command-line-HTTP-Servers-for-Rapid-File-Sharing/","link":"","permalink":"https://abbaswu.github.io/2022/12/26/Command-line-HTTP-Servers-for-Rapid-File-Sharing/","excerpt":"","text":"Sometimes, we need an ad-hoc, quick-and-dirty way of sharing files with others while maintaining complete control of the data transmission. We should not store our data on any third-party servers. The connections established through the network should be point-to-point. Being supported by virtually every Internet-capable device and from both command-line tools (such as wget and curl) and graphical Web browsers, the HTTP protocol is one of our best bets. Thus, we have compiled a list of command-line HTTP servers that enable rapid file sharing and compare their features. python3 -m http.serverThe Python standard library has a barebones built-in HTTP server. Not recommended for production. Language: Python mjpclab&#x2F;go-http-file-serverSimple command line based HTTP file server to share local file system. Features: Cross-Origin Resource Sharing (CORS) Frontend Features: File Upload File Delete Create Subdirectory Download the Current Directory as an Archive HTTP Basic Authentication HTTP Range Requests HTTP Strict Transport Security (HSTS) HTTPS GitHub Stars: 175 Language: Go mar10&#x2F;wsgidavA generic and extendable WebDAV server written in Python and based on WSGI. Features: HTTP Range Requests GitHub Stars: 640 Language: Python Multithreaded WebDAV Server TheWaWaR&#x2F;simple-http-server Features: Cross-Origin-Embedder-Policy (COEP) Cross-Origin Resource Sharing (CORS) Cross-Origin-Opener-Policy (COOP) HTTP Basic Authentication HTTP Range Requests HTTPS Frontend Features: File Upload GitHub Stars: 785 Language: Rust Multithreaded http-party&#x2F;http-serverhttp-server is a simple, zero-configuration command-line static HTTP server. It is powerful enough for production usage, but it’s simple and hackable enough to be used for testing, local development and learning. Features: Cross-Origin Resource Sharing (CORS) HTTP Basic Authentication HTTP Range Requests HTTPS GitHub Stars: 12.4k Language: node.js EstebanBorai&#x2F;http-serverSimple and configurable command-line HTTP server Features: Cross-Origin Resource Sharing (CORS) HTTP Basic Authentication HTTPS GZip Compression","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"}],"tags":[]},{"title":"Paper Review: Efficient scalable thread-safety-violation detection: finding thousands of concurrency bugs during testing","slug":"Paper-Review-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing","date":"2022-11-27T08:00:00.000Z","updated":"2022-12-19T18:38:47.736Z","comments":true,"path":"2022/11/27/Paper-Review-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing/","link":"","permalink":"https://abbaswu.github.io/2022/11/27/Paper-Review-Efficient-scalable-thread-safety-violation-detection-finding-thousands-of-concurrency-bugs-during-testing/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. This paper presents Thread Safety Violation Detection (TSVD), a tool that dynamically detects thread safety violations with low runtime overhead, and which is compatible with real-world, distributed-developed code employing different synchronization mechanisms. The tool frames thread safety violations as two methods, with one of them being a write operation, occurring concurrently. It infers thread safety violations using a very creative approach. First, it instruments the program and detects method calls that access objects behind thread-safety contracts. Later on, during the execution of the program, TSVD injects delays into threads with method calls accessing those objects and monitors whether another thread also accesses the same objects during the delay. As this may incur significant overhead, the tool uses two strategies to determine when to inject delays - keeping track of “near misses”, where the two method calls of two threads occur within a time threshold apart from each other, and inferring “happens before” relations, to rule out two accesses which are causally related. The tool was tested on 43000 .NET programs in Microsoft teams, and its bug-finding capability outperformed both existing tools and configuring TSVD to emulate the strategies of existing tools, which shows the feasibility of TSVD. There are two questions that come to mind after reading this paper: How does the tool acquire the information on which methods are thread-unsafe? The approach the tool uses to infer thread safely - injecting delays and monitoring the behavior of other threads - sounds very interesting to me. Have there been any other applications of such an approach? What is the sensitivity of the relevant parameters used in TSVD to its effectiveness and efficiency? Is there any guide on how to properly adjust these parameters? Feedback from the Class Discussion The proposed approach can handle different concurrency models, such as: async task-based thread-based But can it handle unstructured concurrency? The approach generalizes data race for objects and data structures at the method-level (e.g. there cannot be two simultaneous calls to add() for a List class). Using delays can handle many more cases than reasoning about thread scheduling. It is a “simple thing” which works for many cases (akin to fuzzing). The approach requires manually specifying read and write APIs. Is it possible to create a semi-automatic approach starting from contracts labeled for standard library APIs?","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Hybrid dynamic data race detection","slug":"Paper-Review-Hybrid-dynamic-data-race-detection","date":"2022-11-23T08:00:00.000Z","updated":"2022-12-19T18:29:37.066Z","comments":true,"path":"2022/11/23/Paper-Review-Hybrid-dynamic-data-race-detection/","link":"","permalink":"https://abbaswu.github.io/2022/11/23/Paper-Review-Hybrid-dynamic-data-race-detection/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. The paper proposes a hybrid approach to dynamically (at runtime) data races in multithreaded Java programs. It first proposes two specific detection approaches, each with its strengths and weaknesses. The first is lockset-based detection, which identifies a data race when multiple threads use a shared memory location without holding a shared lock object. Such an approach is fast but may lead to false positives. As a result, the paper proposes another approach, happens-before detection, which uses several heuristics to reason about relations between events and infer whether a potential race has occurred at a particular memory location. In comparison, this approach is more computationally expensive and may lead to false negatives. Considering that neither approach is sound, they combine the two approaches by first using lockset-based detection to identify potential data races before using happens-before detection to reason whether these are probable. The paper then conducts an experimental study of their hybrid approach on various Java programs, demonstrating its effectiveness and efficiency. I like this paper’s idea of combining a pessimistic and optimistic approach when doing program analysis. Are there any other works that use such an idea? However, I have a question concerning the applicability of the hybrid approach in real life. Although pessimistic, shouldn’t lockset-based detection be enough to stamp out all potential data races by providing programmers with feedback to add relevant locks to prevent such possible data races? This is relevant to the requirements for defensive programming. Or are there design patterns where multiple threads can safely use a shared memory location without holding a common lock and not lead to data races? Feedback from the Class Discussion Difference Between Race Condition and Data Race: Race Condition: There are multiple threads, and the behavior of program depends on thread scheduling. Data Race: Different from race condition. This frequently happens when you parallelize a program that shouldn’t be parallelized. Data race can be solved by using locks, but there may still be race coditions. 12345Thread-1:synchronized(...) &#123; x = 1;&#125; 12345Thread-2:synchronized(...)&#123; x = 2;&#125; Modelling in the Paper: Lamport Timestamps&#x2F;Vector Clocks Thread events: statement executions in threads. A thread event is dependent on previous thread events. This is captured used using the happens-before formal definition in the paper, but leads to false negatives. Thread communications: signals (enforce order) and locks (mutually exclusive). Message send&#x2F;receive: enqueue and dequeue. Architecure-dependednt atomic operations can also be a lock-free solution (e.g. C++’s atomic).","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Lightweight Verification of Array Indexing","slug":"Paper-Review-Lightweight-Verification-of-Array-Indexing","date":"2022-11-16T08:00:00.000Z","updated":"2022-12-19T18:52:48.788Z","comments":true,"path":"2022/11/16/Paper-Review-Lightweight-Verification-of-Array-Indexing/","link":"","permalink":"https://abbaswu.github.io/2022/11/16/Paper-Review-Lightweight-Verification-of-Array-Indexing/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Summary of the PaperThe authors propose a methodology to detect out-of-bound array accesses statically. They first define that criteria that ideal techniques for detecting out-of-bound array accesses should satisfy, before analyzing the insufficiency of existing academic and industrial approaches, and presenting their own approach, Index Checker, implemented for Java. Index Checker reduces checking array bonds to identifying 7 kinds of knowledge, which concern array index and array length, and form a hierarchy. It models such hierarchical knowledge as a Type System, requires the user to write “Type” Annotations at procedure boundaries, and verifies that values have the given “Type” at runtime. This is implemented using Checker Framework, an “industrial-strength, open-source tool for building Java type systems”. The authors evaluate Index Checker on 3 large-scale, well-tested Java projects (Google Guava, JFreeChart, Plume-lib), and compare Index Checker with 3 other approaches (FindBugs, KeY, and Clousot), proving the effectiveness of Index Checker (scalability, finding bugs in well-tested programs, and low false positive rate). They also assess the burden of writing type annotations for Index Checker. Questions What is the rationale behind the 7 kinds of knowledge concerning array index and array length proposed in the paper? I am not very familiar with Type Theory, which may have impeded my understanding of the value of the paper. What are the benefits of using Type Systems and Type Inference, and using Type Annotations to capture known constraints? Is it just to leverage the power of Checker Framework, an “industrial-strength, open-source tool for building Java type systems”, for sound inference? Or are there any further benefits? No matter what the benefits are, from this paper, modeling hierarchical knowledge as a Type System, using Type Annotations to capture known constraints, and using Type Inference to verify such constraints sounds like a very innovative technique with many potential use cases. Have there been any other applications of such a technique? Feedback from the Class DiscussionThe hierarchy of knowledge is derived from Exploratory Data Analysis (trying stuff until it works, see Section 2.8). “Subtype” is a kind of Comparable Partial Ordering (‘&lt;’). The Types in the Bottom have more information, while the Types in the Top have less information. In Java, aside from Inheritance, another form of Subtyping is Function Subtyping. e.g. Comparator (to compare two Dog’s we can pass a function that compares two Animal’s) the inputs can be more general types. Rules define what to do when a Pattern is encountered; however, it takes a (nontrivial) search to determine the order to apply the rules. Fixed Point: Convergence of Information. Reach a Fixed Point: Iterate until Convergence. The Paper uses Subtyping to implement Widening.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Modular Checking for Buffer Overflows in the Large","slug":"Paper-Review-Modular-Checking-for-Buffer-Overflows-in-the-Large","date":"2022-11-13T08:00:00.000Z","updated":"2022-12-18T22:20:14.437Z","comments":true,"path":"2022/11/13/Paper-Review-Modular-Checking-for-Buffer-Overflows-in-the-Large/","link":"","permalink":"https://abbaswu.github.io/2022/11/13/Paper-Review-Modular-Checking-for-Buffer-Overflows-in-the-Large/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Background InformationDatalog Declarative Programming Language which began as a Query Language for Relational Databases, and is now used in Data Integration, Information Extraction, Program Analysis, Cloud Computing, Machine Learning, etc. Akin to SQL in many aspects. Not Turing Complete. Used as a Domain Specific Language. No Canonical Implementation, many different Implementations exist for different Applications (c.f. SQLite, MySQL, PostgreSQL, etc. for SQL). Follows the ‘Logic Programming’ Paradigm. A Program consists of Constants, Variables, Facts, and Rules (based on First Order Logic, in a form similar to “a new Fact A is true if B, C, and D are already known to be true”). The Execution of a Program is iteratively inferring new Facts given the Rules. Maps very nicely to many problems encountered during Program Analysis. Summary of the PaperThe authors proposed a Methodology for detecting possible Buffer Overflow-based Security Exploits in C code and providing developers with instant feedback during the build process. The Methodology prefers usability over accuracy, and should be used alongside other tools in a Swiss Cheese Model against Security Exploits. First, the authors proposed a Simple Annotations Language for annotating Pointers passed as parameters to and returned from Functions, to denote Preconditions and Postconditions of Function Execution. The authors propose that for new code, annotation should be inserted manually, and code should be fully annotated before being checked in to Version Control. For legacy codebases and&#x2F;or third-party code without such Annotations, the authors propose an Inference Engline, SALInfer, which tries to infer such Annotations, preferring Coverage over Accuracy. SALInfer supports specifying Inference Algorithms using Datalog. Finally, the authors propose a modular checker, ESPX, which tries to infer if a program is potentially vulnerable to Buffer Overflow-based Security Exploits by statically analyzing the annotations within the program’s code. The confidence of the inference results vary based on the extent and quality of the annotations. Questions Regarding the Paper What is the relevance of such a technique to “safe” programming languages that do not allow using overflowable buffers? The authors state that “control over annotation insertion is given to individual developers”. However, developers might be reluctant to insert Annotations, and inserting Annotations can negatively affect developer productivity. Furthermore, the quality of the inserted Annotations is not guaranteed. Last but not least, inferring Annotations for legacy codebases and&#x2F;or third-party code without such Annotations prefers Coverage over Accuracy, which may not lead to sound results. Considering all these real concerns, the practical usability of this tool is seriously compromised. The authors did an evaluation on an unnamed Microsoft product. With little information regarding the product being disclosed, such an evaluation is far from convincing, and I suspect that there might be manipulation of some kind within the evaluation.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Precise Interprocedural Dataflow Analysis via Graph Reachability","slug":"Paper-Review-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability","date":"2022-11-07T08:00:00.000Z","updated":"2022-12-19T17:03:40.947Z","comments":true,"path":"2022/11/07/Paper-Review-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability/","link":"","permalink":"https://abbaswu.github.io/2022/11/07/Paper-Review-Precise-Interprocedural-Dataflow-Analysis-via-Graph-Reachability/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. To be honest, I found the paper to be almost unreadable due to it being full of unfamiliar concepts and abstract formalizations. I tried my best to do some studying into the topic so that I can understand the problem that they are trying to solve, and important aspects of their algorithm, better. Graph ReachabilityGraph Reachability means whether it is possible to get from one vertex to another vertex within a graph. In an Undirected Graph $G(V, E)$, Graph Reachability between one pair of nodes can be calculated using Breadth-First Search, while Graph Reachability between all pair of nodes can be reduced to calculating the Connected Components of the Undirected Graph, which is an efficient algorithm with $O(|V| + |E|)$ time complexity. In a Directed Graph, Graph Reachability between one pair of nodes can also be calculated using Breadth-First Search. However, there is no efficient algorithm that can calculate Graph Reachability between all pair of nodes for all Directed Graphs. For any Directed Graph, calculating Graph Reachability between all pair of nodes can be reduced to calculating All Pairs Shortest Distance using the Floyd-Warshall Algorithm, which has an $O({|V|}^3)$ time complexity. More efficient algorithms are only applicable to Planar Directed Graphs. Data Flow AnalysisConstant Propogation (determining whether variables at a given point in the program are guaranteed to have constant values) and Live Variable Analysis (determining at a given point in the program, which variables might be used before being overwritten) are two commonly encountered examples of Data Flow Analysis. Given a program’s Control Flow Graph, Data Flow Analysis: Associates each Node of the Control Flow Graph with Information concerning the Variables within that Node (known as Dataflow Fact‘s, usually a Mapping between Variables and their Values or Properties) Models the effect of executing a Node with a Dataflow Function. In most Data Flow Analysis problems, we take one of the following approaches to obtain the Dataflow Facts for each Node: Summarizing paths entering the Node from the Start, such as in Constant Propogation. Known as “Forward Problem”‘s. Summarizing paths exiting the Node from the Exit, such as in Live Variable Analysis. Known as “Backward Problem”‘s. How we summarize paths is known as the Confluence Operator. Data Flow Analysis problems can also be divided into “may” problems and “must” problems. In “may” problems, the Dataflow Facts for each Node include information about what may be true. An example is Live Variable Analysis, where we determine whether a variable may be used before being overwritten in a given point in the program. In “must” problems, the Dataflow Facts for each Node include information about what must be true. An example is Constant Propogation, where we determine whether a variable must have a given value in a given point in the program. Many interesting Data Flow Analysis problems, such as Live Variable Analysis, can be modeled as GEN&#x2F;KILL problems, or bit-vector problems, in which: A set of variables, $KILL[n]$, is defined at Node $n$. A set of variables, $GEN[n]$, is used at Node $n$. We use Union or Intersection to summarize paths entering a Node to obtain the Dataflow Facts for the Node. Interprocedural Dataflow AnalysisThe goal of Interprocedural Dataflow Analysis is to capture an Abstraction of the Effect of calling a Procedure in Dataflow Analysis. A naive approach to Interprocedural Dataflow Analysis is to reduce it to Intraprocedural Dataflow Analysis in some way. Procedure Inlining Exponentially increases the Control Flow Graph Cannot handle recursion Context Sensitive Procedure Inlining Uses Context Information (often an Approximation of the Call Stack) to distinguish between different Calls of the same Procedure, and reduce the number of inlined Procedures. However, even after research, I have failed to understand the more complicated approaches (as well as the approaches proposed in this Paper). I can only get the point that the author shows that many Interprocedural Dataflow Analysis problems, in which: A finite set of Dataflow Facts Dataflow Functions distribute over the Confluence Operator (which I don’t fully understand) including GEN&#x2F;KILL problems, or bit-vector problems, can be reduced to a Graph Reachability Problem on a Directed Graph. Furthermore, I believe the main contribution of this paper is theoretical, but what is its value in real-world Dataflow Analysis problems, especially considering that the Time Complexity of Graph Reachability Problems on Directed Graphs are high? I honestly hope that I can get some insight into these approaches during our class on Monday. Thank you! Feedback from the Class DiscussionSome of the paper’s idea comes from Abstract Interpretation. It is nice theoretically, but it is far from implementation. Graph Reachability in the context of Interprocedure Analysis is also known as Context-Free Language Reachability and Dyck Reachability. In the context of this paper, there are multiple Dataflow Functions, one for each Node in the Control Flow Graph. Given a Node in the Control Flow Graph, we use Pattern Matching to determine what its Dataflow Function is. Lambdas are used to represent these Dataflow Functions. Explanation for the notations: $\\lambda .&lt;return_value&gt;$ means def f(&lt;parameters&gt;): return &lt;return_value&gt;. In the context of this paper, we require all Dataflow Functions to be distributive over the Meet Function (Confluence Function). This means that, given the Meet Function $\\Pi$ and a Dataflow Function $f$, $f(X\\PiY) &#x3D; f(X)\\Pif(Y)$ for any two Dataflow Facts $X, Y$. Each Dataflow Function can be visualized using a Graph Representation. The Edges represent Dependencies between Facts of the Variables in the Old Dataflow Facts and Facts of the Variables in the New Dataflow Facts. Worklist Algorithm: an Algorithm which takes Objects from a Worklist (a Queue of some sort) one at a time, processes it in some way, and perhaps further adds new Objects to the Worklist, until some Target is reached. Example: Breadth First Search.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Mining Input Grammars from Dynamic Taints","slug":"Paper-Review-Mining-Input-Grammars-from-Dynamic-Taints","date":"2022-11-02T07:00:00.000Z","updated":"2022-12-19T07:56:21.312Z","comments":true,"path":"2022/11/02/Paper-Review-Mining-Input-Grammars-from-Dynamic-Taints/","link":"","permalink":"https://abbaswu.github.io/2022/11/02/Paper-Review-Mining-Input-Grammars-from-Dynamic-Taints/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. A program usually accepts a formal language as input. Inferring the grammar of this formal language is an important task with many use cases. Helps humans understand the structure of the formal language. Manually writing valid inputs Reverse Engineering Generate inputs for testing and fuzzing The authors propose Autogram, a method that infers a Context Free Grammar given a set of sample inputs and a Java program that accepts that set of inputs and uses it in some way. Autogram adapts a Dynamic Taining-based approach: It monitors the data flow of each character within the input, with “the Input Fragment it came from” as the taint. It traces method entries, method exits, field accesses, and array accesses within the execution of the program. From such a trace, the Dynamic Call Tree is reconstructed, and the sets of Intervals (Input Fragments) processed by functions, stored in variables, and returned by functions is derived. This is used to build an Interval Tree, and the Interval Tree is refined into a Pure Input Tree free of conflicting overlaps (resulting from parsers using lookaheads). The pure input tree is assumed to be a Parse Tree, and Production Rules are derived from it. The leaf nodes are considered to be Terminals, and Regular Expressions matching them are learned. The authors then conduct an experimental study concerning the accuracy and completeness of the inferred Context Free Grammars using “parts of the Java Standard API that are used to process URLs and property files”, and “open source projects that implement support for CSV, INI, and JSON formats”. However, I had more questions than answers after reading this paper. One of the use cases that the authors mentioned is “the grammar vastly simplifies the creation of parsing programs that decompose existing inputs into their constituents”. Why don’t we directly extract the parsing logic out of the program Autogram runs on? The type of Context Free Grammar inferred by Autogram seems to be an LL(1) Grammar. This type of Grammar is only able to represent simple Grammars, and does not support for Left Recursion, which is pervasive in real-world Grammars. Why don’t they infer an LALR(1) Grammar, which is both simple and expressive (it supports representing may real-world Programming Languages). Perhaps, a Hidden Markov Model could be trained to infer the Transitions between the States within the LALR(1) Parse Table should an LALR(1) Grammar be inferred? In the current implementation of Autogram, tracing is efficient, as the authors have mentioned: “millions of calls result in traces of a few Megabytes”. However, the current implementation incurs a ~100x performance overhead, and there is a lot of room for performance optimization. Maybe ideas that we have discussed for TaintCheck and Qsym (direct Binary Analysis, preinstrumenting Bytecode, JIT compilation etc.) could be used here? The specific process of refining an Interval Tree into a Pure Interval Tree free of conflicting overlaps is not described clearly in the paper. Why don’t the author present an example with figures showing the manipulation of nodes within the Interval Tree during this process? The author also mentions applying “a simple heuristic that assumes left to right processing of the input” to resolve possible ambiguities associated with parsers using lookaheads. However, what is the rationale behind this “simple heuristic”? The specific process of deriving Production Rules from the Pure Interval Tree is also unclear. What do the authors mean by “We can thus check if nodes are compatible and can be used to derive productions for for the same nonterminal symbol”? What is the meaning of “compatible” in this context? The programs used in the experimental study are all open-source programs of very high code quality (containing accurately named variables and functions). However, how well does Autogram work with closed-source programs, and&#x2F;or programs with low code quality, containing obscure variable and function names? This is frequently the situation we encounter when we try to reverse engineer the (often closed-source and&#x2F;or obscure) structure of a program’s input, one of the major use cases of Autogram. Also some inspiration and ideas I got from the paper: The author mentions that “dynamic tainting allows us to precisely identify which parts of a programs input are read, stored and processed at any point in time”. Could this technique be used in a Fuzzing context to identify which bits generated by a Coverage-Guided Fuzzer are used in which sections of a fuzzed program? The logic of building an Interval Tree is very interesting, and it reads like the “Subset Tree” mentioned in the KLEE paper. I conject that both these Tree Structures could be generalized and used in a much wider range of contexts. Feedback from the Class Discussion A Context Free Grammar may not capture the structure of binary files. How does the approach compare to unsupervised parsing in NLP or fine-tuning language models, especially with a lot of input? Is it possible to use feedback to improve the mined grammar? From one tree, we infer one set of grammar rules; from 1000 trees, we infer 1000 sets of grammar rules. They are merged together to derive the final Context Free Grammar.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Various Solutions for Different Types of Reverse Proxying","slug":"Various-Solutions-for-Different-Types-of-Reverse-Proxying","date":"2022-10-30T07:00:00.000Z","updated":"2022-12-18T19:57:43.647Z","comments":true,"path":"2022/10/30/Various-Solutions-for-Different-Types-of-Reverse-Proxying/","link":"","permalink":"https://abbaswu.github.io/2022/10/30/Various-Solutions-for-Different-Types-of-Reverse-Proxying/","excerpt":"","text":"There are some situations in which we have to expose a locally running web service to the Internet. This is know as Reverse Proxying. Depending on the situation in hand, there are multiple ways to do this: Server with Public IP AvailableIn this case, the Server is also known as a Jump Server. Client Accessible from ServerRun a port-forwarding tool such as socat on the Server. 1socat TCP-LISTEN:&lt;Port the Server listens on&gt;,fork,reuseaddr TCP:&lt;IP address of the Client&gt;:&lt;Port of the Service on the Client&gt; Client Inaccessible from ServerUse nat-tunnel on both the Server and the Client. Or, use Reverse SSH Tunneling on the Client. Reverse SSH TunnelingBefore TunnelingOn the Server: Update the sshd config file (/etc/ssh/sshd_config). Set GatewayPorts to yes. Restart the SSH Service. Make sure the Port the Server listens on allows Inbound Traffic. To TunnelOn the Client: 1ssh [-f] [-N] [-T] -R &lt;Port the Server listens on&gt;:localhost:&lt;Port of the Service on the Client&gt; [How you connect to the Server (e.g. `-i key-pair.pem &lt;username&gt;@&lt;domain&gt;`)] -f tells the SSH to background itself after it authenticates, saving you time by not having to run something on the remote server for the tunnel to remain alive. -N if all you need is to create a tunnel without running any remote commands then include this option to save resources. -T useful to disable pseudo-tty allocation, which is fitting if you are not trying to create an interactive shell. -R tells the tunnel to answer on the remote side. Server with Public IP UnavailableUse a commercial service such as ngrok on the Client. References: https://en.wikipedia.org/wiki/Reverse_proxy https://www.kvm.la/1328.html https://blog.csdn.net/weixin_35867652/article/details/104362302 https://www.hostinger.com/tutorials/how-to-set-up-nginx-reverse-proxy/ https://stevessmarthomeguide.com/understanding-port-forwarding/ https://jfrog.com/connect/post/reverse-ssh-tunneling-from-start-to-end/ https://linuxhint.com/ssh-port-forwarding-linux/ https://www.ssh.com/academy/ssh/tunneling-example https://superuser.com/questions/1408427/remote-port-forwarding-through-a-jump-server https://unix.stackexchange.com/questions/436290/single-step-ssh-port-forwarding-not-working-but-only-works-when-ssh-port-forward?rq=1&amp;newreg=def5dfc9fb43466d8685fd7639eb17cc https://www.opensourceforu.com/2021/09/how-to-do-reverse-tunnelling-with-the-amazon-ec2-instance/ https://superuser.com/questions/1194105/ssh-troubleshooting-remote-port-forwarding-failed-for-listen-port-errors https://docs.hevodata.com/getting-started/connection-options/connecting-through-reverse-ssh/ https://www.youtube.com/watch?v=TZ6W9Hi9YJw https://blog.devolutions.net/2017/03/what-is-reverse-ssh-port-forwarding/ https://chenhuijing.com/blog/tunnelling-services-for-exposing-localhost-to-the-web/ https://johackim.com/how-to-expose-local-server-behind-firewall https://gabrieltanner.org/blog/port-forwarding-frp/ https://www.techiediaries.com/public-localhost/ https://superuser.com/questions/121435/is-it-possible-to-host-a-web-server-from-behind-a-nat/1360660 https://medium.com/tech-learnings/how-to-expose-a-local-server-to-the-internet-without-any-additional-tools-ae49e6b8fe93 https://serverfault.com/questions/282959/how-do-i-reach-my-internal-server-on-the-external-ip https://superuser.com/questions/624925/how-to-access-internal-valid-ip-through-internet","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"}],"tags":[]},{"title":"Paper Review: Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software","slug":"Paper-Review-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software","date":"2022-10-29T07:00:00.000Z","updated":"2022-12-19T07:51:11.005Z","comments":true,"path":"2022/10/29/Paper-Review-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software/","link":"","permalink":"https://abbaswu.github.io/2022/10/29/Paper-Review-Dynamic-Taint-Analysis-for-Automatic-Detection-Analysis-and-Signature-Generation-of-Exploits-on-Commodity-Software/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. To combat worms spread by the Internet exploiting software vulnerabilities, the paper proposes TaintCheck, a dynamic taint analysis technique for automatic detection of exploits on software. Summary of TaintCheck: TaintCheck directly operates on an arbitrary executable and does not require its source code. It uses Valgrind to translate basic blocks being executed into Valgrind’s RISC-like instruction set (UCode), inserts UCode instructions for instrumentation, and passes the modified UCode back to Valgrind for execution. TaintCheck by default considers data originating from the network as “untrusted” and taints it. It keeps track of “the propagation of tainted data as the program executes”, which involves monitoring data movement instructions and arithmetic instructions, with the exception of constant functions such as xor eax, eax. To accomplish this, TaintCheck associates “each byte of memory” with a Taint data structure. Different instances of such a data structure are “chained” to record “how tainted data is propagated”. TaintCheck checks whether tainted data is used in ways it considers illegitimate, such as being used as a return address, a function pointer, a format string, and (optionally) as an argument of a system call. When such illegitimate uses are detected, it is possible to collect information about a software vulnerability, especially “the execution path from tainted data’s entry and its use in a probable exploit”. The paper also proposes a new semantic-based automatic signature generation approach on top of TaintCheck. There are several questions that came to my mind when I was reading this paper: The paper mentions that “the current implementation slows program execution between 1.5 and 40 times”, but also mentions that “the prototype has not been optimized”, and proposes optimization techniques. Why didn’t the authors implement these optimization techniques and conduct experiments on the optimized TaintCheck? There is no doubt that using Valgrind to translate basic blocks being executed into UCode greatly simplifies dynamic taint analysis on an arbitrary executable, as TaintCheck deals with an RISC-like instruction set instead of raw machine code. However, this incurs significant overhead. Would directly performing dynamic taint analysis on machine code at runtime using a dynamic binary instrumentation tool such as Intel Pin boost performance (like the case of QSym)? What about generating UCode, inserting instructions for instrumentation, and passing the modified UCode back to Valgrind before the executable is executed? What is the overhead of using the Taint data structure? Would the total size of all Taint data structures explode for long-running processes? And why do they use this Taint data structure, instead of using a conventional Data Flow Graph? What is the list of constant functions that TaintCheck supports? Is it representative, and is it extensible? Are the ways tainted data is used considered by TaintCheck to be illegitimate representative of real exploits? How well can TaintCheck discriminate from “illegitimate” uses with intentional uses, and&#x2F;or uses with checks? Specifically, the paper mentions that TaintCheck can “untaint the data immediately after it has been sanity checked”, but how is this situation detected? In the evaluation section, why are the benchmarks used in assessing “compatibility and false positives” different from those used in assessing “attack detection” on actual exploits? What does a “signature” look like, and how is it used to filter attacks? Feedback from the Class Discussion Performance is not a priority. The paper is more of a proof-of-concept, and even “reads like a grant proposal”, especially Section 6. The “taint data structure” includes more information than a dataflow graph (snapshots of the stacks etc.). It can also use a “memory arena” instead of vanilla heap allocation to improve performance. Some of the detected attacks may not be present in “safe”, managed languages. Due to the large overhead, the technique cannot be used to handle requests in production, but requests can be forked to it instead. Dynamic taint analysis can have applications outside of the security domain.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Python in a Functional Style: Closures, Generators, and Coroutines","slug":"Python-in-a-Functional-Style-Closures-Generators-and-Coroutines","date":"2022-10-28T07:00:00.000Z","updated":"2023-03-18T01:15:40.798Z","comments":true,"path":"2022/10/28/Python-in-a-Functional-Style-Closures-Generators-and-Coroutines/","link":"","permalink":"https://abbaswu.github.io/2022/10/28/Python-in-a-Functional-Style-Closures-Generators-and-Coroutines/","excerpt":"","text":"Python in a Functional Style: Closures, Generators, and CoroutinesJifeng Wu 2022-10-28 Closures👈 Generators Coroutines All Python functions are closures. Function code. Execution environment of function code (variables it depend on). A nested function can be returned. This is a common design pattern for creating tailored functions. 1234def get_greeting_function(name): def greeting_function(): print(f&#x27;Hello, &#123;name&#125;&#x27;) return greeting_function All Python functions are closures. Function code. Execution environment of function code (variables it depend on). A nested function can be returned. This is a common design pattern for creating tailored functions. 1234567&gt;&gt;&gt; function_greeting_a = get_greeting_function(&#x27;A&#x27;)&gt;&gt;&gt; function_greeting_a()Hello, A&gt;&gt;&gt;&gt;&gt;&gt; function_greeting_b = get_greeting_function(&#x27;B&#x27;)&gt;&gt;&gt; function_greeting_b()Hello, B Look into a closure’s cell_contents: 12345678910111213&gt;&gt;&gt; function_greeting_a.__closure__(&lt;cell at 0x7f3c81849ca8: str object at 0x7f3c8185ac70&gt;,)&gt;&gt;&gt; function_greeting_a.__closure__[0]&lt;cell at 0x7f3c81849ca8: str object at 0x7f3c8185ac70&gt;&gt;&gt;&gt; function_greeting_a.__closure__[0].cell_contents&#x27;A&#x27;&gt;&gt;&gt; &gt;&gt;&gt; function_greeting_b.__closure__(&lt;cell at 0x7f3c81849c18: str object at 0x7f3c82f18e30&gt;,)&gt;&gt;&gt; function_greeting_b.__closure__[0]&lt;cell at 0x7f3c81849c18: str object at 0x7f3c82f18e30&gt;&gt;&gt;&gt; function_greeting_b.__closure__[0].cell_contents&#x27;B&#x27; Should an inner function use an outer function’s local variable (instead of shadowing it), that local variable should be declared nonlocal within the inner function. Not using nonlocal: 1234567def outer_function(): string = &#x27;Hello&#x27; def inner_function(): # Shadows the local variable `string` of `outer_function` string = &#x27;World&#x27; inner_function() return string 12&gt;&gt;&gt; outer_function()&#x27;Hello&#x27; Should an inner function use an outer function’s local variable (instead of shadowing it), that local variable should be declared nonlocal within the inner function. Using nonlocal: 12345678def outer_function(): string = &#x27;Hello&#x27; def inner_function(): # Uses the local variable `string` of `outer_function` nonlocal string string = &#x27;World&#x27; inner_function() return string 12&gt;&gt;&gt; outer_function()&#x27;World&#x27; Creating and returning a nested function based on a function argument is widely used in Python, called decorating a function. 1234567891011121314def cached(function): cache = &#123;&#125; def cached_function(*args): nonlocal function, cache if args in cache: print(f&#x27;Cache hit with args: &#123;args&#125;&#x27;) return cache[args] else: print(f&#x27;Cache miss with args: &#123;args&#125;&#x27;) result = function(*args) print(f&#x27;Writing f(&#123;args&#125;) =&gt; &#123;result&#125; to cache&#x27;) cache[args] = result return result return cached_function Python even has special syntatical support for this. 12345678@cacheddef fib(n): if n &lt; 1: return 0 elif n &lt; 2: return 1 else: return fib(n - 1) + fib(n - 2) 1234567891011121314151617In [4]: fib(5) Cache miss with args: (5,)Cache miss with args: (4,)Cache miss with args: (3,)Cache miss with args: (2,)Cache miss with args: (1,)Writing f((1,)) =&gt; 1 to cacheCache miss with args: (0,)Writing f((0,)) =&gt; 0 to cacheWriting f((2,)) =&gt; 1 to cacheCache hit with args: (1,)Writing f((3,)) =&gt; 2 to cacheCache hit with args: (2,)Writing f((4,)) =&gt; 3 to cacheCache hit with args: (3,)Writing f((5,)) =&gt; 5 to cacheOut[4]: 5 $O(n)$ time complexity. LeetCode problem: Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. Example 1: 12Input: n = 3Output: [&quot;((()))&quot;,&quot;(()())&quot;,&quot;(())()&quot;,&quot;()(())&quot;,&quot;()()()&quot;] Example 2: 12Input: n = 1Output: [&quot;()&quot;] We write a Context Free Grammar and analyze it: 12S -&gt; S S&#x27; | S&#x27; .S&#x27; -&gt; ( S ) | ( ) . https://mdaines.github.io/grammophone/# 123456789101112131415161718192021@cacheddef s_generator(number_of_parenthesis): print(f&#x27;s_generator(&#123;number_of_parenthesis&#125;)&#x27;) return_value = [] # s -&gt; ss . if number_of_parenthesis &gt;= 1: for ss_string in ss_generator(number_of_parenthesis): return_value.append(ss_string) # s -&gt; s ss . if number_of_parenthesis &gt;= 2: for i in range(1, number_of_parenthesis): for s_string, ss_string in itertools.product( s_generator(i), ss_generator(number_of_parenthesis - i) ): return_value.append(s_string + ss_string) return return_value 123456789101112131415@cacheddef ss_generator(number_of_parenthesis): print(f&#x27;ss_generator(&#123;number_of_parenthesis&#125;)&#x27;) return_value = [] # ss -&gt; ( ) . if number_of_parenthesis == 1: return_value.append(&#x27;()&#x27;) # ss -&gt; ( s ) . if number_of_parenthesis &gt; 1: for s_string in s_generator(number_of_parenthesis - 1): return_value.append(&#x27;(&#x27; + s_string + &#x27;)&#x27;) return return_value 12Input: n = 3Output: [&quot;((()))&quot;,&quot;(()())&quot;,&quot;(())()&quot;,&quot;()(())&quot;,&quot;()()()&quot;] 1234567891011121314In [4]: s_generator(3) s_generator(3)ss_generator(3)s_generator(2)ss_generator(2)s_generator(1)ss_generator(1)Out[4]: [&#x27;((()))&#x27;, &#x27;(()())&#x27;, &#x27;()(())&#x27;, &#x27;(())()&#x27;, &#x27;()()()&#x27;]In [5]: s_generator.cache_info() Out[5]: CacheInfo(hits=3, misses=3, maxsize=None, currsize=3)In [6]: ss_generator.cache_info() Out[6]: CacheInfo(hits=3, misses=3, maxsize=None, currsize=3) Closures also provide an efficient mechanism for maintaining state between several calls. Traditional (OOP) approach: 12345678class Countdown: def __init__(self, n): self.n = n def next_value(self): old_value = self.n self.n -= 1 return old_value Closure-based approach: 12345678def countdown(n): def get_next_value(): nonlocal n old_value = n n -= 1 return old_value return get_next_value This is not only clean but also fast. 12345678910111213def test_object_oriented_approach(): c = Countdown(1_000_000) while True: value = c.next_value() if value == 0: breakdef test_functional_approach(): get_next_value = countdown(1_000_000) while True: value = get_next_value() if value == 0: break 12345In [5]: %timeit test_object_oriented_approach()182 ms ± 2.61 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)In [6]: %timeit test_functional_approach()96.8 ms ± 1.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Why? 12345678910111213141516In [9]: c = Countdown(1_000_000)In [10]: dis(c.next_value) 6 0 LOAD_FAST 0 (self) 2 LOAD_ATTR 0 (n) 4 STORE_FAST 1 (old_value) 7 6 LOAD_FAST 0 (self) 8 DUP_TOP 10 LOAD_ATTR 0 (n) 12 LOAD_CONST 1 (1) 14 INPLACE_SUBTRACT 16 ROT_TWO 18 STORE_ATTR 0 (n) 8 20 LOAD_FAST 1 (old_value) 22 RETURN_VALUE 12 instructions, 2 LOAD_ATTR instructions, 1 STORE_ATTR instruction. 123456789101112In [11]: get_next_value = countdown(1_000_000)In [12]: dis(get_next_value) 4 0 LOAD_DEREF 0 (n) 2 STORE_FAST 0 (old_value) 5 4 LOAD_DEREF 0 (n) 6 LOAD_CONST 1 (1) 8 INPLACE_SUBTRACT 10 STORE_DEREF 0 (n) 6 12 LOAD_FAST 0 (old_value) 14 RETURN_VALUE 8 instructions, NO LOAD_ATTR, STORE_ATTR instructions. Closures Generators👈 Coroutines When we define a function containing the yield keyword, we define a generator. Defining a generator allows the user to define a custom iterator in the style of defining a function. 1234def countdown(n): while n &gt; 0: yield n n -= 1 We create a generator object when we call a generator definition. The generator object can be used like any iterator: 1234567891011121314In [2]: c = countdown(5)In [3]: next(c)Out[3]: 5In [4]: next(c)Out[4]: 4In [5]: for value in c: ...: print(value) ...:321 When we call next() on a generator object, it will execute code, until it encounters a yield statement. The yield statement tells the generator object to return a value, and continue execution from here when next() is called again. 1234In [2]: c = countdown(5)In [3]: next(c)Out[3]: 5 This executes: 12while n &gt; 0: yield n When we call next() on a generator object, it will execute code, until it encounters a yield statement. The yield statement tells the generator object to return a value, and continue execution from here when next() is called again. 12In [4]: next(c)Out[4]: 4 This executes: 123 n -= 1while n &gt; 0: yield n This is called lazy evaluation. This can dramatically boost performance and reduce memory usage in some applications. For example: 1234567891011def get_comments_from_file(file): with open(file, &#x27;r&#x27;) as fp: for line in fp: # strip whitespace stripped_line = line.strip() # check if the line is empty after stripping whitespace if stripped_line: # check if the line is a comment if stripped_line[0] == &#x27;#&#x27;: # if it is, yield it yield stripped_line This will NOT read the whole file into memory. Only when the user calls next() on the generator object, will the generator read the file LINE BY LINE (with only ONE LINE of the file in memory at once), and return the next comment line. This is an efficient way of extracting comments from GB-sized files (such as logs). itertoolsPython provides many functions for creating an iterator from another iterator. For example: itertools.permutations(iterable [, r]) itertools.combinations(iterable, r) itertools.product(iter1, iter2, iterN, [repeat=1]) Widely used in algorithms: itertools.permutations(iterable [,r]) 123456789101112131415In [1]: import itertoolsIn [2]: numbers = range(4)In [3]: permutations_of_two_numbers_iterator = itertools.permutations(numbers, r=2)In [4]: next(permutations_of_two_numbers_iterator)Out[4]: (0, 1)In [5]: next(permutations_of_two_numbers_iterator)Out[5]: (0, 2)In [6]: next(permutations_of_two_numbers_iterator)Out[6]: (0, 3) Widely used in algorithms: itertools.combinations(iterable ,r) 12345678910111213In [1]: import itertoolsIn [2]: numbers = range(4)In [3]: for first, second in itertools.combinations(numbers, 2): ...: print(first, second) ...:0 10 20 31 21 32 3 Widely used in algorithms: itertools.product(iter1, iter2, iterN, [repeat=1]) 1234567891011121314In [1]: import itertoolsIn [2]: first_list = [1,2,3]In [3]: second_list = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;]In [4]: third_list = [True,False]In [5]: it = itertools.product(first_list, second_list, third_list)In [6]: next(it)Out[6]: (1, &#x27;a&#x27;, True)In [7]: next(it)Out[7]: (1, &#x27;a&#x27;, False)In [8]: next(it)Out[8]: (1, &#x27;b&#x27;, True) Closures Generators Coroutines👈 Starting from Python 2.5, the yield statement can be used as an right value: 1captured_input = yield value_to_yield Generators defined like this can accept sent input while providing output. These generators are called coroutines. The concept of coroutines was proposed in the 60s, but only gained traction in recent years. Coroutines can be seen as a combination of subroutines and threads. Can pause and restart during execution. Controlled by itself instead of the operating system. Different coroutines run within a thread are concurrent instead of parallel. Simple example: 1234567891011121314import mathdef update_mean(): current_input = yield sum = current_input count = 1 while True: current_input = yield sum / count sum += current_input count += 1 Simple example: 123In [3]: updater = update_mean()In [4]: next(updater) This executes: 1current_input = yield And the coroutine waits for an input to be sent. Send an input: 12In [5]: updater.send(2)Out[5]: 2.0 The coroutine receives the input, and executes: 1234sum = current_inputcount = 1while True: current_input = yield sum / count And the coroutine waits for an input to be sent. Send an input: 12In [6]: updater.send(4)Out[6]: 3.0 The coroutine receives the input, and executes: 1234 sum += current_input count += 1while True: current_input = yield sum / count And the coroutine waits again for an input to be sent. More complicated example: set-associative cache simulation number_of_cache_sets * Set number_of_ways_of_associativity * Block block_size_in_bytes * Byte The whole set-associative cache is a coroutine receiving (address, is_write) tuples as input, and calculating (cache_hit, writeback_address) tuples as output. It models each set as a coroutine receiving (tag, is_write) tuples as input, and calculating (cache_hit, writeback_address) tuples as output. Different coroutine definitions for round-robin, LRU, etc. The whole set-associative cache 1234567891011121314151617181920212223242526272829303132def cache_coroutine(cache_set_coroutine_function, block_size_in_bytes, number_of_ways_of_associativity, number_of_cache_sets): # create cache_set_coroutine_list and activate each cache_set_coroutine cache_set_coroutine_list = [ cache_set_coroutine_function(number_of_ways_of_associativity) for _ in range(number_of_cache_sets) ] for cache_set_coroutine in cache_set_coroutine_list: next(cache_set_coroutine) # get function_to_split_address and function_to_merge_address function_to_split_address, function_to_merge_address = get_functions_to_split_and_merge_address( block_size_in_bytes, number_of_cache_sets ) # receive address, is_write # yields nothing address, is_write = yield while True: # splits address tag, cache_set_index, offset = function_to_split_address(address) # send (tag, is_write) to the appropriate cache_set_coroutine cache_hit, victim_tag, writeback_required = cache_set_coroutine_list[cache_set_index].send((tag, is_write)) # create writeback_address if (victim_tag is not None) and writeback_required if (victim_tag is not None) and writeback_required: writeback_address = function_to_merge_address(victim_tag, cache_set_index, 0) else: writeback_address = None # receive address, is_write # yield cache_hit, writeback_address address, is_write = yield cache_hit, writeback_address Cache Set with LRU replacement policy 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def lru_cache_set_coroutine(associativity): tag_list = [ None for _ in range(associativity) ] dirty_bit_list = [ False for _ in range(associativity) ] indices_in_lru_order = OrderedDict() for index in range(associativity - 1, -1, -1): indices_in_lru_order[index] = None # receive first tag and is_write tag, is_write = yield while True: cache_hit = False victim_tag = None writeback_required = False try: # find tag_index tag_index = tag_list.index(tag) # tag_index found cache_hit = True if is_write: dirty_bit_list[tag_index] = True # move tag_index to the end of indices_in_lru_order indices_in_lru_order.move_to_end(tag_index) except ValueError: # tag_index not found # get index_of_victim from indices_in_lru_order index_of_victim, _ = indices_in_lru_order.popitem(last=False) victim_tag = tag_list[index_of_victim] if dirty_bit_list[index_of_victim]: writeback_required = True tag_list[index_of_victim] = tag if is_write: dirty_bit_list[index_of_victim] = True else: dirty_bit_list[index_of_victim] = False # insert index_of_victim to the end of indices_in_lru_order indices_in_lru_order[index_of_victim] = None # receive tag and is_write # yield (cache_hit, victim_tag, writeback_required) tag, is_write = yield (cache_hit, victim_tag, writeback_required) Suppose our cache has only eight blocks and each block contains four words. The cache is 2-way set associative, so there are four sets of two blocks. The write policy is write-back and write-allocate. LRU replacement is used. https://courses.cs.washington.edu/courses/cse378/02sp/sections/section9-3.html 12345678910111213141516171819202122232425In [3]: cache = cache_coroutine(lru_cache_set_coroutine, block_size_in_bytes=4 * ...: 2, number_of_ways_of_associativity=2, number_of_cache_sets=4) In [4]: next(cache) In [5]: cache.send((0, True)) Out[5]: (False, None)In [6]: cache.send((64, False)) Out[6]: (False, None)In [7]: cache.send((4, True)) Out[7]: (True, None)In [8]: cache.send((40, True)) Out[8]: (False, None)In [9]: cache.send((68, False)) Out[9]: (True, None)In [10]: cache.send((128, True)) Out[10]: (False, 0)In [11]: cache.send((0, False)) Out[11]: (False, None)","categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"}],"tags":[]},{"title":"Paper Review: The Fundamentals of Writing Questions","slug":"Paper-Review-The-Fundamentals-of-Writing-Questions","date":"2022-10-26T07:00:00.000Z","updated":"2022-12-18T19:51:10.711Z","comments":true,"path":"2022/10/26/Paper-Review-The-Fundamentals-of-Writing-Questions/","link":"","permalink":"https://abbaswu.github.io/2022/10/26/Paper-Review-The-Fundamentals-of-Writing-Questions/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. This part of the book addresses the problem of crafting survey questions that respondents are willing to answer and respond to accurately. It first discusses issues to consider when designing survey questions, then presents the structure of a survey question and different question formats, before providing specific guidelines on wording survey questions. Issues to consider when designing survey questions What concepts to measure Recommended: Adopt established measures from existing surveys What data to collect Factual information: precise, readily available Opinion: requires time to formulate, strongly influenced by context Behavior: better memory of recent, memorable events compared with distant, mundane events What question format to use Different cognitive information processing for aural and visual surveys What mode to adopt The presence of an interviewer may speed up surveys, but may induce social desirability and acquiescence, leading to interviewer bias. Lack of standardization among different interviewers may lead to interviewer variance. What to modify (from existing surveys) no changes or only minimal changes when replicating or comparing results questions should also be asked in a similar fashion How to motivate respondents think about the cognitive process respondents go through pay attention to the context and wording The structure of a survey question Question stem Additional instructions Answer spaces or choices Different question formats Open-ended rich, detailed more prone to skipping requires lengthy data processing Closed-ended nominal or ordinal categories set of answer choices known in advance easy to analyze Partially closed-ended closed-ended with “other” response respondents more likely to select the options instead of “other” Specific guidelines on wording survey questions Choose the appropriate question format. Make sure the question applies to the respondent. Ask one question at a time. Make sure the question is technically accurate. Use simple, familiar and specific words. Use short, simple sentences that take a question form. Avoid double negatives. Organize questions in a more straightforward, comprehensible way. Also sprinkled throughout the section is the notion that the crafter should get into a respondent’s state of mind when crafting survey questions, and also test the survey questions to evaluate their quality. This section is very comprehensive and convincing, as the author supports his arguments by analyzing specific examples from actual surveys, and also frequently quoting previous work on the topic. From such a chapter we can gain a deep understanding of the nature of survey questions, especially the underlying cognitive, psychology and sociology problems, as well as the best practices within the domain, and we can also refer to this chapter as a guide and checklist when we craft survey questions ourselves.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: ReCrash: Making Software Failures Reproducible by Preserving Object States","slug":"Paper-Review-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States","date":"2022-10-25T07:00:00.000Z","updated":"2023-02-26T17:03:34.863Z","comments":true,"path":"2022/10/25/Paper-Review-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States/","link":"","permalink":"https://abbaswu.github.io/2022/10/25/Paper-Review-ReCrash-Making-Software-Failures-Reproducible-by-Preserving-Object-States/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. BackgroundReproduction is key to finding and fixing software problems and verifying proposed solutions, but reproduction can be difficult. Nondeterminism: A problem may depend on timing (e.g., context switching), memory layout (e.g., hash codes), or random number generators. Remote detection: A problem may be discovered by someone other than the developer, and it may depend on implicit program inputs such as user GUI actions, environment variables, the state of the file system, operating system behavior, etc. This information may be easy to miss, difficult to collect, or confidential. Test case complexity: The exposing execution might be complex, and the buggy method might be called multiple times before the bug is triggered. Proposed Solution: ReCrashReCrash maintains a shadow stack with copies of the receiver and arguments to each method during execution of the target program. Several copy strategies Several optimizations When the program crashes, ReCrash serializes the shadow stack, and generates unit tests by calling each method on the shadow call stack with their receiver and arguments. Calling the method at top of the call stack may not provide enough context. Calling a method closer to the bottom provides more context, but is less likely to reproduce the original failure. Proposed Solution: ReCrashAssumption: It is possible to reproduce many failures with only some of the information available on entry to the methods on the stack at the time of the failure. Many bugs are dependent on small parts of the heap. Good object-oriented style encapsulates important state nearby. Good object-oriented style avoids excessive use of globals. ReCrash has access to and will store any parts of the global state or environment that are passed as method arguments. Question: What if global state is read or written in the method? Monitoring Phase Several copy strategies Several optimizations Monitoring fewer methods Second-chance mode Copy StrategiesAn argument may be side-effected between the method entry and the point of the failure in the method. Copying strategies: Reference: copying only the reference to the argument. Shallow: copying the argument itself. Depth-i: copying all the state reachable with $\\le i$ dereferences from the argument. Deep-copy: copying the entire state. Options: Used-fields: deeper copying on fields that are used (read or written) in the method. ReCrash always uses the reference strategy for immutable parameters. Monitoring Fewer MethodsDosen’t monitor methods that cannot be used in the generated tests, or are unlikely to expose problems. non-public methods empty methods simple methods such as getters and setters (no more than 6 opcodes) Second-chance Mode ReCrash initially monitors no method calls. Each time a failure occurs, ReCrash enables method argument monitoring for all methods found on the stack trace. Efficient, but requires a failure to be repeated twice. If the developer doesn’t mind missing the first time a failure happens, and the failure occurs relatively often, second chance mode is a good fit. Question: could recording all inputs provided to the program be used in tandom with second-chance mode (such that the failure is probable to happen the second time)? Test Generation PhaseReCrash generates a test for each of the methods in the shadow stack. Restores the state of the arguments that were passed to a method. Invokes the method the same way it was invoked in the original execution. Only tests that end with the same exception as the original failure are saved. Storing more than one test that ends with the same failure is useful. Some tests reproduce a failure, but would not help the developer understand, fix, or check her solution. Experimental StudySubject programs: Javac-jsr308: the OpenJDK Java compiler, extended with JSR308 (“Annotations on Java Types”), with four crashes provided by the developers. SVNKit: a subversion client, with three crash examples from bug reports. Eclipsec: a Java compiler included in the Eclipse JDT, with a crash found in the Eclipse bug database. BST: a toy subject program used by Csallner in evaluating CnC, with three crashes found by CnC. Experimental StudyFor each subject program: Run PIDASA for parameter immutability classification. For different argument copying strategies, with and without second-chance mode: Run ReCrash on inputs that made the subject programs crash. Count how many test cases reproduced each crash. Question: how useful would ReCrash be in reality where it is unknown whether the subject projects could crash, and which inputs would make the subject programs crash? Experimental StudyResearch questions: How reliably can ReCrashJ reproduce crashes? What is the size of the stored deep copy of the shadow stack? Are the tests generated by ReCrash useful for debugging? Like a case study: an analysis of two crashes, and comments from developers What is the overhead (time and memory) of running ReCrash? Aspects assessed: different argument copying strategies with and without second-chance mode How reliably can ReCrash reproduce crashes?ReCrash was able to reproduce the crash in all cases. For some crashes, every candidate test case reproduces the crash. For other crashes, only a subset of the generated test cases reproduces the crash. In most cases, simply copying references is enough to reproduce crashes. In other cases, using the shallow copying strategy with used-fields was necessary. What is the size of the stored deep copy of the shadow stack? Question: why isn’t it compared with the program size and the program memory usage? An analysis of two crashesEclipsec bug e1: Eclipsec crashes in callee canBeInstantiated because an earlier if statement in the caller resolveType failed to set a boolean flag hasError to true. The test case for canBeInstantiated will reproduce the crash, but is not helpful. Demonstrates importance of generating tests for multiple methods on the stack. Javac-jsr308 bug j4: Compiling source code containing an annotation with too many arguments results in an index-out-of-bounds exception in method visitMethodInvocation. The generated test does not require the whole source code and encodes only the necessary minimum to reproduce the crash. Useful when the compiler crash happens in the field, and the user cannot provide the entire source code for debugging. Comments from DevelopersWe gave the tests for j1-4 to two Javac-jsr308 developers and asked for comments about the tests’ usefulness, receiving positive responses. I often have to climb back up through a stack trace when debugging. ReCrash seems to generate a test method for multiple levels of the stack, making it useful. I find that you wouldn’t have to wait for the crash to occur again useful. When I set a break point, the break point maybe be executed multiple times before the error. Using ReCrash, I was able to jump (almost directly) to the necessary breakpoint. Question: Why only analyze two crashes and ask only two developers? What is the overhead (time and memory) of running ReCrash?Time overheadNon second-chance mode: Copying only the references can be expensive (11%-42%), and shallow copying with used-fields is similar (13%–60%). Usable for in-house testing. Deep copying is completely unusable (12,000%-638,000%). Second-chance mode: A barely noticeable 0%–1.7% under copying only the references and shallow copying with used-fields, after a crash has already been observed. What is the overhead (time and memory) of running ReCrash?Memory overheadNon second-chance mode: 0.2M–4.7M (2.6%-90.3%) under shallow copying with used-fields. Second-chance mode: negligible ConclusionsReCrashJ is usable in real software deployment Simple to implement Scalable Generates simple, helpful test cases that effectively reproduce failures Time and memory overhead (13%–60%, 2.6%-90.3%) under non second-chance mode and shallow copying with used-fields usable for in-house testing Extremely efficient under second-chance mode","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: All You Ever Wanted to Know About Dynamic Taint Analysis and Forward Symbolic Execution (but might have been afraid to ask)","slug":"Paper-Review-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask","date":"2022-10-25T07:00:00.000Z","updated":"2022-12-19T07:44:38.128Z","comments":true,"path":"2022/10/25/Paper-Review-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask/","link":"","permalink":"https://abbaswu.github.io/2022/10/25/Paper-Review-All-You-Ever-Wanted-to-Know-About-Dynamic-Taint-Analysis-and-Forward-Symbolic-Execution-but-might-have-been-afraid-to-ask/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. Forward symbolic execution and dynamic taint analysis are quickly becoming “staple techniques in security analyses”. Dynamic taint analysis runs a program and observes which computations are affected by predefined taint sources such as user input. Dynamic forward symbolic execution automatically builds a logical formula describing a program execution path, which reduces the problem of reasoning about the execution to logic, allowing us to reason about the behavior of a program on many different inputs at one time. The two analyses can be used in conjunction to build formulas representing only the parts of an execution that depend upon tainted values. Forward symbolic execution: Test Case Generation (automatically generate inputs to test programs, generate inputs that cause two implementations of the same protocol to behave differently) Automatic Input Filter Generation (input filters that detect and remove exploits from the input stream) Cristian Cadar, Daniel Dunbar, and Dawson Engler. Klee: Unassisted and automatic generation of high-coverage tests for complex systems programs. In Proceedings of the USENIX Symposium on Operating System Design and Implementation, 2008. Cristian Cadar, Vijay Ganesh, Peter Pawlowski, David Dill, and Dawson Engler. EXE: A system for automatically generating inputs of death using symbolic execution. In Proceedings of the ACM Conference on Computer and Communications Security, October 2006. Patrice Godefroid, Nils Klarlund, and Koushik Sen. DART: Directed automated random testing. In Proceedings of the ACM Conference on Programming Language Design and Implementation, 2005. Dynamic taint analysis: Unknown Vulnerability Detection (misuses of user input) Automatic Network Protocol Understanding. Dynamic taint analysis has been used to automatically understand the behavior of network protocols when given an implementation of the protocol. Malware Analysis (analyze how information flows through a malware binary, explore trigger-based behavior, and detect emulators) However, there has been little effort to formally define them and summarize critical issues that arise when applying these techniques in “typical security contexts”. The authors formalize the runtime semantics of dynamic taint analysis and forward symbolic execution by using SIMPIL (Simple Intermediate Language), which is “representative of internal representations used by compilers and is powerful enough to express typical languages”. Concepts: Statements: assignments, assertions, jumps, conditional jumps. Expressions: constants, variables, binary operators, unary operators, get_input. Execution state: the list of program statements, the current memory state, the current value for variables, the program counter, the current statement. Notation: $\\Sigma$: list of program statements $\\Sigma[v_1]$: statement at $pc &#x3D; v_1$. $\\mu$: memory state $\\mu[v_1]$: memory content at address $v_1$ $\\Delta$: register state (values of all variables) $\\Delta[x]$: value of variable $x$ $\\Delta[x \\leftarrow 10]$: setting the value of variable $x$ to 10 $pc$: program counter. $\\mu, \\Delta \\vdash e \\Downarrow v$: Given memory state $\\mu$ and register state $\\Delta$, the value of expression $e$ is $v$. $\\Sigma, \\mu, \\Delta, pc, EXPRESSION \\rightsquigarrow \\Sigma, \\mu’, \\Delta’, pc’, {EXPRESSION}’$: Given list of program statements $\\Sigma$, memory state $\\mu$, register state $\\Delta$, program counter $pc$, executing expression $EXPRESSION$ leads to new memory state $\\mu’$, new register state $\\Delta’$, new program counter $pc’$, and the next expression is ${EXPRESSION}’$. Other high-level language constructs such as functions or scopes can be easily represented using these constructs. Dynamic taint analysis tracks values in a program dependent on data derived from a “taint source” at runtime. As it is conducted at runtime, it can be expressed by extending SIMPIL. Dynamic taint analysis is conducted in different ways (i.e., under different “taint policies”) for different applications. The differences lie in “how new taint is introduced to a program”, “how taint propagates as instructions execute”, and “how taint is checked during execution”. The author presents the example of the “tainted jump policy” for attack detection, points out several challenges it faces, and analyzes the proposed solutions. “Distinguishing between memory addresses and cells is not always appropriate”. An alternative “tainted addresses policy” could be used, but this may also overtaint. Information flow can occur through control dependencies in addition to dataflow dependencies. This requires “reasoning about multiple paths”, while pure dynamic taint analysis “executes on a single path at a time”. Solutions include “supplementing dynamic analysis with static analysis” and “using heuristics”. Taint is only added and never removed (i.e., “sanitized”), leading to the problem of “taint spread”, reducing precision. Well-known constant functions (i.e. using XOR to zero out registers in x86 code) can be checked. In addition, we can consider the outputs of some functions like cryptographic hash functions as untainted, due to limited influence of input on output. This can be quantified (Newsome et al.) to automatically recognize such cases. Furthermore, values can be untainted “if the program logic performs sanitization itself” (e.g., index bounds checking). In conclusion, this paper is an useful introductory paper in forward symbolic execution and dynamic taint analysis, and I have mainly learned the following two things from the paper: The idea of formalizing runtime semantics using RISC-like bytecode An introduction to dynamic taint analysis - what it is, what it can do, and what challenges it faces Feedback from the Class Discussion What is the difference between a statement and an expression? A statement can modify program state when it is executed, while an expression doesn’t modify program state. In the formalism of SIMPIL, we determine which expression to evaluate by pattern-matching the rule. LLVM has a dataflow sanitization pass, which may be useful for implementing taint analysis. Dynamic program analysis only looks at a single path. If we are to prove something about a program, static program analysis would be a better direction.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Evolutionary Generation of Whole Test Suites","slug":"Paper-Review-Evolutionary-Generation-of-Whole-Test-Suites","date":"2022-10-24T07:00:00.000Z","updated":"2022-12-18T22:20:14.435Z","comments":true,"path":"2022/10/24/Paper-Review-Evolutionary-Generation-of-Whole-Test-Suites/","link":"","permalink":"https://abbaswu.github.io/2022/10/24/Paper-Review-Evolutionary-Generation-of-Whole-Test-Suites/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. BackgroundAutomatically deriving test cases for realistically sized programs: Select one coverage goal (e.g., program branch) at a time, and derive a test case that exercises this particular goal. Solving path constraints generated with symbolic execution &#x2F; dynamic symbolic execution Meta-heuristic search techniques Mutation testing Alternative approaches not directly aimed to achieve code coverage Randoop incrementally generate sequences of function calls to find buggy test sequences requires automated oracles (e.g. developer-written assertions and exceptions) ProblemsMany coverage goals are unreachable. 1234567891011void push(int x) &#123; if (size &gt;= values.length) &#123; resize(); &#125; if (size &lt; values.length) &#123; values[size++] = x; &#125; else &#123; // UNREACHABLE &#125;&#125; ProblemsSome coverage goals are more difficult to satisfy than others. The order of coverage goals is important: a lucky choice can result in a good test suite, while an unlucky choice can result in a waste of resources. 12345678910111213void push(int x) &#123; if (size &gt;= values.length) &#123; // HARD resize(); &#125; else &#123; // EASY &#125; if (size &lt; values.length) &#123; values[size++] = x; &#125;&#125; ProblemsSatisfying a particular coverage goal frequently entails satisfying further coverage goals by accident. The order of coverage goals is important. 123456789int pop() &#123; if (size &gt; 0) &#123; // May imply coverage in `push` and `resize` return values[size]; &#125; else &#123; throw new EmptyStackException(); &#125;&#125; Our Solution: EvoSuite Optimize an entire test suite at once instead of considering distinct test cases. Evolve a population of test suites towards satisfying a coverage criterion. Assume automated oracles are not available, and require the outputs of the test cases to be manually verified. The generated test suites should be of manageable size. Solves the problem of: difficult and unreachable coverage goals order of coverage goals accidentally satisfying further coverage goals Our Solution: EvoSuiteQuestions: We are interested in sequences in OOP. Should coverage in terms of a new ordering seen in the last $n$ function calls in the sequence should make more sense? (Praveen) It seems like Evosuite offloads the responsibility of adding in correct assertions to the developers. How easy is it for the developers to do this, especially when compared with manually writing all of the test suite? (Shizuko, ToTo, Larry) EvoSuite ModelingPopulation 1 .. M Test Suite 1 .. N Test Case 1 .. L Statement Four types of statements are modeled. Primitive statements: numeric variables (e.g. int var0 = 54;) Constructor statements: new instances of a class (e.g. Stack var1 = new Stack();). All parameters of the constructor call have to be values of previous statements. Field statements: public fields of objects (e.g. int var2 = var1.size;). If the field is non-static, then the source object of the field has to be a value of a previous statement. Method statements: public methods of objects (e.g. int var3 = var1.pop();). The source object and all parameters have to be values of previous statements. EvoSuite ModelingThe set of available classes, their public constructors, methods, and fields are extracted from the given software under test. An optimal solution is a test suite that covers all the feasible branches&#x2F;methods and is minimal in the number of statements. EvoSuite Process Overview Randomly generate a set of initial test suites. Evolve using evolutionary search towards satisfying a coverage criterion. Minimize the best resulting test suite. Questions: How are test suites randomly generated? The author discusses “sampling”. Where are we sampling from? (Larry, Jifeng) Evolutionary Search Test Suite Fitness Function Crossover Accepting the Mutated Offspring Bloat Control Test Suite Fitness FunctionCovering all branches $B$ and methods $M$ of a program. To estimates how close a test suite $T$ is to covering all branches $B$ of a program, for each branch $b$, minimal branch distance $d_{min}(b, T)$ is measured. If the branch predicate is $x \\ge 10$, and during execution, $x &#x3D;&#x3D; 5$, then the minimal branch distance is $10 - 5 &#x3D; 5$. The minimal branch distance is then normalized to get the branch distance $d(b, T) &#x3D; f(d_{min}(b, T))$, where $f(x) &#x3D; \\frac{x}{x + 1}$. $fitness(T) &#x3D; |M| - |M_T| + \\sum_{b \\in B}{d(b, T)}$ If execution exceeds a time limit of 5 minutes, maximum fitness is automatically assigned. Test Suite Fitness FunctionQuestions: What does branch distance actually mean? Why do we use it? (Eric, Rut, Yayu, Udit, Jifeng) Doesn’t $\\sum_{b \\in B}{d(b, T)}$ already consider that branch distances are maximal in unvisited methods? Why do we need an additional $|M| - |M_T|$ term? Furthermore, different methods could have a different number of branches. Should the branch distance sum for all branches within a method be normalized? (Jifeng) CrossoverRank selection based on the fitness function is used to select two parent test suites $P_1$ and $P_2$ for crossover. In case of ties, smaller test suites are assigned better ranks. During crossover: a random value $\\alpha$ is chosen from $(0, 1)$ the first offspring test suite $O_1$ will contain the first $\\alpha |P_1|$ test cases from $P_1$ and the last $(1 - \\alpha)|P_2|$ test cases from $P_2$ the second offspring test suite $O_2$ will contain the first $\\alpha |P_2|$ test cases from $P_2$ and the last $(1 - \\alpha)|P_1|$ test cases from $P_1$ because test cases are independent, $O_1$ and $O_2$ will always be valid MutationThe two offspring test suites $O_1$ and $O_2$ are then mutated. When a test suite T is mutated, each of its test cases is mutated with probability $\\frac{1}{|T|}$. If a test case $t$ is mutated, remove statements, change statements, and insert statements are each applied with probability $\\frac{1}{3}$. Then, a number of new random test cases are added to $T$. Remove Statements If a test case $t$ contains $n$ statements, each statement is removed with probability $\\frac{1}{n}$. If the removed statement $s_i$ is subsequently used by $s_j (j &gt; i)$, try to replace this use with another statement before $s_j$. If this is not possible, recursively remove $s_j$. If all statements have been removed from $t$, remove $t$ from $T$. Change Statements If a test case $t$ contains $n$ statements, each statement is changed with probability $\\frac{1}{n}$. If the changed statement $s_i$ is a primitive statement, its numeric value is changed by a random value. Otherwise, a method, field, or constructor with the same return type is randomly chosen. Insert Statements With probability $p$, a new statement is inserted at a random position in the test case. With probability $p^2$, a second statement is inserted, and so on. Questions: What are the justifications for the probabilities? (Kevin) Can we change the probabilities used in the mutation and insertion by using method calls they kept track of and variables generated in each iteration? (Joyce) When deleting, if the statement is chosen from the beginning few statements, is there a high probability that many&#x2F;multiple following statements would be removed? Because an initial statement usually has a higher probability of containing an initialization&#x2F;declaration function. (Rut) Why is the probability of inserting the first, second, etc. statement different? This is not the case with remove statements and change statements. (Jifeng) To mutate and generate test cases, the GA algorithm should have knowledge of the programming language constructs, fields &amp; methods of the software under test, etc. Does this require a significant engineering effort? (Udit) Accepting the Mutated OffspringThe coverage achieved by the Mutated Offspring is measured by the Test Suite Fitness Function. Conditions for accepting the mutated offspring: The coverage achieved by the Mutated Offspring exceeds that achieved by its parents, or is on par with that achieved by its parents, and that the mutated offspring are shorter. Their length do not exceed twice that of the Test Suite with the best coverage in the community. Accepting the Mutated OffspringQuestions: Are the parents removed before adding the children? (Rut) Compared with the single branch strategy, only the crossover is different, and the mutation is done in the same way. (Tarcisio) Bloat ControlA variable size representation could lead to bloat, where small negligible improvements in the fitness value are obtained with larger solutions. This is a very common problem in Genetic Programming. The following measures are used for bloat control: Limit the maximum number $N$ of test cases within a test suite and the maximum number of statements $L$ within a test case. (still need to choose comparatively larger $N$ and $L$ and then reduce their length during&#x2F;after the search to dramatically boost coverage) Crossover selection policy Mutated offspring acception policy Bloat ControlQuestions: Does coverage-guided fuzzing, which uses a variant of Genetic Programming, suffer from bloat? If so, could any measures be applied to solve this problem? (Jifeng) How to reduce the length during&#x2F;after the search? (Yayu, Jifeng) EvaluationEvoSuite is compared with the traditional single branch approach on top of EvoSuite infrastructure. Offspring is generated using the crossover function, but is conducted on two sequences of statements. Because there are dependencies between statements, the statements of the second part are appended one at a time, trying to satisfy dependencies with existing values, generating new values if necessary. The traditional approach level plus normalized branch distance fitness function is used. The two approaches are compared on five open source libraries and a subset of an industrial case study project previously used by Arcuri et al. The units are testable without complex interactions with external resources and are not multithreaded. Evaluation“Best practices” based on past experience are used for EvoSuite: Population size: 80 Maximum test suite size $N &#x3D; 100$ Maximum test case size $L &#x3D; 80$ The initial test suites are generated with 2 test cases each Initial probability for test case insertion: 0.1 Crossover probability: 3 &#x2F; 4 Initial probability for statement insertion: 0.5 EvaluationThe search operators for test cases make use of only the type information in the test cluster, and so difficulties can arise when method signatures are imprecise. To overcome this problem for container classes, we always put Integer objects into container classes, and cast returned Object instances back to Integer. As the length of test cases can vary greatly and longer test cases generally have higher coverage, we decided to take the number of executed statements as execution limit. The search is performed until either a solution with 100% branch coverage is found, or $k &#x3D; 1,000,000$ statements have been executed as part of the fitness evaluations. EvaluationQuestions: Why not compare EvoSuite to any other (non genetic-testing based) approach? (Zack) Why “the units are testable without complex interactions with external resources and are not multithreaded”? (Marie) Is there a justification for these “best practices”? (Praveen, Kevin, Madonna, Jifeng) Do the “best practices” overfit the 5 open-source libraries? (Joyce) Why the choice of an Integer? And does it work in practice? Given that the internals of the program might be expecting something else? (Rut) Results Whole test suite generation achieves higher coverage than single branch test case generation. Whole test suite generation produces smaller test suites than single branch test case generation. ResultsQuestions: While we have focused on branch coverage in this paper, the findings also carry over to other test criteria is an unwarranted extrapolation. (Zack) Evosuite claims that the test cases are smaller, but how much smaller? (not obvious from Figure 7) (ToTo) High coverage test suite does not necessary mean high bug-finding abilities. How does the performance compare to other tools? (ToTo, Praveen, Kevin, Madonna) The authors did not evaluate EvoSuite against a human in software engineering. Whether EvoSuite will improve the ability to test software from a software developer’s point of view is unknown. (Marie)","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Personal Website Design Considerations","slug":"Personal-Website-Design-Considerations","date":"2022-10-20T07:00:00.000Z","updated":"2023-02-26T16:51:38.634Z","comments":true,"path":"2022/10/20/Personal-Website-Design-Considerations/","link":"","permalink":"https://abbaswu.github.io/2022/10/20/Personal-Website-Design-Considerations/","excerpt":"","text":"HostingWe host our personal website on GitHub Pages, a static site hosting service. Considerations: No need to buy&#x2F;rent and set up infrastructure, such as Elastic Computing instances, Domain Name, Content Distribution Network, Load Balancer, DDoS protection Hosted directly from GitHub repository Our personal website meets its limitations: Non-commercial. No confidential information. Published GitHub Pages sites may be no larger than 1 GB. GitHub Pages sites have a soft bandwidth limit of 100 GB per month. GitHub Pages sites have a soft limit of 10 builds per hour. Implications: Static pages. Limit content of our personal website to text and lightweight multimedia, such as vector graphics and vector PDFs. Use raster graphics sparingly, and avoid heavyweight multimedia such as audio and video. Do not rebuild too frequently (&gt;10 builds per hour). FrameworkOur personal website uses the Hexo blog framework. Considerations: Support for GitHub Flavored Markdown. Easy-to-use CLI. One-command deploy to GitHub Pages. Support for two types of pages (Posts and Pages), adequate for a personal website. Huge library of spectacular, feature-packed and customizable themes. ThemeOur personal website uses the fluid theme for Hexo. Considerations: Appropriate Features Support for many third-party commenting systems. Mathjax support, renders equations like $E&#x3D;mc^2$. Mermaid support. Social network links. Extremely Detailed Documentation. Actively Maintained. Our Considerations When Writing Posts Make the Markdown file as self-contained as possible. This includes: Using third-party pictures from the Internet with stable URLs whenever possible. Utilize fluid’s support for Mermaid, and use Mermaid to describe and render in real-time diagrams such as Flowcharts, Sequence Diagrams, Class Diagrams, State Diagrams, and Mindmaps whenever possible, as opposed to including diagrams generated using other tools.","categories":[{"name":"Planning","slug":"Planning","permalink":"https://abbaswu.github.io/categories/Planning/"}],"tags":[]},{"title":"Paper Review: Feedback-Directed Random Test Generation","slug":"Paper-Review-Feedback-Directed-Random-Test-Generation","date":"2022-10-18T07:00:00.000Z","updated":"2022-12-18T22:20:14.436Z","comments":true,"path":"2022/10/18/Paper-Review-Feedback-Directed-Random-Test-Generation/","link":"","permalink":"https://abbaswu.github.io/2022/10/18/Paper-Review-Feedback-Directed-Random-Test-Generation/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. OverviewThe authors present Randoop, a feedback-directed random unit test generator for object-oriented programs which generates sequences of method calls that create and mutate objects, and uses feedback obtained from executing the sequences to guide the search towards new sequences. Logic of RandoopRandoop builds sequences incrementally starting from an empty set of sequences. In each iteration, it generates and executes a new sequence. Sequence GenerationFirst, it selects a method randomly among the public methods of classes. Second, it finds arguments to provide to the method. If an argument is a primitive type, select a primitive value from a fixed pool of values. If an argument is a reference type, select an extensible value of the corresponding type from a previously generated sequence in $nonErrorSeqs$ and put the previous generated sequence into a temporary list if possible, or select null otherwise. Third, a new sequence is formed by concatenating the sequences in the temporary list and the randomly selected method. Fourth, the new sequence is checked whether it has been generated before. If so, the process is repeated. Furthermore, the authors considers that repeated calls to a method may increase code coverage (e.g. reach code that increases the capacity of a container object, or reach code that goes down certain branches). Thus, with a probability $p &#x3D; 0.1$, instead of appending a single call of a chosen method, a maximum of $N &#x3D; 100$ calls are appended. Sequence ExecutionAfter a new sequence is generated, each method call in the sequence is executed, and after each call, contracts are checked. Default contracts checked by Randoop include: method throws no NullPointerException if no input parameter was null method throws no AssertionError o.equals(o) returns true and throws no exception o.hashCode() throws no exception o.toString() throws no exception If at least one contract is violated, the sequence is put in $errorSeqs$, and no values within the sequence can be extended. If all contracts are not violated, the sequence is put in $nonErrorSeqs$, and all values within the sequence are checked whether they can be extended. If the value has been encountered before, is null, or an exception occurs when executing the sequence leading to the value, the value cannot be extended. Experimental StudyThe authors evaluate the effectiveness of Randoop through three experiments. Comparing the basic block and predicate coverage of Randoop and five systematic input generation techniques on four container data structures used previously to evaluate these systematic input generation techniques. Comparing Randoop with JPF (a systematic testing technique) and undirected random testing on 14 widely-used libraries. A case study using Randoop to find regression errors between different implementations of the Java JDK. The experimental results strongly suggest that Randoop outperforms systematic and undirected random test generation in both coverage and error detection. Personal Thoughts In my opinion, a key advantage of Randoop is the “sparse, global sampling” that it performs, which “retains the benefits of random testing (scalability, simplicity of implementation)”, while avoiding undirected random testing’s pitfalls (generation of redundant or meaningless inputs), and is better adapted to large-scale library code than the “dense, local sampling” of systematic test generation. The sequences Randoop builds are akin to seeds in coverage-guided fuzzing, and I believe the efficiency and effectiveness of Randoop may be further boosted by applying a power schedule to the built sequences, much like applying a power schedule to the seeds in coverage-guided fuzzing. The built sequences could possibly have overlapping prefixes. Would using a tree structure be better than storing each sequence on its own? Randoop only supports a limited number of contracts, and its error-detection ability is rather weak. It may be appropriate on library code filled with assertions and checks, but may not work well on client code where these may be sparse.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Exploiting Dynamic Information in IDEs Improves Speed and Correctness of Software Maintenance Tasks","slug":"Paper-Review-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks","date":"2022-10-16T07:00:00.000Z","updated":"2022-12-18T17:41:34.052Z","comments":true,"path":"2022/10/16/Paper-Review-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks/","link":"","permalink":"https://abbaswu.github.io/2022/10/16/Paper-Review-Exploiting-Dynamic-Information-in-IDEs-Improves-Speed-and-Correctness-of-Software-Maintenance-Tasks/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. SummaryThe pervasive use of inheritance, interfaces, and runtime polymorphism in object-oriented software systems leads to it being unclear which concrete method is invoked at a call site. Modern IDEs such as Eclipse offer static views of the source code, but provide little help in revealing the execution paths an object-oriented software system actually takes at runtime. In this situation, developers usually resort to debuggers or profilers. However, the information extracted by such tools are volatile, and developers cannot continuously benefit from dynamic information integrated in the static source views in the IDE. To solve this problem, the authors propose Senseo, an Eclipse plugin that collects dynamic information by running unit and&#x2F;or system tests of the project with a customized JVM, that enriches the source views of Eclipse with dynamic information, such as: which concrete methods a particular method invokes, and how often which methods invoke this particular method how many objects are allocated in methods the dynamic collaborations between different source artifacts a visualization of the system’s Calling Context Tree These are displayed in tooltips, ruler columns, the Package Explorer, and a dedicated Collaboration Overview. The authors conducted an experiment with 30 professional Java developers solving five typical software maintenance tasks in JEdit, an unfamiliar, medium-sized software system, measured the time and correctness of the tasks, and conducted statistical tests on the measurements. Senseo yields a significant decrease in time of 17.5 percent and a significant increase in correctness of 33.5 percent, which validates the practical usefulness of Senseo. Personal ThoughtsThere is no doubt that the idea of enriching the source views of an IDE with dynamic information, as well as its implementation Senseo, is of great practical value to developers writing object-oriented software systems. However, I do have a few concerns after reading the paper. To enrich the source views of Eclipse with dynamic information, Senseo runs unit and&#x2F;or system tests of the project with a customized JVM. There are several concerns here. The project should have unit and&#x2F;or system tests that thoroughly exercise all units in a manner resembling an actual execution of the project in production, otherwise, the dynamic information for some units may be missing and&#x2F;or inaccurate. The unit and&#x2F;or system tests should be self-contained and not rely on interacting with the environment, such as getting input from the user, using OS services, etc. If so, a possible remedy would be to carve unit tests from such executions. There is significant overhead in the process of collecting dynamic information. As the authors have reported: “On average (geometric mean), CCT creation alone causes an overhead of factor 2.68. CCT creation and collection of dynamic information result in an overhead of factor 9.07. The total overhead, including serialization&#x2F;transmission, is of factor 9.47.” Although the authors claim that “even though the overall overhead is high when gathering dynamic information, we do not consider this a major issue as the application does not need to run at productive speed while analyzing it”, this could be a problem for lengthy system tests, especially if units in the system tests are frequently modified, and new dynamic information has to be reacquired. Carving unit tests from such system tests would also be a possible remedy. Furthermore, aside from the idea and implementation of the tool, something else I appreciate and have learned from this paper is the experimental study, in which two measures, the time and correctness of the tasks, are selected, and statistical tests on the measurements are conducted. This convincingly proves the effectiveness of Senseo.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: A Practical Guide for Using Statistical Tests to Assess Randomized Algorithms in Software Engineering","slug":"Paper-Review-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering","date":"2022-10-16T07:00:00.000Z","updated":"2022-12-18T22:20:14.429Z","comments":true,"path":"2022/10/16/Paper-Review-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering/","link":"","permalink":"https://abbaswu.github.io/2022/10/16/Paper-Review-A-Practical-Guide-for-Using-Statistical-Tests-to-Assess-Randomized-Algorithms-in-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. OverviewThere are many problems in software engineering which are undecidable and use randomized algorithms, such as automated unit test generation, random testing, and search algorithms (including Genetic Algorithms). As the outcomes of these randomized algorithms vary greatly from run to run, assessing their effectiveness is an important topic. To uncover whether randomized algorithms are properly assessed in software engineering research, the authors conducted a small-scale systematic review on three representative software engineering venues, namely IEEE Transactions of Software Engineering (TSE), IEEE International Conference on Software Engineering (ICSE) and International Symposium on Search Based Software Engineering (SSBSE), in the year 2009. The review shows that the analyses “are either missing, inadequate, or incomplete”, and “randomness is not properly taken into account”. The authors then put forward guidelines for properly assessing randomized algorithms in software engineering research. DefinitionsCensoring: a condition in which only the range (i.e. above a certan value, below a certain value, within an interval) of a measurement or observation is known, and its precise value is unknown.: Akin to clamping in saturated arithmetic.Commonly encountered in software engineering experiments when time limits are used.Assessment Procedure and GuidelinesA novel randomized algorithm is commonly compared against an existing technique. After determing a measure to compare (e.g. source code coverage, execution time), we should run both algorithms a large enough number of times independently (the author recommends “a very high number of runs” and not the rule of thumb of $n &#x3D; 30$ in medicine and behavioral science, as human aspects are not involved. With the collected measure data, we conduct the following: Statistical TestingWe use a statistical test to assess “whether there is enough empirical evidence to claim a difference between the two algorithms”. In such a statistical test, the null hypothesis is typically “there is no difference”, and we verify whether we should reject the null hypothesis. Definitions related to Statistical TestingThere are two conflicting types of error when performing statistical testing: (I) we reject the null hypothesis when it is true, and (II) we accept the null hypothesis when it is false. The p-value of a statistical test is the probability of rejecting the null hypothesis when it is true. The significant level $\\alpha$ of a statistical test is the highest p-value we accept for rejecting the null hypothesis. There is a tradition of using $\\alpha &#x3D; 0.05$ in the natural sciences. However, an increasing number of researchers believe that, and the author endorses that, such thresholds are arbitrary, and that researchers should “simply report p-values and let the reader decide in context”. The statistical power of a statistical test is the probability of rejecting the null hypothesis when it is false. Selection of Statistical TestIn different statistical tests, different probability distributions of the collected measures are assumed, and different aspects of the probability distributions of the collected measures are being compared. Common statistical tests include: parametric Student’s t-test Welch’s t-test F-test ANOVA nonparametric Fisher exact test Wilcoxon signed ranks test Mann-Whitney U-test When selecting a statistical test, tt is worth paying attention to the probability distributions of the collected measures: there may be a “very strong departure from normality” the mean and variance may not exist the data may be censored Effect Size MeasurementIn addition to using a statistic test to assess improvement of one algorithm over another, it is also critical to assess “the magnitude of the improvement”, for which effect size measures are used. Unstandardized effect size measures: dependent on the unit of measurement difference in mean Standardized effect size measures: d family &#x2F; Mahalanobis distance, assumes the normality of the data Common Language (CL) Statistic. The probability that a randomly selected score from the first population $X_1$ is greater than a randomly selected score from the second population $X_2$, $P(X_1 &gt; X_2)$. Measure of Stochastic Superiority. A generalization of Common Language Statistic, $A_{12} &#x3D; P(X_1 &gt; X_2) + 0.5 P(X_1 &#x3D; X_2)$. Recommended. Odds ratio. A measure of “how many times greater the odds are that a member of a certain population will fall into a certain category than the odds are that a member of another population will fall into that category”. If the total number of runs is $n$, and the number of times two algorithms find optimal solutions are $n_1$ and $n_2$, then the odds ratio is $\\psi &#x3D; \\frac{\\frac{n_1}{n - n_1}}{\\frac{n_2}{n - n_2}}$. Recommended. Multiple Statistical Tests and Effect Size MeasurementsWhen comparing $k$ algorithms, we frequently would like to know the performance of each algorithm “compared against all other alternatives individually”. This incurs $\\frac{k (k - 1)}{2}$ comparisons. However, when doing multiple stastical tests, given a significant level $\\alpha$ and the number of tests $n$, the probability that at least one null hypothesis is true is $1 - {(1 - \\alpha)}^n$, which converges to $1$ as $n$ increases. A remedy is the Bonferroni adjustment, in which we use an adjusted significant level $\\frac{\\alpha}{n}$. However, this has been “seriously criticized in the literature”, and the author recommends “simply report p-values and let the reader decide in context” instead.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Breaking the Barriers to Successful Refactoring: Observations and Tools for Extract Method","slug":"Paper-Review-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method","date":"2022-10-13T07:00:00.000Z","updated":"2022-12-18T17:36:14.449Z","comments":true,"path":"2022/10/13/Paper-Review-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method/","link":"","permalink":"https://abbaswu.github.io/2022/10/13/Paper-Review-Breaking-the-Barriers-to-Successful-Refactoring-Observations-and-Tools-for-Extract-Method/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. Refactoring is important to software development. Performing a refactoring is not trivial, for which refactoring tools have been developed. Nevertheless, programmers do not use refactoring tools as often as they could. To investigate this problem, the authors focus on one type of refactoring and one specific tool - the Extract Method tool in the Eclipse IDE. Fowler reports that Extract Method is “one of the most common refactorings”, “a key refactoring” which if successful, means “you can go on [to do] more refactorings”. The Extract Method tool in the Eclipse IDE it is a mature, non-trivial refactoring tool. Most refactoring tool user interfaces are very similar. The authors first conject tools are non-specific and unhelpful in diagnosing problems, and undertake a formative study observing 11 programmers perform a number of Extract Method refactorings on several large, open-source projects, which suggest that programmers fairly frequently encounter a variety of errors arising from violated refactoring preconditions. The authors further conjecture error messages were conflated, insufficiently descriptive, and discouraged programmers from refactoring, and built three visualization tools within the Eclipse IDE as solutions. Then, they conducted a study to assess whether or not the new tools overcome these usability problems by comparing the accuracy and time to complete refactoring tasks with and without the new tools, and administered a post-test questionnaire for the subjects to express their preferences. The results of the study were very positive, and subjects found the new tools superior and helpful outside of the context of the study. Finally, the authors provide recommendations for future tools. Code Selection: A selection tool should be lightweight, task-specific, and help the programmer overcome unfamiliar&#x2F;unusual code formatting. Displaying Violated Preconditions: quickly comprehensible, indicate location, easily distinguishable from warnings and advisories, display amount of work required, display relations between precondition violations, distinguish different types of violations. The experimental study is very concise, and there are many aspects that can be borrowed. Undertaking a formative study to verify conjections about problems within current tools, before building new tools based on the verified conjections, and evaluating them. The visualization comparing the the accuracy and time of each participant to complete refactoring tasks with and without the new tools is accurate and straightforward. Using a questionnaire to acquire subjective feedback complimentary to an objective evaluation. However, there are still some flaws. Only one type of refactoring (Extract Method) and one specific tool was considered. The takeaways may not apply to other types of refactoring. Several key variates were not controlled in the formative study, such as participants were free to refactor whatever code they thought necessary. Future directions of work include: Replicating the study for other types of refactoring. Build and assess new refactoring tools with increased usability.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: QSYM: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing","slug":"Paper-Review-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing","date":"2022-10-11T07:00:00.000Z","updated":"2022-12-18T22:20:14.438Z","comments":true,"path":"2022/10/11/Paper-Review-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing/","link":"","permalink":"https://abbaswu.github.io/2022/10/11/Paper-Review-QSYM-A-Practical-Concolic-Execution-Engine-Tailored-for-Hybrid-Fuzzing/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? How was it addressed by prior work?There are two notable technologies to automatically find vulnerabilities in software: Coverage-guided fuzzing, quickly explores the input space, but only good at discovering inputs leading to an execution path with loose branch conditions Concolic execution, good at finding inputs driving the program into tight and complex branch conditions, but very expensive to formulate and solve constraints A hybrid approach, hybrid fuzzing, was recently proposed. The fuzzer will quickly explore trivial input spaces (loose conditions) The concolic execution will solve the complex branches (tight conditions) Still suffer from scaling to find real bugs in real-world applications. Bottlenecks are their concolic executors. The symbolic emulation is too slow in formulating path constraints, and it is often not even possible to generate constraints due to incomplete and erroneous environment models. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you?Concolic executors adopt IR in their symbolic emulation. Although IR makes implementation easy, it incurs additional overhead and blocks further optimization. According to our measurement with real-world software, only 30% of instructions require symbolic execution. This implies an instruction-level approach has an opportunity to reduce the number of unnecessary symbolic executions. Concolic execution engines use snapshot techniques to reduce the overhead of re-executing a target program when exploring its multiple paths. However, in hybrid fuzzing, test cases from the fuzzer are associated with greatly different paths, rendering snapshoting inefficient. Furthermore, snapshots cannot reflect external status, and solving this problem through full system concolic execution or external environment modeling is expensive and&#x2F;or inaccurate. Concolic execution tries to guarantee soundness by collecting complete constraints. However, this can be expensive, and also over-constrain a path, limiting finding future paths. To solve these problems, Qsym uses Intel Pin along with a coverage-guided fuzzer: Get input test cases and validate newly produced test cases (potentially unsound) from the fuzzer. Employ instruction-level taint tracking, and only symbolically execute tainted instructions. Generate more relaxed (incomplete) forms of constraints that can be easily solved (can result in unsound test cases, but quickly checked with fuzzer). Fast execution makes re-execution much preferable to snapshoting for repetitive concolic testing. Considers external environments as “black-boxes” and simply executes them concretely (can result in unsound test cases, but quickly checked with fuzzer). Chooses the last constraint of a path for optimistic solving. It typically has a very simple form, and avoids solving irrelevant constraints repeatedly tested by fuzzers. This can be applied to other domains to speed up symbolic execution, if the domain has an efficient validator like a fuzzer. If a basic block has been executed too frequently in a context (a call stack of the current execution), Qsym stops generating further constraints from it. Extremely suitable for loops. This can directly be applied to other concolic executors as a heuristic path exploration strategy. How are those innovations evaluated? How does the paper’s evaluation match with the proposed problem statement?A series of experiments are conducted. To highlight the effectiveness, we applied QSYM to non-trivial programs that are large in size and well-tested - all applications and libraries tested by OSS-Fuzz. To show how effectively our concolic executor can assist a fuzzer in discovering new code paths, we measured the achieved code coverage during the fuzzing process using Qsym and AFM with a varying number of input seed files. We selected libpng as a fuzzing target because it contained various narrow-ranged checks. To show the performance benefits of QSYM’s symbolic emulation, we used the DARPA CGC dataset to compare QSYM with Driller, which placed third in the CGC competition. To evaluate the effect of optimistic solving, we compared Qsym with others using the LAVA dataset, a test suite that injects hard-to-find bugs in Linux utilities to evaluate bug-finding techniques. To show the effect of basic block pruning, we evaluated Qsym with and without this technique with four widely-used open-source programs - libjpeg, libpng, libtiff, and file. The author then analyzes new bugs found by Qsym. These experiments comprehensively assess different innovations and support the notion that Qsym “scales to find real bugs in real-world applications”. However, I do have some questions concerning the experimental study, stated below. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper?Qsym generates more relaxed (incomplete) forms of constraints that can be easily solved. Specifically how this is done is not clear. Questions concerning the experimental study: The experiments “to highlight the effectiveness” and “to show the performance benefits of QSYM’s symbolic emulation” seem to be redundant. To show how effectively our concolic executor can assist a fuzzer in discovering new code paths, we compared Qsym with AFM on libpng, because it contained various narrow-ranged checks. The benchmark appears to be cherry-picked. This is also the case with “to show the effect of basic block pruning”. Why are completely different datasets used in different experiments? Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper?The coverage-guided fuzzer used within Qsym is “vanilla” AFL. Other coverage-guided fuzzers exist that enhance AFL. How Qsym can complement these fuzzers can be a direction for future research. Unlike other IR-based executors, QSYM cannot test programs targeting other architectures. We plan to overcome this limitation by improving QSYM to work with architecture specifications, rather than a specific architecture implementation. (Is taint analysis on IR+JIT also a possible solution?) QSYM currently supports only memory, arithmetic, bitwise, and vector instructions. Other instructions, including floating-point operations, remain to be supported.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Semantic Fuzzing with Zest","slug":"Paper-Review-Semantic-Fuzzing-with-Zest","date":"2022-10-11T07:00:00.000Z","updated":"2022-12-18T22:20:14.438Z","comments":true,"path":"2022/10/11/Paper-Review-Semantic-Fuzzing-with-Zest/","link":"","permalink":"https://abbaswu.github.io/2022/10/11/Paper-Review-Semantic-Fuzzing-with-Zest/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled?The paper tackles the problem of generating random, syntactically valid inputs to exercise various code paths in the semantic analysis stages of programs and leveraging feedback to generate new inputs via mutations. How was it addressed by prior work?On one hand, QuickCheck-like random-input generators allow generating random, syntactically valid inputs. On the other hand, coverage-guided fuzzing tools such as AFL and libFuzzer randomly mutate known byte sequences to produce new byte sequences, and if the mutated byte sequences lead to new code coverage in the test program, they are saved for subsequent mutation. What are the innovation(s) proposed in this paper?The paper proposes Zest, a technique for automatically guiding QuickCheck-like random-input generators to exercise various code paths in the semantic analysis stages of programs. It first converts a QuickCheck-like random-input generator to a parametric generator, which can generate a syntactically valid input from a byte sequence. It then uses a coverage-guided fuzzing technique with the parametric generator in order to produce syntactically valid input that can increase code coverage in the semantic analysis stages. How are those innovations evaluated? How does the paper’s evaluation match with the proposed problem statement?The authors integrated Zest into the open-source JQF framework and evaluated Zest on five real-world Java benchmarks, comparing it to QuickCheck and AFL. They evaluated the three techniques on two fronts: The amount of code coverage achieved in the semantic analysis stage after a fixed amount of time. Their effectiveness in triggering bugs in the semantic analysis stage. QuickCheck and Zest make use of generators for synthesizing syntactically valid input, and do not exercise code paths corresponding to parse errors in the syntax analysis stage. In contrast, AFL performs mutations directly on raw input strings, and spends most of its time testing error paths within the syntax analysis stages. The experimental results suggest that when given QuickCheck-like random-input generators, Zest excels at exercising semantic analyses and is very effective at discovering semantic bugs. The paper’s evaluation matches well with the proposed problem statement, as the experimental design accurately assesses factors directly correlated with the problem of “generating random, syntactically valid inputs to exercise various code paths in the semantic analysis stages of programs and leveraging feedback to generate new inputs via mutations”, and the experimental results support the effectiveness of the proposed approach. Which technical innovations are most compelling to you?The most compelling technical innovation is Zest’s design of generating a syntactically valid input from a byte sequence given a QuickCheck-like random-input generator, by using bytes from the byte sequence to “fill in” randomly generated primitive data types of various length (bool, char, int, etc.) required within the random-input generator. This allows bit-level mutations on byte sequences to correspond to high-level structural mutations in the space of syntactically valid inputs, enabling Zest to leverage the mature coverage-guided fuzzing algorithm originally designed for byte sequence inputs. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper?The author states that the Zest algorithm “extends the CGF algorithm by keeping track of the coverage achieved by semantically valid inputs”, and that “we hypothesize that this biases the search towards generating even more valid inputs and in turn increases code coverage in the semantic analysis stage”. However, how semantically valid inputs are used is not stated in the description of the algorithm. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome?Zest assumes the availability of QuickCheck-like random-input generators to exercise the semantic analysis classes and find semantic bugs, which may be unavailable for specialized data structures. There has also been some recent interest in automatically generating input grammars from existing inputs, using machine learning and language inference algorithms. These techniques are complementary to Zest - the grammars generated by these techniques could be transformed into parametric generators for Zest.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: How We Refactor, and How We Know It","slug":"Paper-Review-How-We-Refactor-and-How-We-Know-It","date":"2022-10-10T07:00:00.000Z","updated":"2022-12-18T17:48:22.992Z","comments":true,"path":"2022/10/10/Paper-Review-How-We-Refactor-and-How-We-Know-It/","link":"","permalink":"https://abbaswu.github.io/2022/10/10/Paper-Review-How-We-Refactor-and-How-We-Know-It/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? How was the work validated?In his book on refactoring, Fowler catalogs 72 different refactorings, ranging from localized changes to more global changes, and Fowler claims that refactoring produces significant benefits. Although case studies have demonstrated that refactoring is a common practice and can improve code metrics, they tend to examine just a few software products. To help put refactoring research on a sound scientific basis, we replicate the study in wider contexts and explore factors that previous authors may not have explored. We analyze four sets of Eclipse IDE usage data and apply different several different refactoring-detection strategies to them. We then use this data to test nine hypotheses about refactoring, casting doubt on several previously stated assumptions about how programmers refactor, while validating others. Refactoring behavior of refactoring tool developers differs from that of their users. Specifically, RENAMEs and MOVEs are more frequent among users. About 40% of refactorings performed using a tool occur in batches (several refactorings of the same kind within a short time period). About 90% of configuration defaults of refactoring tools remain unchanged when programmers use the tools. messages written by programmers in commit logs do not reliably indicate the presence of refactoring. Programmers frequently floss refactor (interleave refactoring with other types of programming activity). About half of refactorings are not high-level, so refactoring detection tools that look exclusively for high-levelrefactorings will not detect them. Refactorings are performed frequently. Almost 90% of refactorings are performed manually, and the kinds of refactorings performed with tools differ from the kinds performed manually. How could this research be extended? How could this research be applied in practice?For the toolsmith: Most kinds of refactorings will not be used as frequently as the toolsmiths hoped. Improving the under-used tools or their documentation may increase tool use. Programmers often do not configure refactoring tools. Configuration-less refactoring tools, which have recently seen increasing support in Eclipse and other environments, will suit the majority of, but not all, refactoring situations. 30 refactorings did not have tool support, the most popular of these was MODIFY ENTITY PROPERTY, performed 8 times, which would allow developers to safely modify properties such as static or final. For researchers: Questions still remain to answer. Why is the RENAME refactoring tool so much more popular than other refactoring tools? Why do some refactorings tend to be batched while others do not? Our experiments should be repeated in other projects and for other refactorings to validate our findings. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you?Of particular interest to me is the inspiration for the hypothesis the authors verify - previous literature (frequently in other software engineering domains), personal experience, anecdotes from programmers, surveys. The benefit from this is twofold. First, it provides a source of inspiration for formulating hypotheses. Second, it endorses the validity of the hypotheses. We hypothesize refactoring behavior of refactoring tool developers differs from that of their users. Toleman and Welsh assume a variant of this hypothesis - that the designers of software tools erroneously consider themselves typical tool users - and argue that the usability of software tools should be objectively evaluated. We hypothesize that programmers typically perform refactoring in batches. Based on personal experience and anecdotes from programmers, we suspect that programmers often refactor several pieces of code because several related program elements may need to be refactored in order to perform a composite refactoring. In previous research, Murphy-Hill and Black built a refactoring tool that supported refactoring several program elements at once, on the assumption that this is common. We hypothesize that programmers do not often configure refactoring tools. We suspect this because tweaking code manually after the refactoring may be easier than configuring the tool. In the past, we have found some limited evidence that programmers perform only a small amount of configuration of refactoring tools. When we did a small survey in September 2007 at a Portland Java Users Group meeting, 8 programmers estimated that, on average, they supply configuration information only 25% of the time. In Xing and Stroulia’s automated analysis of the Eclipse codebase, the authors conclude that “indeed refactoring is a frequent practice”. Although flawed, this becomes one of the authors’ hypotheses. Furthermore, some hypotheses are formed from a critique of previous literature, combined with domain expertise and&#x2F;or other literature. Several researchers have used messages attached to commits into a version control as indicators of refactoring activity. However, we hypothesize that this assumption is false, because refactoring may be an unconscious activity, and because the programmer may consider it subordinate to some other activity, such as adding a feature. Past research has often drawn conclusions based on observations of high-level refactorings. We hypothesize that in practice programmers also perform many lower-level refactorings. We suspect this because lower-level refactorings will not change the program’s interface and thus programmers may feel more free to perform them. Additionally, much of the methodology presented in this paper can be borrowed. The fourth dataset used by the authors is Eclipse CVS, the version history of the Eclipse and JUnit code bases extracted from their Concurrent Versioning System (CVS) repositories. CVS does not maintain records showing which file revisions were committed as a single transaction. The standard approach for recovering transactions is to find revisions committed by the same developer with the same commit message within a small time window; we use a 60 second time window. In our experiments, we randomly sampled from about 3400 source file commits that correspond to the same time period, the same projects, and the same developers represented in Toolsmiths. Using these data, two of the authors inferred which refactorings were performed by comparing adjacent commits manually. Ratzinger describes the most sophisticated strategy for finding refactoring messages: searching for the occurrence of keywords such as “move” and “rename”, and excluding “needs refactoring”. We replicated Ratzinger’s experiment for the Eclipse code base to nullify Ratzinger’s conclusions. In order for refactoring activity to be defined as frequent, we seek to apply criteria that require refactoring to be habitual and occurring at regular intervals. First, we examined the Toolsmiths data to determine how refactoring activity was spread throughout development. Second, we examined the Users data to determine how often refactoring occurred within a programming session and whether there was significant variation among the population. We hypothesize that programmers often do not use refactoring tools, because existing tools may not have a sufficiently usable user-interface. To validate this hypothesis, we correlated the refactorings that we observed by manually inspecting Eclipse CVS commits with the refactoring tool usages in the Toolsmiths data set.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: \"Cloning Considered Harmful: Considered Harmful","slug":"Paper-Review-Cloning-Considered-Harmful-Considered-Harmful","date":"2022-10-05T07:00:00.000Z","updated":"2022-12-18T17:36:02.916Z","comments":true,"path":"2022/10/05/Paper-Review-Cloning-Considered-Harmful-Considered-Harmful/","link":"","permalink":"https://abbaswu.github.io/2022/10/05/Paper-Review-Cloning-Considered-Harmful-Considered-Harmful/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How was the work validated?Current literature on the topic of duplicated code in software systems often considers duplication harmful to the system quality, and the reasons commonly cited for duplicating code often have a negative connotation. While these positions are sometimes correct, during our case studies we have found that this is not universally true, and we have found several situations where code duplication seems to be a reasonable or even beneficial design option. This paper introduces eight cloning patterns that we have uncovered during case studies on large software systems, and discusses the advantages and disadvantages associated with using them. Forking, cloning used to bootstrap development of similar solutions, with the expectation that evolution of the code will occur somewhat independently Hardware variation Platform variation Experimental variation Templating, directly copy behavior of existing code but appropriate abstraction mechanisms are unavailable Boiler-plating due to language in-expressiveness API&#x2F;Library protocols General language or algorithmic idioms Customization, currently existing code does not adequately meet anew set of requirements Bug workarounds Replicate and specialize What were the main contributions of the paper as you (the reader) see it? How does this work move the research forward? How could this research be extended?This paper introduces the notion of categorizing high level patterns of cloning in a similar fashion to the cataloging of design patterns or anti-patterns. There are several benefits that can be gained from this characterization. It provides a flexible framework on top of which we can document our knowledge about how and why cloning occurs in software. This documentation crystallizes a vocabulary that researchers and practitioners can possibly use to communicate about cloning. This categorization is a first step towards formally defining these patterns to aid in automated detection and classification. These classifications can then be used to define metrics concerning code quality and maintenance efforts. Automatic classifications will also provide us with better measures of code cloning in software systems and severity of the problem in general. How could this research be applied in practice?In each uncovered cloning pattern, the author describes its advantages, disadvantages, how it can be managed, issues to be aware of when deciding to use it as a long-term solution, as well as real examples in large software systems. These provide practical guidelines when considering a trade-off between code cloning and formulating abstractions for code reuse, as well as how to manage code cloning should it be used, when developing a software project.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: Go To Statement Considered Harmful","slug":"Paper-Review-Go-To-Statement-Considered-Harmful","date":"2022-10-04T07:00:00.000Z","updated":"2022-12-18T17:43:17.010Z","comments":true,"path":"2022/10/04/Paper-Review-Go-To-Statement-Considered-Harmful/","link":"","permalink":"https://abbaswu.github.io/2022/10/04/Paper-Review-Go-To-Statement-Considered-Harmful/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. The author has been familiar with the observation that the quality of programmers is a decreasing function of the density of go to statements in the programs they produce, and in this paper, he explains why the use of the go to statement has negative effects. He first remarks that the process taking place under control of the program, instead of the program itself, is the true subject matter of a programmer’s activity, and it is this process whose behavior has to satisfy the desired specifications. He then argues that our intellectual powers can better master static relations than visualize processes evolving in time, for which reason we should shorten the conceptual gap between the static program and the dynamic progress. The author continues characterizing the progress of a progress, explaining that it can be uniquely characterized by a mixed sequence of textual and&#x2F;or dynamic indices, when conditionals, procedures, and repetition clauses are considered. However, the unbridled use of the go to statement has an immediate consequence that it becomes terribly hard to find a meaningful set of coordinates in which to describe the process progress, which will in turn “make a mess of one’s program”. However, in my opinion, although the go to statement is considered harmful, abolishing the go to statement from all “higher level” programming languages is an overstatement. As the author himself stated: The exercise to translate an arbitrary flow diagram more or less mechanically into a jump-less one, is not to be recommended. Then the resulting flow diagram cannot be expected to be more transparent than the original one. There exist situations where an “arbitrary flow diagram” has to be implemented (especially when implementing Finite-State Machines in lexers, regex engines, and protocols), and in these situations, implementing the flow diagram using go to statements is much more direct, straightforward and easier to reason about (not to mention more efficient) than mashing up structured programming constructs.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: KLEE: unassisted and automatic generation of high-coverage tests for complex systems programs","slug":"Paper-Review-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs","date":"2022-10-04T07:00:00.000Z","updated":"2022-12-18T22:20:14.437Z","comments":true,"path":"2022/10/04/Paper-Review-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs/","link":"","permalink":"https://abbaswu.github.io/2022/10/04/Paper-Review-KLEE-unassisted-and-automatic-generation-of-high-coverage-tests-for-complex-systems-programs/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled? How was it addressed by prior work?Many classes of errors are difficult to find without executing a piece of code. The importance of such testing, combined with the difficulty and poor performance of random and manual approaches, has led to much work in using symbolic execution to automatically generate test inputs. It has been an open question whether the approach has any hope of consistently achieving high coverage on real applications, facing the challenges in handling code that interacts with the environment, and the exponential number of paths through code. Traditional symbolic execution systems either cannot handle programs interacting with the environment or require a complete working model. More recent work in test generation does allow external interactions, but forces them to use entirely concrete procedure call arguments, which limits the behaviors they can explore. For the path explosion problem, search strategies proposed in the past include Best First Search, Generational Search, and Hybrid Concolic Testing. Orthogonal to search heuristics, researchers have addressed the path explosion problem by testing paths compositionally, and by tracking the values read and written by the program. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you?KLEE interprets programs compiled to LLVM IR, and typically requires no source modification. It functions as a hybrid between an operating system for symbolic processes and an interpreter. Each symbolic process has a register file, stack, heap, program counter, and path condition. Unlike a normal process, storage locations for a symbolic process - registers, stack and heap objects - refer to expression trees instead of raw data values. The leaves of an expression are symbolic variables or constants, and the interior nodes come from LLVM IR operations. Conditional branches take a boolean expression and alter the instruction pointer of the symbolic process based on whether the condition is true or false. KLEE queries the constraint solver to determine if the branch condition is either provably true or false along the current path. If so, the instruction pointer is updated to the appropriate location. Otherwise, both branches are possible. KLEE forks the symbolic process so that it can explore both paths. The number of forked symbolic processs grows quite quickly in practice. KLEE implements the heap as an immutable map, and portions of the heap structure itself can also be shared amongst multiple symbolic processs. Additionally, this heap structure can be forked in constant time, which is important given the frequency of this operation. Potentially dangerous operations implicitly generate branches that check if any input value exists that could cause an error. For example, a division instruction generates a branch that checks for a zero divisor. If so, KLEE solves the current path’s constraints to produce a test case that will follow the same path when rerun on an unmodified version of the checked program, and terminates the current symbolic process. KLEE will then continue execution on the false path, which adds the negation of the check as a constraint (e.g., making the divisor not zero). The core of KLEE is an interpreter loop which selects a symbolic process to run and then symbolically executes a single instruction in the context of that symbolic process. Given more than one symbolic process, KLEE must pick which one to execute first. KLEE selects the symbolic process to run at each instruction by uses each strategy in a round robin fashion. Random Path Selection: Use a binary tree to record the program path followed for all active symbolic processs. A symbolic process is selected by traversing this tree from the root and randomly selecting the path to follow at branch points. This strategy has two important properties. Favors symbolic processs high in the branch tree. They have less constraints on their symbolic inputs and have greater freedom to reach uncovered code. Avoids starvation when some part of the program is rapidly creating new symbolic processs (“fork bombing”) as it happens when a tight loop contains a symbolic condition. Coverage-Optimized Search: Select symbolic processs likely to cover new code in the immediate future using heuristics. This loop continues until there are no symbolic processs remaining, or a user-defined timeout is reached. KLEE ensures that a symbolic process which frequently executes expensive instructions will not dominate execution time by running each symbolic process for a “time slice” defined by both a maximum number of instructions and a maximum amount of time. KLEE uses STP as its constraint solver. KLEE maps every memory object in the checked code to a distinct STP array. This representation dramatically improves performance since it lets STP ignore all arrays not referenced by a given expression. Furthermore, there are tricks to simplify expressions and ideally eliminate queries before they reach STP, including: Expression Rewriting Constraint Set Simplification Implied Value Concretization Constraint Independence Counter-example Cache: Redundant queries are frequent, and a simple cache is effective at eliminating a large number of them. However, it is possible to build a more sophisticated cache due to the particular structure of constraint sets. The counter-example cache maps sets of constraints to counter-examples (i.e., variable assignments), along with a special sentinel used when a set of constraints has no solution. This mapping is stored in a custom data structure — derived from the UBTree structure of Hoffmann and Hoehler, which allows efficient searching for cache entries for both subsets and supersets of a constraint set. By storing the cache in this fashion, the counter-example cache gains three additional ways to eliminate queries. When a subset of a constraint set has no solution, then neither does the original constraint set. When a superset of a constraint set has a solution, that solution also satisfies the original constraint set. When a subset of a constraint set has a solution, it is likely that this is also a solution for the original set. KLEE handles the environment by redirecting library calls to models that understand the semantics of the desired action well enough to generate the required constraints. The real environment can fail in unexpected ways. Such failures can often lead to unexpected and hard to diagnose bugs. To help catch such errors, KLEE will optionally simulate environmental failures by failing system calls in a controlled manner. How are those innovations evaluated? How does the paper’s evaluation match with the proposed problem statement?Four sets of experiments are conducted. We do intensive runs to both get high coverage and find bugs on Coreutils and BusyBox tools, do a comparision with random tests and developer test suites, and discuss the bugs found. To demonstrate KLEE’s applicability to bug finding, we used KLEE to check all 279 BusyBox tools and 84 MINIX tools in a series of short runs. Thus far, we have focused on finding generic errors that do not require knowledge of a program’s intended behavior. We now show how to do much deeper checking, including verifying full functional correctness on a finite set of explored paths. We use KLEE to find deep correctness errors by cross-checking purportedly equivalent Coreutils and BusyBox tool implementations. We have also applied KLEE to checking non-application code by using it to check the HiStar kernel. We chose line coverage as reported by gcov as a conservative measure of KLEE-produced test case effectiveness, because it is widely-understood and uncontroversial. The results of the experiments are very positive, and convincingly prove the proposed problem statement. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper?Coverage-Optimized Search tries to select symbolic processs likely to cover new code in the immediate future. It uses heuristics to compute a weight for each symbolic process and then randomly selects a symbolic process according to these weights. How these heuristics work, which is critical for performance, is not symbolic processd, and remains unclear. KLEE ensures that a symbolic process which frequently executes expensive instructions will not dominate execution time by running each symbolic process for a “time slice” defined by both a maximum number of instructions and a maximum amount of time. Precisely how this “time slice” is calculated is also unclear. KLEE handles the environment by redirecting library calls to models that understand the semantics of the desired action well enough to generate the required constraints. These models are written in normal C code which the user can readily customize, extend, or even replace without having to understand the internals of KLEE. However, what “understand the semantics of the desired action well enough” means is unclear. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper?KLEE does not currently support symbolic floating point, longjmp, threads, and assembly code. Additionally, memory objectsare required to have concrete sizes. These block KLEE’s application towards floating point-heavy scientific computation and data science code, and may also limit KLEE to simple programming languages such as C, not supporting the numerous dynamics, including exception handling, within C++.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: CUTE: A Concolic Unit Testing Engine for C","slug":"Paper-Review-CUTE-A-Concolic-Unit-Testing-Engine-for-C","date":"2022-10-02T07:00:00.000Z","updated":"2022-12-18T22:20:14.433Z","comments":true,"path":"2022/10/02/Paper-Review-CUTE-A-Concolic-Unit-Testing-Engine-for-C/","link":"","permalink":"https://abbaswu.github.io/2022/10/02/Paper-Review-CUTE-A-Concolic-Unit-Testing-Engine-for-C/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. NOTE: I believe the paper to be written very obscurely, so I will explain the ideas of the paper in my own words. What is the problem being tackled? How was it addressed by prior work?Unit testing is a method for modular testing of a program’s functional behavior. Such testing requires specification of values for the inputs (or test inputs) to the unit. Manual specification of such values is labor intensive and cannot guarantee that all possible behaviors of the unit will be observed during the testing. Several techniques have been proposed to automatically generate values for the inputs. Randomly choose the values over the domain of potential inputs Many values may lead to the same behavior and are redundant. The probability of selecting inputs causing buggy behavior may be astronomically small. Symbolic Exection Addresses the problem of redundant executions and increases test coverage For large or complex units, it is intractable to maintain and solve the constraints required for test generation Incrementally generating test inputs by combining concrete and symbolic execution During a concrete execution, a conjunction of symbolic constraints along the path of execution is generated. These constraints are modified and then solved to generate further test inputs to direct the program along alternative paths. If it is not feasible to solve, simply substitute random concrete values. This problem is particularly complex for programs with dynamic data structures using pointer operations. Pointers may have aliases. In this paper, we provide a method for representing and solving approximate pointer constraints to generate test inputs. Our method is thus applicable to a broad class of sequential programs. What are the innovation(s) proposed in this paper? Which technical innovations are most compelling to you?We consider the execution of a function to be determined by all the stack variables, global variables, and heap objects it exercises. Only primitive types and pointer types are taken into consideration. For structures and arrays, each member is considered to be a separate variable. External OS services are not modelled. We associate the following properties with each stack variable, global variable, and heap object. Concrete Value Symbolic Value Concrete Address Symbolic Address The branches taken within an execution can be described with a predicate sequence called a path constraint. Each predicate is described using the aforementioned stack variables, global variables, and&#x2F;or heap objects. Symbolic values are used when available, otherwise, concrete values are used. Predicates involving primitive types are of the form $a_1 x_1 + \\dots + a_n x_n + cR0, R \\in {&lt;, &gt;, \\le, \\ge, &#x3D;, \\ne}$, where $a_i, \\dots, a_n, c$ are integer constants. (Essentially considers only linear combinations of primitive types) Predicates involving pointers are of the form $xRy$ or $xRNULL$, $R \\in {&#x3D;, \\ne}$. (Essentially considers only being able to assign to a pointer NULL or another previously known address, and does not allow converting integers to pointers) Running process of CUTE. while True: Execute, in the process: When allocating a stack variable, global variable, or heap object without initialization (incl. function parameters): Modify “known stack variables, global variables, and heap objects” if needed. If its concrete value has been stored, initialize it to its stored concrete value. Otherwise, generate a random concrete value for it. Record its concrete value and concrete address. When allocating a stack variable, global variable, or heap object with initialization: Modify “known stack variables, global variables, and heap objects” if needed. Record its concrete value and concrete address. Record its symbolic value and symbolic address. When assigning an existing stack variable, global variable, or heap object: Update its concrete value. Update its symbolic value. When taking a branch, add a new predicate to the path constraint. After execution, negate the last predicate within the path constraint, and solve for the concrete values of “stack variables, global variables, and heap objects allocated without initialization”. Update their recorded concrete values. Solving optimizations: Check if the last predicate is syntactically the negation of any preceding predicate Identify and eliminate common arithmetic subconstraints. Identify dependencies between predicates and exploit them. The path constraints from two consecutive concolic executions, $C$ and $C’$ differ only in a small number of predicates, and their respective solutions are similar. The solver collects all the predicates in C that are dependent on the negation of the last and solves for them. In practice, we have found that the size of this set is almost one eighth the size of $C$ on average. Generated random concrete values: Primitive Type: random number Pointer Type: NULL We next consider testing of functions that take data structures as inputs. We want to test such functions with valid inputs only. There are two main approaches to obtaining valid inputs: Generating inputs with call sequences Use the functions that check if an input is a valid data structure by solving them, i.e., generating input for which they return true. Previous techniques include a search that uses purely concrete execution and a search that uses symbolic execution for primitive data but concrete values for pointers. CUTE, in contrast, uses symbolic execution for both primitive data and pointers. This allows it to solve these functions asymptotically faster than the fastest previous techniques. How are those innovations evaluated? How does the paper’s evaluation match with the proposed problem statement?We illustrate two case studies that show how CUTE can detect errors. We applied CUTE to test its own data structures. Our goal in this case study was to detect memory leaks in addition to standard errors such as segmentation faults, assertion violation etc. We also applied CUTE to unit test SGLIB version 1.0.1, a popular, open-source C library for generic data structures. We chose SGLIB as a case study primarily to measure the efficiency of CUTE. We found two bugs in SGLIB using CUTE. The case studies showcase the power of CUTE’s concolic unit testing approach, and match well with the proposed problem statement. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper?After execution, negate the last predicate within the path constraint, and solve for the concrete values of “stack variables, global variables, and heap objects allocated without initialization”. A solving optimization that the author proposed is “identifing and eliminating common arithmetic subconstraints”. However, how this is done is not explained. Which problems remain unsolved after this paper? Do you foresee any barriers to the applicability of the technique proposed in the paper? For structures and arrays, each member is considered to be a separate variable. Although this facilicates analysis, this could incur significant overhead and impede scalability. External OS services are not modelled. Predicates involving primitive types are of the form $a_1 x_1 + \\dots + a_n x_n + cR0, R \\in {&lt;, &gt;, \\le, \\ge, &#x3D;, \\ne}$, where $a_i, \\dots, a_n, c$ are integer constants. This essentially considers only linear combinations of primitive types. The author shows preference to using the technique of “using the functions that check if an input is a valid data structure by solving them” to solve the problem of testing of functions that take data structures as inputs. However, such an approach may be impossible for object-oriented languages such as C++, in which data structures are encapsulated in classes, and the logic of validness is enforced with the constructor and public methods of the classes.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Selection and Presentation Practices for Code Example Summarization","slug":"Paper-Review-Selection-and-Presentation-Practices-for-Code-Example-Summarization","date":"2022-09-28T07:00:00.000Z","updated":"2022-12-18T19:44:11.204Z","comments":true,"path":"2022/09/28/Paper-Review-Selection-and-Presentation-Practices-for-Code-Example-Summarization/","link":"","permalink":"https://abbaswu.github.io/2022/09/28/Paper-Review-Selection-and-Presentation-Practices-for-Code-Example-Summarization/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? How could this research be applied in practice?Code examples are important in modern software development. As part of the first steps toward automatic source-to-source summarization, the authors studied how humans summarize examples to understand how to automate the process, and propose empirically-supported hypotheses justifying the use of specific practices. Selection Practices Practices Related to Language Constructs Practices Based on Query Term Practices Considering the Human ReaderPresentation Practices Trimming a Line When Needed Compressing a Large Amount of Code Truncating Code Formatting Code for Readability Improving Code The results provide a grounded basis for the development of code example summarization and presentation technology. How was the work validated?We chose a well-defined corpus of programming documents, The Official Android API Guides, which contains a mix of natural-language text and code fragments. We collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. We analyzed common practices behind these decisions across the hand-generated representations, as well as the rationale behind the practices. What were the main contributions of the paper as you (the reader) see it?In my opinion, aside from the obvious contributions of the paper presented by the author, there is a lot to learn from the study set-up and the conceptual framework for interpreting the results. To understand the rationale behind the practices, we instructed the participants to verbalize their thought process using the think-aloud protocol. We distinguished practices concerning the type of content selected and the way the content was presented in a summary, because even summaries with content associated with the same part of the original fragment could vary on how to present the summary. To make hypotheses justifying the use of different practices, we relied on a quantitative analysis of the distribution of each practice across code fragments and participants. In-lined histograms presents the distribution of observations of a given practice for the participants over the code fragments. This provides a convenient and compact assessment of the amount of evidence for a practice. Furthermore, the authors have borrowed a lot from related domains of research, including natural language generation, natural language summarization of code, etc. Some examples: The separation of content selection from presentation is typical in a natural language generation system. The comments demonstrated a number of different ways to abstract content, including aggregating lexically and aggregating semantically - natural language generation terminology. Seven participants injected additional natural language into the code summaries. This motivates a novel type of transformations that mix code and text. The only work we know of in this area is the natural summaries generated by Rastkar et al. This gives revelations on exploiting knowledge from related domains when doing our own research. How could this research be extended?The goal of the study was to inform the design of concise representations of source code and automatic summarization algorithms. A natural future direction is to implement these representations and algorithms, and conduct empirical studies assessing their usefulness in summarizing source code.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: Finding and Understanding Bugs in C Compilers","slug":"Paper-Review-Finding-and-Understanding-Bugs-in-C-Compilers","date":"2022-09-24T07:00:00.000Z","updated":"2022-12-18T22:20:14.436Z","comments":true,"path":"2022/09/24/Paper-Review-Finding-and-Understanding-Bugs-in-C-Compilers/","link":"","permalink":"https://abbaswu.github.io/2022/09/24/Paper-Review-Finding-and-Understanding-Bugs-in-C-Compilers/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled?Finding compiler bugs, especially bugs in the “middle end” of a compiler that performs transformations on an intermediate representation, to improve the quality of C compilers. How was it addressed by prior work?Compilers have been tested using randomized methods for nearly 50 years. In 1998, McKeeman coined the term “differential testing”. His work resulted in DDT, a family of program generators that conform to the C standard at various levels. However, DDT avoided only a small subset of all undefined behaviors, and only then during test-case reduction, not during normal testing. Thus, it is not a suitable basis for automatic bug-finding. Lindig used randomly generated C programs to find several compiler bugs related to calling conventions. His tests are self-checking, but far less expressive than Csmith. Sheridan also used a random generator to find bugs in C compilers. Sheridan’s tool produces self-checking tests. However, it is less expressive than Csmith and it fails to avoid undefined behavior such as signed overflow. Zhao et al. created an automated program generator for testing an embedded C++ compiler, which allows a general test requirement, such as which optimization to test, to be specified. What are the innovation(s) proposed in this paper?The paper proposes Csmith, a randomized test-case generation tool which generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. This advances the state of the art in compiler testing. Csmith supports compiler bug-hunting using differential testing. Csmith generates a C program, a test harness then compiles the program using several compilers, runs the executables, and compares the outputs. How are those innovations evaluated? How does the paper’s evaluation match with the proposed problem statement?The authors conducted five experiments. Finding and reporting bugs in a a variety of C compilers over a three-year period. They have found and reported more than 325 bugs in mainstream C compilers including GCC, LLVM, and commercial tools. Compiling and running one million random programs using several years’ worth of versions of GCC and LLVM, to understand how their robustness is evolving over time. Evaluating Csmith’s bug-finding power as a function of the size of the generated C programs. Comparing Csmith’s bug-finding power to that of fourprevious random C program generators. Investigating the effect of testing random programs on branch, function, and line coverage of the GCC and LLVM source code. The experiments thoroughly evaluate and demonstrate Csmith’s bug-finding power and provide guidelines for using Csmith to find bugs. Which technical innovations are most compelling to you?Csmith uses randomized differential testing. This has the advantage that no oracle for test results is needed. It exploits the idea that if one has multiple, deterministic implementations of the same specification, all implementations must produce the same result from the same valid input. When two implementations produce different outputs, one of them must be faulty. Given three or more implementations, a tester can use voting to heuristically determine which implementations are wrong. How Csmith designs the results used for differential testing is also worthwhile. A Csmith-generated program prints a value summarizing the computation performed by the program, which is implemented as a checksum of the program’s non-pointer global variables at the end of the program’s execution. Thus, if changing the compiler or compiler options causes the checksum emitted by a Csmith-generated program to change, a compiler bug has been found. Also compelling are the mechanisms that Csmith uses to avoid generating C programs that execute undefined behaviors or depend on unspecified behaviors, including performing incremental pointer and dataflow analysis in the process of generating programs. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper?In the process of randomly generating programs, Csmith randomly selects an allowable production from its grammar for the current program point. To make the choice, it consults a probability table and a filter function specific to the current point: there is a table&#x2F;filter pair for statements, another for expressions, and so on. The table assigns a probability to each of the alternatives, where the sum of the probabilities is one. However, how this probability table is constructed and maintained, which obviously is critical to generating high-quality random programs, is not stated in the paper, and requires clarification. Do you forsee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome? Which problems remain unsolved after this paper?The most important language features not currently supported by Csmith are strings, dynamic memory allocation, floating-point types, unions, recursion, and function pointers. These are language features that are ubiquitous in real-world programs, thus, not supporting them is a serious barrier to the applicability of Csmith. The authors plan to add some of these features to future versions of our tool. Although Csmith-generated programs allowed discovering bugs missed by compilers’ standard test suites, branch, function, and line coverage of the GCC and LLVM source code did not significantly improve compared to the compilers’ existing test suites. ‘Coverage-guided’ fuzzing may represent a future direction of research to discover more bugs lurking in unvisited sections of compiler source code.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges","slug":"Paper-Review-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges","date":"2022-09-22T07:00:00.000Z","updated":"2022-12-18T17:39:20.403Z","comments":true,"path":"2022/09/22/Paper-Review-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges/","link":"","permalink":"https://abbaswu.github.io/2022/09/22/Paper-Review-Do-Automatically-Generated-Unit-Tests-Find-Real-Faults-An-Empirical-Study-of-Effectiveness-and-Challenges/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How could this research be applied in practice?The paper conducts an empirical study of the effectiveness and challenges of automatically generated unit tests at finding real faults, and derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. Improving the obtained code coverage so that faulty statements are executed in the first instance. A high code coverage ratio does not necessarily indicate that the bug was covered. Improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, is also required. Improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time. How was the work validated?The authors applied three state-of-the art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. To account for randomness in test generation, we generated 10 test suites for each tool and fault. Tools may generate flaky tests, which may also fail on the fixed version. They are automatically removed. Even if a test is not flaky, it might still fail on the buggy version for reasons unrelated to the actual fault. Such false positives are identified. For each executed test, we collected information on whether it passed or failed, and the reason of failure. In order to study how code coverage relates to fault detection, we measured statement coverage, and also bug coverage - whether a fault was 1) fully covered (all modified statements covered), 2) partially covered (some modified statements covered), or 3) not covered. To gain insights on how to increase the fault detection rate of test generation tools, the authors did case studies on the challenges that prevent fault detection, and studied the root causes for flaky and false-positive tests. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? How could this research be extended?The revelations from the case studies supporting the primary contributions of the paper as the author sees it are particularly important, as they identify specific challenges and provide plausible solutions for increasing the fault detection rate of test generation tools. Creation of complex objects, such as a control flow graph, which often requires a certain sequence of prior method calls. Viable solutions include seeding objects observed at runtime, mining of common usage patterns of objects to guide object creation, or carving of complex object states from system tests. Complex strings satisfying a certain syntax. Search-based tools are capable in principle of generating string inputs, but doing so can take very long. Symbolic approaches using string solvers or dedicated solvers for regular expressions are generally restricted to fixed length strings. If an input grammar is known, this can be used to generate test data more efficiently. Complex conditions which randomly initialized inputs are unlikely to satisfy. Dynamic symbolic execution would not suffer from this problem. Errors are not propagated. To some extent, this is the result of focusing on simple structural criteria such as branch coverage, rather than aiming to exercise more complex intra-class data flow dependencies. Environmental dependencies and dependencies on the static state of the system under test resulting in flaky tests. Aggressive mocking, which monitors and asserts on the internal state (e.g. the order of method calls) of the class under test, rather than testing the class on what its public method returns, and its side effects.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: The Art of Testing Less without Sacrificing Quality","slug":"Paper-Review-The-Art-of-Testing-Less-without-Sacrificing-Quality","date":"2022-09-21T07:00:00.000Z","updated":"2022-12-18T19:55:44.317Z","comments":true,"path":"2022/09/21/Paper-Review-The-Art-of-Testing-Less-without-Sacrificing-Quality/","link":"","permalink":"https://abbaswu.github.io/2022/09/21/Paper-Review-The-Art-of-Testing-Less-without-Sacrificing-Quality/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward?For large complex software products, there is a need to check that changes do not negatively impact other parts of the software and they comply with system constraints such as backward compatibility, performance, security etc. Ensuring these system constraints may require complex test procedures, but long tests conflict with strategic aims to shorten release cycles. To accelerate test processes without sacrificing product quality, the paper develops a cost model for test executions based on historic test execution results that causes no test execution runtime overhead. The paper then presents a novel cost based test selection strategy, THEO, which skips test executions where the expected cost of running the test exceeds the expected cost of not running it, while ensuring that all tests will execute on all code changes at least once. How was the work validated?The paper replayed past development periods of Microsoft Windows, Office, and Dynamics with THEO. THEO would have reduced the number of test executions by up to 50%, cutting down test time by up to 47%. At the same time, product quality was not sacrificed as the process ensures that all tests are ran at least once on all code changes. Simulation shows that THEO produced an overall cost reduction of up to $2 million per development year, per product. Furthermore, this paper have convinced an increasing number of Microsoft product teams to explore ways to integrate THEO into their actual live production test environments. This further endorses THEO’s effectiveness. How could this research be extended?The paper stated that through reducing the overall test time, THEO would also have other impacts on the product development process, such as increasing code velocity and developer satisfaction. An empirical study on the effects of cost based test selection strategies on these aspects would be a direction for extending this research. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you? How could this research be applied in practice?In my opinion, the main contribution of this paper, and the aspect most able to be used as a reference in other projects, is the cost model where each test execution is considered an investment and the expected test result considered as return of investment. Several factors are considered in the cost model, with their values easily derived from past observations. $p_{TP}$, the probability the combination of test and execution context will detect a defect (true positive). $p_{FP}$, the probability the combination of test and execution context will report a false alarm (false positive). $engineers$, the number of engineers whose code changes passed the current code branch. $time_{delay}$, the average time span required to fix historic defects on the corresponding code branch. When a test is executed: $cost_{machine}$: the per-minute infrastructure cost of test execution. $cost_{inspect}$: the average cost per test inspection, equal to inspection time times the salary of the engineer. For simplicity reasons, an average cost of test inspection is used. When a test is skipped: $cost_{escaped}$: the average cost of an escaped defect, per developer and hour of delay. Defect severity is not modeled, as it cannot be determined beforehand, and all defects causing development activity to freeze on the corresponding branch must be considered severe. After collecting these data, two cost functions are calculated: the expected cost of executing a test $cost_{exec} &#x3D; cost_{machine} + p_{FP} \\times cost_{inspect}$, and the expected cost for not executing a test $cost_{skip} &#x3D; p_{TP} \\times cost_{escaped} \\times time_{delay} \\times engineers$. Through a reasonable and tested quantization like this, objective decisions can be made, boosting the efficiency of software development.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: Boosting Fuzzer Eficiency: An Information Theoretic Perspective","slug":"Paper-Review-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective","date":"2022-09-20T07:00:00.000Z","updated":"2022-12-18T22:20:14.431Z","comments":true,"path":"2022/09/20/Paper-Review-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective/","link":"","permalink":"https://abbaswu.github.io/2022/09/20/Paper-Review-Boosting-Fuzzer-Eficiency-An-Information-Theoretic-Perspective/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. What is the problem being tackled?Finding a solid theoretical foundation for fuzzing and using it to boost fuzzing efficiency is a direction of research of great practical value. How was it addressed by prior work?Previous works have proposed various heuristics to boost fuzzing efficiency, such as assigning more energy to seeds that have previously been observed to crash, that maximize execution counts to discover algorithmic complexity vulnerabilities, that exercise low-probability paths, etc. Furthermore, there has also been prior research in theoretical aspects of fuzzing, such as conducting a probabilistic analysis on the efficiency of blackbox versus whitebox fuzzing, empirically investigating the scalability of non-deterministic black- and greybox fuzzing, etc. What are the innovation(s) proposed in this paper?First, the paper develops an information-theoretic foundation for non-deterministic fuzzing. AssumptionsFuzzing Heuristics remain constant throughout the fuzzing process. ConceptsNeighborhoodAll inputs generated from mutating a seed.SpeciesA branch within a program.Species DiscoveryProgram execution traverses a previously untraversed branch when some input is provided to the program.Incidence FrequencyThe number of times a species is covered.EnergyThe probability the fuzzer chooses a seed for mutation.Power ScheduleThe procedure of assigning energy to a seed.Local Species Distribution of a SeedGiven a seed, the probability of each species being covered, when an input generated by mutation from the seed is fed to the program.Entropy in the Context of FuzzingUsing the metaphor of a “language” with “words” of varying frequencies, entropy in the context of fuzzing can be understood as: “Sentences” of the “language”: Program executions resulting from generated inputs. “Words” of the “language”: Species. Frequencies of the “words”: The frequencies of each species being traversed. Entropy can be calculated using the frequencies of the “words”, and represents the frequency distribution of the “words”. As high entropy implies that the species of the program have all been well covered, it can be used as a proxy for fuzzing efficiency. Local Entropy of a SeedStill using the metaphor of a “language” with “words” of varying frequencies, local entropy of a seed can be understood as: “Sentences” of the “language”: Program executions resulting from inputs within the seed’s neighborhood. “Words” of the “language”: Species. Frequencies of the “words”: The frequencies of each species being traversed. The local entropy of a seed quantifies the information that feeding the inputs within the seed’s neighborhood into the program reveals about the species. Second, the paper presents the first entropy-based power schedule to boost the efficiency of greybox fuzzers. More energy is assigned to seeds that elicit more information about the program’s species. Thus, every time when randomly choosing a seed for mutation, each seed is assigned an energy proportional to its local entropy. However, a new seed that has never been fuzzed will always be assigned zero energy, and they will never be chosen for mutation. To solve this problem, add-one smoothing is used for the frequency of the species. Specifically, the frequency of species $i$ used to calculate local entropy of seed $t$: $p_i^t &#x3D; \\frac{Y_i^t + 1}{S + Y_1^t + \\dots + Y_S^t}$ Where: $Y_i^t$ is the number of times species $i$ has been traversed by the neighborhood of $t$. $S$ is the total number of species at the time of calculation. Furthermore, in the experiments, the authors noticed that the local entropies for different seeds were almost the same, because a small number of very abundant species had a huge impact on the local entropies. Thus, the authors defined an abundance threshold $\\theta$ which is an upper bound for $Y_i^t$. How are those innovations evaluated? How does the paper’s evaluation match with the proposed problem statement?The paper provides an open-source implementation, Entropic, within LLVM libFuzzer, and presents a substantial empirical evaluation on over 250 widely-used, open-source C&#x2F;C++ programs producing over 2 CPU years worth of data. Four research questions were asked to evaluate the hypothesis that increasing information per generated input increases fuzzer efficiency. What is the empirical coverage improvement over the baseline? How much faster are bugs detected compared to the baseline? How does the choice of abundance threshold influence the performance of our technique? What is the cost of maintaining incidence frequencies? The answers to these research strongly support the hypothesis, thus the evaluation matches well with the proposed problem statement. Your opinion of the paperWhich technical innovations are most compelling to you?Developing an information-theoric foundation for non-deterministic fuzzing, in which entropy in the context of fuzzing is calculated using the probability distribution of species (branches). This is both intuitive and allows us to effectively use entropy, which has “really nice properties, and a principled origin” as a “convenient proxy” for fuzzing efficiency. What remains unclear after reading the paper? Are there any clarification questions whose answers would substantially change your opinion of the paper? Which problems remain unsolved after this paper?The paper develops an information-theoretic foundation for non-deterministic fuzzing, before presenting the first entropy-based power schedule to boost the efficiency of greybox fuzzers. I have questions regarding both aspects. Entropy is calculated using the probability distribution of species, which are branches. Is is possible to utilize a different definition of “species”? The entropy-based power schedule assigns each seed with energy proportional to its local entropy. However, the authors noticed that the local entropies for different seeds were almost the same, because a small number of very abundant species had a huge impact on the local entropies. Thus, the authors defined an abundance threshold $\\theta$ for $Y_i^t$, a task-relevant hyperparameter. Is there a better approach for calculating the local entropies? Do you forsee any barriers to the applicability of the technique proposed in the paper? If so, how could these barriers be overcome?As stated above, regarding the entropy-based power schedule.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]},{"title":"Paper Review: How Effective Developers Investigate Source Code: An Exploratory Study","slug":"Paper-Review-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study","date":"2022-09-19T07:00:00.000Z","updated":"2022-12-18T17:46:20.275Z","comments":true,"path":"2022/09/19/Paper-Review-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study/","link":"","permalink":"https://abbaswu.github.io/2022/09/19/Paper-Review-How-Effective-Developers-Investigate-Source-Code-An-Exploratory-Study/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward? The paper provides a set of detailed observations about the characteristics of effective program investigation. These observations are accompanies by hypotheses that can be validated by additional research and practical experience. The paper’s results support the intuitive notion that developers should follow a general plan, perform focused searches in the context of this plan, and keep some form of record of their findings when investigating a program. The paper describes a methodology and analysis technique for studying the behavior of software developers. How was the work validated?The authors conducted a study of five developers undertaking an identical software change task on a medium-sized system, where understanding the existing software is a precursor to modification and validation. They did a detailed qualitative analysis of a few replicated cases, rather than a statistical analysis of causality between dependent variables. Many previous studies were based on heavily abstracted characterizations of both developer behavior and success level. It involved a detailed study of the examined code, the methods used to navigate between different locations in the code, and the modified source code. They contrasted the program investigation behavior of successful and unsuccessful developers, and isolated the factors associated with the behavior of a developer, rather than external factors (such as the influence of the workplace, the programming environment, etc.) How could this research be applied in practice?Ensuring that developers in charge of modifying software systems investigate the code of the system effectively can yield important benefits such as decreasing the cost of performing software changes and increasing the quality of the change. Understanding the nature of program investigation behavior that is associated with successful software modification tasks can help us improve the tool support and training programs offered to software developers. How could this research be extended?Researchers can reuse the authors’ strategy (stated in “How was the work validated?”) to help validate the hypotheses the authors’ proposed, or to study other aspects of programmer behavior. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you?In my opinion, the main contributions of the paper include the primary contributions of the paper as the author sees it, how the work was validated, and how this research could be applied in practice. However, what is most meaningful for me is how the work was validated. Such methodology is of great reference value for conducting studies on other aspects of programmer behavior. There are many technical details within that have left a deep impression on me. Each phase was described entirely through written instructions, and the subjects were given an Eclipse training phase and an investigation phase before the modification phrase. To record the actions of a developer in the investigation and modification phases, they recorded the developers’ screens, and transcribed the recordings into a structured list of events. Each event contains the properties time, method, navigation, and modification. To analyze the quality of change, the authors analyzed the source code to determine the characteristics of an ideal solution, and divided the task into five subtasks. The authors examined how each subject had implemented each subtask, and characterized its quality.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Understanding the Formulation of Information Entropy","slug":"Understanding-the-Formulation-of-Information-Entropy","date":"2022-09-16T07:00:00.000Z","updated":"2023-02-26T16:53:22.089Z","comments":true,"path":"2022/09/16/Understanding-the-Formulation-of-Information-Entropy/","link":"","permalink":"https://abbaswu.github.io/2022/09/16/Understanding-the-Formulation-of-Information-Entropy/","excerpt":"","text":"NOTE: The terms “language” and “word” are used metaphorically in this document. A “language” often has many “words”, and the frequency of each “word” varies. If a “language” $X$ has a total of $n$ “words”, then we can encode a word with $\\log_{2}{n}$ binary bits. But when transmitting the words, we want to keep the encoding of each “word” as short as possible. A common practice is that for those high-frequency “words”, we can use shorter encodings, and for those “words” that we use less frequently, we can allow longer encodings. An example is the Morse code encoding for a “language” consisting of 36 “words” - 26 Latin letters and 10 Arabic numerals. So, under some optimal encoding, what limit can the weighted average encoding length of all “words” achieve? Suppose our “language” has $n$ “words”, $x_1, x_2, \\dots, x_n$, and their probability of occurrence is $p(x_1), p(x_2), \\dots, p(x_n)$ (known quantities). Assuming that the lengths of the encodings of these “words” are $L(x_1), L(x_2), \\dots, L(x_n)$ respectively, the weighted average encoding length of each “word” is: $\\bar{L} &#x3D; p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)$ How do we find the minimum value of $\\bar{L}$? ConstraintsObviously, the encoded length of all “words” is greater than 0. But beyond that, there is a hidden constraint. We do not allow one encoding to be a prefix of another encoding, otherwise there will be ambiguity during decoding. For example, assuming that the three “words” of “A”, “B”, and “C” in the alphabet are encoded as “0”, “1”, and “01” respectively, then for For a code like “001”, should we decode it as “AAB” or “AC”? We call a type of code which requires that there is no whole code word in the system that is a prefix of any other code word in the system as a prefix code. This means that if we assign a shorter encoding to a “word”, it will squeeze a lot of resources out of the encoding space. For example, suppose the “word” “A” is encoded as “0”, then it would “squeeze out” “00”, “01”, etc. from the codewords. Suppose the maximum value in $L(x_1), L(x_2), \\dots, L(x_n)$ is $L_{max}$. Then the encoding of all “words” are nodes on a full binary tree with a height of $L_{max}$, and the full binary subtrees below each node have no intersection (otherwise violating the properties of the prefix code), as shown below. It is obvious that, all the full binary subtrees below each node, at most cover all the leaves of the full binary tree with height $L_{max}$. For a “word” $x_i, i \\in {1, 2, \\dots, n}$, the height of the full binary subtree below it is $L_{max} - L(x_i)$, and it covers $2^{L_{max} - L(x_i)}$ leaves. As the full binary tree with height $L_{max}$ has a total of $2^{L_{max}}$, we have: $2^{L_{max} - L(x_1)} + 2^{L_{max} - L(x_2)} + \\dots + 2^{L_{max} - L(x_n)} \\le 2^{L_{max}}$ This simplifies to: $2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1$ This is the Kraft-McMillan inequality. OptimizationTherefore, our overall optimization objective is: $\\bar{L} &#x3D; p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)$ Subject to: $p(x_i) \\in (0, 1), i \\in {1, 2, \\dots, n}$ are constants $p(x_1) + p(x_2) + \\dots + p(x_n) &#x3D; 1$ $L(x_i) &gt; 0, i \\in {1, 2, \\dots, n}$ are independent variables $2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1$ We can analyze the problem for the case where there are only two words $x_1, x_2$. At this point, we have: $\\bar{L} &#x3D; p(x_1) L(x_1) + p(x_2) L(x_2)$ Equivalently: $\\bar{L} &#x3D; p(x_1) L(x_1) + (1 - p(x_1)) L(x_2)$ Subject to: $p(x_1) \\in (0, 1)$ is a constant $L(x_i) &gt; 0, i \\in {1, 2}$ are independent variables $2^{- L(x_1)} + 2^{- L(x_2)} \\le 1$ Define $a_1 &#x3D; 2^{- L(x_1)}, a_2 &#x3D; 2^{- L(x_2)}$. Now we have: $\\bar{L} &#x3D; - p(x_1) \\log_2{a_1} - (1 - p(x_1)) \\log_2{a_2}$ Subject to: $p(x_1) \\in (0, 1)$ is a constant $0 &lt; a_i &lt;1, i \\in {1, 2}$ are independent variables $a_1 + a_2 \\le 1$ At this point, $\\bar{L}$ can be regarded as a binary function whose independent variables are $a_1, a_2$, and the value ranges of the independent variables $a_1, a_2$ are as follows: We want to find the minimum value of $\\bar{L}(a_1, a_2)$ within this range of values. The gradient of $\\bar{L}(a_1, a_2)$ is as follows: $\\nabla\\bar{L}(a_1, a_2) &#x3D; {(-p(x_1) \\log{2} \\frac{1}{a_1}, -(1 - p(x_1)) \\log{2} \\frac{1}{a_2})}^T$ Within the value range of the independent variables $a_1, a_2$, $\\nabla\\bar{L}(a_1, a_2)$ is always less than 0, which means that with the growth of $a_1, a_2$, $\\bar{L}(a_1, a_2)$ decreases. Therefore, the maximum value of $\\bar{L}(a_1, a_2)$ must occur when $(a_1, a_2)$ is on the boundary line $a_1 + a_2 &#x3D; 1$. Substituting the boundary line $a_1 + a_2 &#x3D; 1$ into $\\bar{L}(a_1, a_2)$, you can get a unary function: $\\bar{L}(a_1) &#x3D; - p(x_1) \\log_2{a_1} - (1 - p(x_1)) \\log_2{(1 - a_1)}$ The constraints include: $p(x_1) \\in (0, 1)$, constant $0 &lt; a_1 &lt; 1$ The derivative of $\\bar{L}(a_1)$ is as follows: $\\frac{d \\bar{L}(a_1)}{d a_1} &#x3D; \\frac{\\log{2} (a_1 - p(x_1))}{a_1 (1 - a_1)}$ The constraints include: $p(x_1) \\in (0, 1)$ is a constant $0 &lt; a_1 &lt; 1$ When $0 &lt; a_1 &lt; p(x_1)$, $\\frac{d \\bar{L}(a_1)}{d a_1} &lt; 0$, $\\bar{L}(a_1)$ monotonically decreases, and when $p(x_1) &lt; a_1 &lt; 1$, $\\frac{d \\bar{L}(a_1)}{d a_1} &gt; 0$, $\\bar{L}(a_1)$ monotonically increases. Therefore, when $a_1 &#x3D; p(x_1)$, $\\bar{L}(a_1)$ obtains the minimum value. As $a_1 &#x3D; 2^{- L(x_1)}, a_2 &#x3D; 2^{- L(x_2)}$, this means that, for: $\\bar{L} &#x3D; p(x_1) L(x_1) + (1 - p(x_1)) L(x_2)$ Subject to: $p(x_1) \\in (0, 1)$ is a constant $L(x_i) &gt; 0, i \\in {1, 2}$ are independent variables $2^{- L(x_1)} + 2^{- L(x_2)} \\le 1$ $\\bar{L}$’s minima occurs when $L(x_1) &#x3D; -\\log_2{p(x_1)}, L(x_2) &#x3D; -\\log_2{p(x_2)}$, and the minima is $- p(x_1) \\log_2{p(x_1)} - p(x_2) \\log_2{p(x_2)}$. Going back to the multivariate optimization problem: $\\bar{L} &#x3D; p(x_1) L(x_1) + \\dots + p(x_n) L(x_n)$ Subject to: $p(x_i) \\in (0, 1), i \\in {1, 2, \\dots, n}$ are constants $p(x_1) + p(x_2) + \\dots + p(x_n) &#x3D; 1$ $L(x_i) &gt; 0, i \\in {1, 2, \\dots, n}$ are independent variables $2^{- L(x_1)} + 2^{- L(x_2)} + \\dots + 2^{- L(x_n)} \\le 1$ $\\bar{L}$’s minima occurs when $L(x_i) &#x3D; -\\log_2{p(x_i)}, i \\in {1, 2, \\dots, n}$, and the minima is $- p(x_1) \\log_2{p(x_1)} - \\dots - p(x_n) \\log_2{p(x_n)}$. Definition of Information EntropyIf a language “language” $X$ has $n$ “words”, $x_1, x_2, \\dots, x_n$, the probability of their occurrence is $p(x_1), p(x_2), \\dots, p(x_n)$, then all “words” under a certain optimal encoding, the previously calculated minimum weighted average encoding length, $- p(x_1) \\log_2{p(x_1)} - \\dots - p (x_n) \\log_2{p(x_n)}$, is called the information entropy of the “language”, denoted as $H(X)$. The reason why it is called “information entropy” is mainly due to the following reasons: From von Neumann’s naming suggestion for Shannon: My greatest concern was what to call it. I thought of calling it ‘information,’ but the word was overly used, so I decided to call it ‘uncertainty.’ When I discussed it with John von Neumann, he had a better idea. Von Neumann told me, ‘You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage. In a sense, it does reflect the frequency distribution of the “words” of “language” $X$, just as entropy in thermodynamics reflects the distribution of microscopic particles. The lower $H(X)$ is, the more the case that only a few words are used frequently in $X$; the higher $H(X)$ is, the more the case that all words in $X$ are used frequency. Links to Explanations of Related Concepts Cross Entropy Joint Entropy Mutual Information How These Concept are Applied in Practicehttps://colah.github.io/posts/2015-09-Visual-Information/#conclusion References https://colah.github.io/posts/2015-09-Visual-Information/ https://mbernste.github.io/posts/sourcecoding/ https://en.wikipedia.org/wiki/Kraft–McMillan_inequality https://mathoverflow.net/questions/403036/john-von-neumanns-remark-on-entropy","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://abbaswu.github.io/categories/Mathematics/"}],"tags":[]},{"title":"Paper Review: Asking and Answering Questions during a Programming Change Task","slug":"Paper-Review-Asking-and-Answering-Questions-during-a-Programming-Change-Task","date":"2022-09-14T07:00:00.000Z","updated":"2022-12-18T17:35:58.250Z","comments":true,"path":"2022/09/14/Paper-Review-Asking-and-Answering-Questions-during-a-Programming-Change-Task/","link":"","permalink":"https://abbaswu.github.io/2022/09/14/Paper-Review-Asking-and-Answering-Questions-during-a-Programming-Change-Task/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? A catalog of 44 types of questions programmers ask during software evaluation tasks, organized into four categories based on the kind and scope of information needed to answer a question. Finding a focus point Expanding a focus point Understanding a subgraph Over groups of subgraphs A description of the observed behavior around answering these questions. A description of how existing deployed and proposed tools do, and do not, support answering programmers’ questions. How was the work validated?The author interviewed participants in two studies. 9 participants in academia worked on a code base that was new to them. 16 participants in industry worked on a code base for which they had responsibility. The two studies have allowed us to observe programmers in situations that vary along several dimensions: the programming tools the type of change task the system paired versus individual programming prior knowledge of the code base The differences have increased the authors’ ability to generate an extensive set of questions programmers ask. They build rather than test theory and the specific result of this process is a theoretical understanding of the situation of interest grounded in the data collected. What were the main contributions of the paper as you (the reader) see it? How does the work apply to you?In my opinion, aside from the final results, three important considerations learned from this paper are: Interviewing participants in two very different groups. Developing generic versions of the questions participants asked, which slightly abstract from the specifics of a particular situation and code base. Compared the generic questions and categorized those questions into four categories based on the kind and scope of information needed to answer a question. This is an example of extracting generalized knowledge from specific case studies, which makes it a great example to study for conducting empirical studies. How could this research be extended? How could this research be applied in practice?The research identified clear gaps of tool support in answering programmers’ questions. Support for more refined or precise questions. Some questions can he seen as more refined versions of other questions. A programmer’s questions also often have an explicit or implicit scope. Due to limited tool support, programmers end up asking questions more globally than they intend, and, the result sets will include many irrelevant items. Support for maintaining context. A particular question is often part of a larger process involving multiple questions. There are missed opportunities for tools to make use of the larger context to help programmers more efficiently scope their questions and to determine what is relevant to their higher level questions. Support for piecing information together. Many questions require considering multiple entities and relationships. In these situations, the burden is on the programmer to assemble the information needed to answer their intended question. Tool support is missing for bringing information together and building toward an answer. Improved tools and an assessment of these tools in answering these questions present directions for future research.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: Qualitative Methods in Empirical Studies of Software Engineering","slug":"Paper-Review-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering","date":"2022-09-14T07:00:00.000Z","updated":"2022-12-18T19:41:57.427Z","comments":true,"path":"2022/09/14/Paper-Review-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering/","link":"","permalink":"https://abbaswu.github.io/2022/09/14/Paper-Review-Qualitative-Methods-in-Empirical-Studies-of-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. What were the primary contributions of the paper as the author sees it? How does this work move the research forward?With empirical studies of software engineering beginning to address the human aspects of software development, the author presents and reviews a number of different methods for the collection and analysis of qualitative data, and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combines with quantitative methods. Collecting Qualitative Data Participant Observation Interviewing Extracting Quantitative Values from Qualitative Data for Quantitative Analysis (Coding) Analyzing Qualitative Data Theory Generation: extract from a set of field notes a statement or preposition that is supported in multiple ways by the data. Theory Confirmation: confirming a preposition after it has been generated from the data. What were the main contributions of the paper as you (the reader) see it? How could this research be applied in practice?Aside from the primary contributions of the paper as the author sees it, in my opinion, another major contribution of the paper is identifying the four main categories of empirical studies, and explaining in detail how combinations of quantitative and qualitative methods can be designed for each category. The four main categories of empirical studies: Blocked subject-project study: Several projects, several subjects. Reduces bias, but increases the cost of the experiment. Replicated project study: One project, several subjects. Isolates the effect of differences between subjects. Multiproject variation: Several projects, one subject. Observes the performance of the subject on a project before some treatment is applied, and on a different project after that treatment is applied. Single project study: One project, one subject. Similar to a case study. Certain attributes are examined and possibly compared to some baseline. How combinations of quantitative and qualitative methods can be designed for each category: Blocked subject-project study, Replicated project study: When testing hypotheses and finding casual relationships between variables, use qualitative data to illuminate the statistical results. Multiproject variation study: Qualitative analysis: revealing new issues and tracking changes relative to other issues. Quantitative analysis: looking more closely at the issues suggested by the qualitative analysis. Single project study: First, data is collected qualitatively through interviews. A taxonomy of the question under research is generated. Part of the interview data is coded to yield quantitative variables. Any relationships found between quantitative variables are checked against qualitative data. How was the work validated?Examples, interviews, quotes from experts, and paper citations are used to validate the points presented when reviewing a number of different methods for the collection and analysis of qualitative data, identifying the four main categories of empirical studies, and explaining in detail how combinations of quantitative and qualitative methods can be designed for each category. How could this research be extended?In the last paragraph, the author points out that “we must exploit to the fullest every opportunity we do have, by collecting and analyzing as much data of as many different types as possible”. Aside from the examples presented in the paper, what other types of data can be collected, and how they can be analyzed, is a future direction of research.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: No Silver Bullet Essence and Accidents of Software Engineering","slug":"Paper-Review-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering","date":"2022-09-12T07:00:00.000Z","updated":"2022-12-18T17:51:34.258Z","comments":true,"path":"2022/09/12/Paper-Review-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering/","link":"","permalink":"https://abbaswu.github.io/2022/09/12/Paper-Review-No-Silver-Bullet-Essence-and-Accidents-of-Software-Engineering/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. How does this work move the research forward?What were the primary contributions of the paper as the author sees it?The author concludes that there is no elixir or “silver bullet” to the problems software engineering is facing. Furthermore, the author also examines encouraging innovations, and shows that a disciplined, consistent effort to develop, propagate, and exploit them should alleviate the problem. What were the main contributions of the paper as you (the reader) see it?In my opinion, what the paper is most remarkable at is shedding light upon the nature of the software problem and its implications. The essence of a software entity is a construct of interlocking concepts that cannot be accurately visualized. The complexity of software is an essential property, and it increases non-linearly with size. This has many implications. Difficulty of design. Hindrance of communication among team members, which leads to product flaws, cost overruns, schedule delays. Hard to use programs. Difficulty of extending to new functions without creating side effects. Security trapdoors. Personnel turnover incurs tremendous learning and understanding burden. Software is constantly subject to pressure for change. The aforementioned points clarified by the paper illuminates research directions in software engineering aimed at ameliorating the software problem. How could this research be extended?In the last section of the paper, the author examines promising attacks on the essence of the software problem. Buying off-the-shelf software instead of building in-house software. Rapid prototyping and iterative specification of requirements with client feedback. Incremental development of software from a simple and incomplete, yet running, system. Growing great designers who are the core of the development team. The effectiveness of these and other approaches in mitigating the software problem could be assessed in subsequent works. How was the work validated? First, the author examines the nature of the software problem and its implications. Further on, the author recalls the three steps in software technology that have been most fruitful in the past - high-level languages, time-sharing, and unified programming environments, concluding that they have their limits and the difficulties that they attacked are accidental, not essential. The author continues to consider the technical developments that are most often advanced as potential silver bullets - high-level language advances, object-oriented programming, artificial intelligence, automatic programming, graphical programming, program verification, environments and tools, workstations - analyzing the problems they assess, their advantages, and their disadvantages. Finally, the author presents promising attacks on the conceptual essence, explaining why they would be useful. How could this research be applied in practice?The lessons learned from this research are of great practical value. In shedding light upon the nature of the software problem and its implications, the author provides criteria for organizations to assess the effectiveness of their development practices. In considering the technical developments that are most often advanced as potential silver bullets, the author examines their advantages, and their disadvantages, and provide insights into whether to, and how to adequately use them. In presenting promising attacks on the conceptual essence, the author provides meaningful suggestions for organizations to improve their software development processes, and provides convincing rationale for doing so. As this is a classic paper, many promising attacks on the conceptual essence have already materialized and become mainstream. Rapid prototyping and incremental development have been manifested as “agile development” and have been widely adopted. With the advent of the open-source revolution and code-hosting platforms such as GitHub, reusing off-the-shelf software instead of building in-house software has become ubiquitous. However, the call for organizations to “grow great designers who are the core of the development team” incurs significant requirements on corporate management competency, and sadly, hasn’t fully become reality. How does the work apply to you?It sheds light upon the nature of the software problem and its implications, illuminates research directions in software engineering aimed at ameliorating the software problem, and provides a reference research methodology for problems within software engineering.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: Software's Chronic Crisis","slug":"Paper-Review-Software-s-Chronic-Crisis","date":"2022-09-12T07:00:00.000Z","updated":"2022-12-18T19:47:53.939Z","comments":true,"path":"2022/09/12/Paper-Review-Software-s-Chronic-Crisis/","link":"","permalink":"https://abbaswu.github.io/2022/09/12/Paper-Review-Software-s-Chronic-Crisis/","excerpt":"","text":"NOTE: This is a Paper Review for Advanced Software Engineering. The original paper can be found here. How does this work move the research forward?What were the primary contributions of the paper as the author sees it?The author identifies software’s chronic crisis and how it is exacerbated by current trends in software engineering. The vast majority of code is handcrafted by artisans using techniques they neither measure nor are able to repeat consistently. The software industry remains short of the mature engineering discipline needed to meet the demands of an information-age society, including getting software right the first time in embedded environments, distributed systems and systems integration, rapid increasing system sizes, and systems becoming so complex that no manager can comprehend the entirety. Later, the author analyzes proposed remedies to the aforementioned problems and points out directions for future work. Remedies: Capability Maturity Model, which quantifies a developer’s software engineering and management excellence. Consistent and quantitative measurement of development. Strategies to avoid bugs or attack them early. Recognizing changing requirements Growing software from rapid prototypes and customer feedback Formal verification when necessary Clean-room process Cautious approach to technological innovations such as object-oriented analysis and programming Directions for Future Work: An experimental branch of computer science to separate the general results from the accidental Standard unit of measurement of developer productivity Codified proven solutions for novices Academic-industrial collaboration to gather data and try things Generalized, reusable software components Certifying software engineers Outsourcing More software development-oriented computer science curricula What were the main contributions of the paper as you (the reader) see it? How could this research be applied in practice?Aside for the primary contributions of the paper as the author sees it, in my opinion, a major contribution of the paper in a practical sense are revelations for improving the culture within software developing organizations. For example, Focus on interchangeability. Follow best practices. Fix not just the bug but also the flaw in the testing process that allowed it to slip through. Value verification in addition to innovation. Pay attention to the difference in competence between employees. Furthermore, as a historical paper, many of its proposals have already materialized. For example, the open-source revolution and collaboration platforms such as GitHub have greatly facilitated gathering data and trying things for research, and has provided a wealth of generalized, reusable software components. How could this research be extended?Implementing and assessing the proposed directions for future work represents a natural extension of this research. How was the work validated?The authors validate their arguments on software’s chronic crisis and base their proposals for remedies and future work by analyzing real cases in software engineering, as well as compiling the opinions of experts in the field, including university professors and corporate managers. How does the work apply to you? From a theoretical perspective, as a milestone paper in the domain of software engineering, this paper provides a model research methodology for practical problems within software engineering - analyzing real cases and compiling the opinions of experts. From a practical perspective, this paper identifies core values and skills that us, as practitioners of software engineering, should firmly grasp.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"}],"tags":[]},{"title":"Paper Review: An empirical study of the reliability of UNIX utilities","slug":"Paper-Review-An-empirical-study-of-the-reliability-of-UNIX-utilities","date":"2022-09-10T07:00:00.000Z","updated":"2022-12-18T22:20:14.428Z","comments":true,"path":"2022/09/10/Paper-Review-An-empirical-study-of-the-reliability-of-UNIX-utilities/","link":"","permalink":"https://abbaswu.github.io/2022/09/10/Paper-Review-An-empirical-study-of-the-reliability-of-UNIX-utilities/","excerpt":"","text":"NOTE: This is a Paper Review for Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis. The original paper can be found here. “An empirical study of the reliability of UNIX utilities” is the work that spawned research into the domain of software fuzzing. It proposes a technique later known as random fuzzing, testing the reliability of UNIX utilities by feeding them a stream of randomly generated characters and checking whether the program crashed with a core dump or hangs. Although the technique is simple and is not a substitute for formal verification or testing, it is inexpensive and easy to apply. Its effectiveness in identifying bugs and increasing overall system reliability has been proven in many ways. It crashed 25-33% of the utility programs considered to be “reliable” on each platform. It was able to find recurring security bugs resulting from bad programming practices that even the best static analysis tools have limited success in detecting, including: Accessing outside the bounds of a buffer Dereferencing a null pointer Unintentionally overwriting data or code Ignoring return codes, especially error-indicating return codes Faulty communication with subprocesses Unintended interaction between modules Improper error handling Signed characters Race conditions during signal handling Its relevance has remained strong over the years. Subsequent studies using the same technique showed that similar problems also existed within other operating systems, such as Microsoft Windows. Even after thirty years, the utility programs in the modern Unix distributions of Linux, macOS, and FreeBSD are still crashing at a noticeable rate and not getting better, as evidenced in “The Relevance of Classic Fuzz Testing: Have We Solved This One?” The contributions of this work is multi-fold. As mentioned before, it proposed random fuzzing, an inexpensive, easy to apply, and time-proven way of finding security bugs which is complimentary with formal verification and testing. It spawned research into the domain of software fuzzing. New fuzz tools usually take a gray- or white-box approach, diving deeper into a program’s control flow, and they have been applied to many new contexts. However, they often require more advanced specification of the input and&#x2F;or long execution times to explore the input and program control-flow space. It provides revelations for software engineering: good design, good education, ongoing training, testing integrated into the development cycle, and most importantly, a culture that promotes and rewards reliability. Some personal thoughts after reading the paper. Given the source code of a program and an input, what is the mechanism through which the researchers determine the position where the program crashes and hangs when given the input? This is mentioned in neither “An empirical study of the reliability of UNIX utilities” nor its sequel “The Relevance of Classic Fuzz Testing: Have We Solved This One?”, but is of great practical value. There is a surprising number of security bugs stemming from language defects such as not checking array bounds and dereferencing null pointers, as well as ad-hoc, hacky solutions to recurring problems such as lexical analysis, syntax analysis, structured error handling, as well as graph algorithms including cycle detection, topological sort, etc. Personally, this is not my style of coding. I make extensive a lot of “safe” language constructs such as null coalescing, heavily exploit performant and well-tested algorithms within standard libraries and widely-adapted third-party libraries (such as boost in C++ and networkx in Python), and use theoretically sound tools (such as automatically generated LALR parsers for syntax analysis) in software projects. The efficiency, effectiveness, and practical value of these and other solutions, as well as how they can be improved, is an interesting question that comes to my mind after reading this paper.","categories":[{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"}],"tags":[]}],"categories":[{"name":"Code","slug":"Code","permalink":"https://abbaswu.github.io/categories/Code/"},{"name":"Computer Networking","slug":"Code/Computer-Networking","permalink":"https://abbaswu.github.io/categories/Code/Computer-Networking/"},{"name":"Python","slug":"Code/Python","permalink":"https://abbaswu.github.io/categories/Code/Python/"},{"name":"Unix","slug":"Code/Unix","permalink":"https://abbaswu.github.io/categories/Code/Unix/"},{"name":"Paper Review","slug":"Paper-Review","permalink":"https://abbaswu.github.io/categories/Paper-Review/"},{"name":"Topics in Programming Languages: Type Systems","slug":"Paper-Review/Topics-in-Programming-Languages-Type-Systems","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Type-Systems/"},{"name":"C++","slug":"Code/C","permalink":"https://abbaswu.github.io/categories/Code/C/"},{"name":"PySide6","slug":"Code/Python/PySide6","permalink":"https://abbaswu.github.io/categories/Code/Python/PySide6/"},{"name":"Planning","slug":"Planning","permalink":"https://abbaswu.github.io/categories/Planning/"},{"name":"Topics in Programming Languages: Automated Testing, Bug Detection, and Program Analysis","slug":"Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis","permalink":"https://abbaswu.github.io/categories/Paper-Review/Topics-in-Programming-Languages-Automated-Testing-Bug-Detection-and-Program-Analysis/"},{"name":"Advanced Software Engineering","slug":"Paper-Review/Advanced-Software-Engineering","permalink":"https://abbaswu.github.io/categories/Paper-Review/Advanced-Software-Engineering/"},{"name":"Mathematics","slug":"Mathematics","permalink":"https://abbaswu.github.io/categories/Mathematics/"}],"tags":[]}