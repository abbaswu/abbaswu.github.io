<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jifeng Wu&#39;s Personal Website</title>
  
  <subtitle>Jifeng Wu&#39;s Personal Website</subtitle>
  <link href="https://abbaswu.github.io/atom.xml" rel="self"/>
  
  <link href="https://abbaswu.github.io/"/>
  <updated>2024-06-17T00:26:21.050Z</updated>
  <id>https://abbaswu.github.io/</id>
  
  <author>
    <name>Jifeng Wu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Financial strategies during master&#39;s and PhD degrees (tentative)</title>
    <link href="https://abbaswu.github.io/2024/06/16/Financial-strategies-during-master-s-and-PhD-degrees-tentative/"/>
    <id>https://abbaswu.github.io/2024/06/16/Financial-strategies-during-master-s-and-PhD-degrees-tentative/</id>
    <published>2024-06-16T07:00:00.000Z</published>
    <updated>2024-06-17T00:26:21.050Z</updated>
    
    <content type="html"><![CDATA[<h1 id="original-chinese">Original (Chinese)</h1><ul><li>记录每月工资，跟踪每月固定必需开销（房租、订阅服务等），分配每月可变必需开销（食品百货等）、可变可选开销（衣物、旅行等）与储蓄。<ul><li>储蓄包括银行的储蓄账户和各种理财产品。<ul><li>对我们而言，各种理财产品的职责首先是对抗通货膨胀的<strong>储蓄账户</strong>，而非盈利的手段——我们是斗不过高频交易算法的。不得花过多的时间经历关注理财产品的价格变化，严格禁止各种赌博心理。</li></ul></li></ul></li><li>充分利用各种税收和学费资助的“免费”服务：<ul><li>规划好行程，充分利用公共交通，少打车。</li><li>充分利用学校提供的医疗保险；</li><li>通过学校订购的在线课程，不断提升与科研需求相辅相成的实用技能，如网页前端开发、数据科学等，并获取相应证书，既提升我们欠缺的implementation能力，又方便找实习与工作等。</li></ul></li><li>做好灵活的食品百货采货计划。<ul><li>关注超市传单；</li><li>书包中常备购物袋；</li><li>适量减少红肉采货量，适量增加鱼、蛋白粉、谷物、水果、蛋、奶采货量。</li></ul></li></ul><hr /><h1 id="translation-deepl.com">Translation (DeepL.com)</h1><ul><li>Record monthly paychecks, track monthly fixed essential expenses (rent, subscription services, etc.), and allocate monthly variable essential expenses (food, groceries, etc.) and variable optional expenses (clothing, travel, etc.) with savings.</li><li>Savings include savings accounts in banks and various financial products.</li><li>For us, the duty of various financial products is, first and foremost, an inflation-fighting <strong>savings account</strong>, not a means of profit - we can't beat high-frequency trading algorithms. We must not spend excessive time paying attention to price changes in financial products and strictly prohibit all kinds of gambling mentality.</li><li>Take advantage of tax and tuition assistance "freebies":</li><li>Plan trips well, use public transportation, and take fewer cabs.</li><li>Take advantage of the school's health insurance;</li><li>Through online courses purchased by the school, improve practical skills that complement research needs, such as web front-end development, data science, etc., and obtain the appropriate certificates to improve our lack of implementation skills and facilitate the search for internships and jobs.</li><li>Make a flexible food and grocery procurement plan.</li><li>Pay attention to supermarket flyers;</li><li>Always carry a shopping bag in your bag;</li><li>Reduce the amount of red meat and increase the amount of fish, protein powder, cereals, fruits, eggs, and milk.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;original-chinese&quot;&gt;Original (Chinese)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;记录每月工资，跟踪每月固定必需开销（房租、订阅服务等），分配每月可变必需开销（食品百货等）、可变可选开销（衣物、旅行等）与储蓄。
&lt;ul&gt;
&lt;li&gt;储蓄包括银行的</summary>
      
    
    
    
    <category term="Reflections" scheme="https://abbaswu.github.io/categories/Reflections/"/>
    
    
  </entry>
  
  <entry>
    <title>On doing things with others</title>
    <link href="https://abbaswu.github.io/2024/05/30/On-Doing-Things-with-Others/"/>
    <id>https://abbaswu.github.io/2024/05/30/On-Doing-Things-with-Others/</id>
    <published>2024-05-30T07:00:00.000Z</published>
    <updated>2024-05-30T14:10:48.730Z</updated>
    
    <content type="html"><![CDATA[<h1 id="original-chinese">Original (Chinese)</h1><p>和别人一起做事的时候，如果遇到了某件说好了要做的事别人不愿意开始做，我们不应该面露难色，变得焦虑、愤怒；我们更好的做法应该是给对方一个善意的提醒，然后重新规划时间——这时候规划的时间必须具体，不能是大而化之的“之后”“再过几天”等。</p><h1 id="translation-deepl.com">Translation (DeepL.com)</h1><p>When doing things with others, if we encounter a certain thing that we said we would do that others are not willing to start doing, we should not look embarrassed, become anxious and angry; our better approach should be to give each other a kind reminder, and then re-planning time - this time the planning time must be specific, not a big and generalized "afterward," "a few more days," etc.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;original-chinese&quot;&gt;Original (Chinese)&lt;/h1&gt;
&lt;p&gt;和别人一起做事的时候，如果遇到了某件说好了要做的事别人不愿意开始做，我们不应该面露难色，变得焦虑、愤怒；我们更好的做法应该是给对方一个善意的提醒，然后重新规划时间——这时候规</summary>
      
    
    
    
    <category term="Reflections" scheme="https://abbaswu.github.io/categories/Reflections/"/>
    
    
  </entry>
  
  <entry>
    <title>Some thoughts on maintaining fitness habits and improving physical fitness in the future</title>
    <link href="https://abbaswu.github.io/2024/02/29/Some-thoughts-on-maintaining-fitness-habits-and-improving-physical-fitness-in-the-future/"/>
    <id>https://abbaswu.github.io/2024/02/29/Some-thoughts-on-maintaining-fitness-habits-and-improving-physical-fitness-in-the-future/</id>
    <published>2024-02-29T08:00:00.000Z</published>
    <updated>2024-02-29T19:50:56.635Z</updated>
    
    <content type="html"><![CDATA[<h1 id="original-chinese">Original (Chinese)</h1><h2 id="关于今后健身习惯的保持与身体素质的提升的若干思考">关于今后健身习惯的保持与身体素质的提升的若干思考</h2><p>在今后漫长的人生之路上，我们要让健身成为一种像散步、跑步、爬山一样深入我们日常生活并且随时都有心情做的消遣活动。</p><p>结合我们初高中时期跑步、跳绳、坐位体前屈通过强制性的体育课逐渐成为我们习惯去做并能在不断进步中获得成就感的活动这样一段经历，<strong>我们也可以充分利用我们在UBC的最后一学期，每一次去学校学习时，在离开之前尽可能去一趟健身房——且不论时间长短，但还是尽可能去一趟</strong>。近日的实践证明，在脑力劳动之后，这样的“体力劳动”在有效缓解我们精神上的疲惫感的同时，又有助于提升我们的身体素质，可谓一举两得。</p><p>与之相伴的另一个问题是如何在坚持健身的同时在力量上取得突破。如果有人陪伴我们锻炼，那么这将不是一个难题。但当我们一个人锻炼的时候，我们又将怎么做呢？我们或许也可以从初高中时期独自练习跑步、跳绳、坐位体前屈的经历中获得灵感。</p><p>首先，我们必须有耐心。进步必定是一个漫长的过程，这个过程急不得。</p><p>其次，我们可以做到以巩固为主，提升为辅——在一段较长的时间段内，尽可能做到不倒退，稳扎稳打，保持并巩固当前来之不易的训练成果。为此，我们需要记录我们做各项锻炼是我们能接受的最大强度，从而做到每次锻炼时心中有数。</p><p>最后，还要时不时像扰动一样尝试突破自己能接受的最大强度。每一次的突破，哪怕只能做一两下，都将成为我们下一阶段巩固的目标。</p><h1 id="translation-deepl.com">Translation (DeepL.com)</h1><h2 id="some-thoughts-on-maintaining-fitness-habits-and-improving-physical-fitness-in-the-future">Some thoughts on maintaining fitness habits and improving physical fitness in the future</h2><p>In the long road of life ahead, we want to make fitness a pastime that is as deep into our daily lives as walking, running, and hiking and that we are always in the mood to do.</p><p>Combined with our experiences in junior and senior high school, where running, jumping rope, and seated forward bends became habitual through mandatory gym classes, and we gained a sense of accomplishment through continuous improvement,<strong>we can also make the most of our last semester at UBC, and make it a point to go to the gym every time we go to school before we leave - regardless of the length of time</strong>. Recently, it has been proven that this kind of "physical labor" effectively relieves our mental fatigue and improves our physical fitness after mental work, so it is a double win.</p><p>Another issue with this is how to make a breakthrough in strength while staying fit. If we have someone to accompany us in our workouts, then this will not be a problem. But what will we do when we work out alone? We might also draw inspiration from our middle and high school years when we practiced running, jumping rope, and seated forward bends alone.</p><p>First, we must be patient. Progress is bound to be a long process that cannot be rushed.</p><p>Secondly, we can do this by consolidating and improving - as much as possible over a longer period- without regressing, building steadily, maintaining and consolidating our current hard-won training gains. To do this, we need to keep track of the maximum intensity we are comfortable with each workout so that we can be mindful of what we do each time we work out.</p><p>Finally, from time to time, we must also try to break through the maximum intensity that we can accept, just like a perturbation. Each breakthrough, even if we can only do one or two reps, will be our goal for the next consolidation phase.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;original-chinese&quot;&gt;Original (Chinese)&lt;/h1&gt;
&lt;h2 id=&quot;关于今后健身习惯的保持与身体素质的提升的若干思考&quot;&gt;关于今后健身习惯的保持与身体素质的提升的若干思考&lt;/h2&gt;
&lt;p&gt;在今后漫长的人生之路上，我们要让健身成为一种像</summary>
      
    
    
    
    <category term="Reflections" scheme="https://abbaswu.github.io/categories/Reflections/"/>
    
    
  </entry>
  
  <entry>
    <title>Paper Reading: Asynchronous Functional Reactive Programming for GUIs (The Elm Paper)</title>
    <link href="https://abbaswu.github.io/2024/01/24/Paper-Reading-Asynchronous-Functional-Reactive-Programming-for-GUIs/"/>
    <id>https://abbaswu.github.io/2024/01/24/Paper-Reading-Asynchronous-Functional-Reactive-Programming-for-GUIs/</id>
    <published>2024-01-24T08:00:00.000Z</published>
    <updated>2024-01-31T07:11:34.544Z</updated>
    
    <content type="html"><![CDATA[<p>NOTE: This is a Paper Reading for the PL Reading Group. The original paper can be found <a href="https://people.seas.harvard.edu/~chong/pubs/pldi13-elm.pdf">here</a>.</p><h1 id="introduction">Introduction</h1><ul><li>Functional reactive programming (FRP) integrates pure functional programming with time-varying values (signals), useful for GUIs.</li><li>FRP allows modeling of complex, time-dependent relationships in a declarative style.</li><li>Previous FRP languages faced inefficiencies, including unnecessary recomputation and global delays.</li><li>Most FRP languages treat signals as continuously changing, leading to excessive sampling and recomputation.</li><li>Elm, an FRP language, treats all signals as discrete, reducing unnecessary recomputation by detecting unchanged signals.</li><li>In Elm, signals change only with discrete events (like user inputs), necessitating program recomputation.</li><li>Traditional FRP systems process events synchronously, causing delays if event processing is time-consuming.</li><li>Synchronous processing in GUIs can lead to unresponsiveness during long computations.</li><li>Elm introduces an abstraction for specifying asynchronous computations within FRP.</li><li>This feature in Elm allows concurrent execution of long-running computations and other events, maintaining GUI responsiveness.</li><li>Elm's approach to asynchronous computation in FRP is novel and is formalized in its language semantics.</li><li>Elm restricts signal use for efficient implementation, similar to previous efficient FRP systems.</li></ul><h1 id="core-language">Core Language</h1><ul><li>The core language of Elm, termed "FElm" (Featherweight Elm), is introduced, outlining Elm's key abstractions.</li><li>FElm is a simply-typed functional language with a set of reactive primitives.</li><li>It differentiates between simple types (like unit and int, and functions from simple types to simple types) and signal types (like <code>Signal[T]</code> and functions producing signal types).</li><li>Signals are conceptualized as streams of values, and input signals are required to have a default value.</li><li>Signal transformations and combinations are achieved using <code>lift_n</code> primitives, which apply functions to the current values of signals.</li><li>The <code>foldp</code> primitive performs computations on both current and previous signal values, acting like a fold operation on a signal.</li><li>An example of <code>foldp</code> is counting key presses using a signal that indicates the latest key pressed.</li><li>FElm's type system prohibits signals of signals to avoid potential computational inefficiencies and inconsistencies.</li><li>FElm programs evaluate in two stages: functional constructs are first evaluated, resulting in an intermediate term showing signal connections; then signals are evaluated in a push-based manner as new input values arrive.</li><li>Signal terms are represented as directed acyclic graphs, where nodes represent source nodes, <code>liftn</code> terms, and <code>foldp</code> terms.</li><li>An event occurs when a source node produces a new value, with a global event dispatcher ensuring total order and non-simultaneity of events.</li><li>Whenever an event occurs, all source nodes are notified by the global event dispatcher: the one source node relevant to the event produces the new value, and all other source nodes generate a special value noChange v, where v is the current (unchanged) value of the signal.</li><li>Nodes perform computations on signal values, with synchronous conceptual computation but pipelined execution for efficiency.</li><li>The <code>async</code> primitive allows for specifying asynchronous computations, enabling separation of long-running computations and maintaining GUI responsiveness. An <code>async</code> node creates a new source node. When an <code>async</code> node produces a value, it is treated like an event from the external environment associated from that new source node.</li><li><code>async</code> effectively divides the synchronous graph into a primary subgraph and multiple secondary subgraphs, maintaining event order within each subgraph but not globally, enhancing responsiveness without strict global event ordering.</li></ul><h1 id="building-guis-with-elm">Building GUIs with Elm</h1><ul><li>Elm encourages separation between reactive code and GUI layout code, using a purely functional and declarative approach for graphical layout.</li><li>Elm supports various base values, including strings, numbers, time, tuples, and graphical values like Elements and Forms.</li><li>Libraries in Elm offer data structures like options, lists, sets, and dictionaries.</li><li>Elm provides input support from devices like mouse, keyboard, touch, and also handles window attributes and network communication via HTTP.</li><li>It supports JSON, Markdown for text creation, let-polymorphism, recursive simple types, type inference, extensible records, and a module system.</li><li>Elm has two major graphical primitives: elements and forms.<ul><li>Elements are rectangles that can contain text, images, or videos.</li><li>Forms allow for non-rectangular shapes and text, including arbitrary 2D shapes with texture and color enhancements.</li></ul></li><li>Reactive GUIs in Elm interact with user input and environmental events using primitive signals.</li><li>Elm's signal identifiers include Mouse.position, Mouse.clicks, Window.dimensions, Time.every, Time.fps, Touch.touches, Touch.taps, Keyboard.keysDown, Keyboard.arrows, and Keyboard.shift.</li><li>Input components like text boxes, buttons, and sliders are represented as pairs of signals: an element for the graphical component and a value for the input.</li><li>The <code>Input.text</code> function in Elm allows for the creation of text input components, returning a pair of signals for the graphical input field and the current user input.</li></ul><h1 id="other-links">Other Links</h1><ul><li><a href="https://package.elm-lang.org/packages/evancz/automaton/latest/Automaton">Understanding the Automaton</a></li><li>Time-Travel Debugging (when Elm was still based on FRP)<ul><li><a href="https://web.archive.org/web/20160206080252/http://elm-lang.org/blog/interactive-programming">Interactive Programming</a></li><li><a href="https://web.archive.org/web/20160503091931/http://debug.elm-lang.org/">Elm’s Time Traveling Debugger</a></li><li>https://web.archive.org/web/20160504183927/http://elm-lang.org/</li><li><a href="https://www.youtube.com/watch?v=lK0vph1zR8s">Bret Victor style reactive debugging ‒ Laszlo Pandy</a></li><li><a href="https://www.youtube.com/watch?v=PUv66718DII">Bret Victor - Inventing on Principle</a></li></ul></li><li><a href="https://www.youtube.com/live/vHI7XlgmYCg?si=wWyHsFa6P6FHOZbw&amp;t=2324">Reason for a farewell to FRP: learning curve as Elm went mainstream</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;NOTE: This is a Paper Reading for the PL Reading Group. The original paper can be found &lt;a href=&quot;https://people.seas.harvard.edu/~chong/p</summary>
      
    
    
    
    <category term="Paper Reading" scheme="https://abbaswu.github.io/categories/Paper-Reading/"/>
    
    <category term="PL Reading Group" scheme="https://abbaswu.github.io/categories/Paper-Reading/PL-Reading-Group/"/>
    
    
  </entry>
  
  <entry>
    <title>Resources for Functional Array Programming for Data Science</title>
    <link href="https://abbaswu.github.io/2024/01/04/Resources-for-Functional-Array-Programming-for-Data-Science/"/>
    <id>https://abbaswu.github.io/2024/01/04/Resources-for-Functional-Array-Programming-for-Data-Science/</id>
    <published>2024-01-04T08:00:00.000Z</published>
    <updated>2024-01-21T23:34:21.897Z</updated>
    
    <content type="html"><![CDATA[<h1 id="functional-programming-in-r-focus-on-usability-contains-detailed-description-of-removing-for-loops">Functional Programming in R (focus on <em>usability</em>, contains detailed description of <em>removing for loops</em>)</h1><ul><li>http://modern-rstats.eu/functional-programming.html</li><li>https://appsilon.com/functional-programming-in-r-part-1/</li><li>http://adv-r.had.co.nz/Functional-programming.html</li><li>https://www.stat.umn.edu/geyer/8054/notes/functional.html</li><li>https://www.reddit.com/r/Rlanguage/comments/vxsf4p/is_r_a_functional_programming_language/</li></ul><h1 id="functional-programming-in-python-focus-on-usability-contains-detailed-description-of-removing-for-loops-broadcasting-persistent-ndarrays-jax-and-representing-multimodal-data-using-records-and-trees">Functional Programming in Python (focus on <em>usability</em>, contains detailed description of <em>removing for loops</em>, <em>broadcasting</em>, <em>persistent ndarrays (JAX)</em> and <em>representing multimodal data using records and trees</em>)</h1><ul><li>https://realpython.com/numpy-array-programming/</li><li>https://jax.readthedocs.io/en/latest/jax-101/07-state.html</li><li>https://data-apis.org/</li><li>https://github.com/docarray/docarray</li><li>https://jax.readthedocs.io/en/latest/pytrees.html</li></ul><h1 id="scientific-computing-in-ocaml-focus-on-ocaml-comprehensive-not-necessarily-pure">Scientific Computing in OCaml (focus on <em>OCaml</em>, comprehensive, not necessarily pure)</h1><ul><li>https://link.springer.com/book/10.1007/978-3-030-97645-3</li></ul><h1 id="functional-array-programming-per-se-focus-on-theory---rank-polymorphism-and-performance">Functional Array Programming Per Se (focus on <em>theory</em> - <em>rank polymorphism</em> and <em>performance</em>)</h1><ul><li>https://github.com/f5devcentral/shapeRank</li><li>https://prl.khoury.northeastern.edu/blog/2017/05/04/rank-polymorphism/</li><li>https://futhark-lang.org/publications.html</li></ul><h1 id="related-topics">Related Topics</h1><h2 id="arrays-vs.-linked-lists-in-functional-programming">Arrays vs. Linked Lists in Functional Programming</h2><ul><li>https://www.reddit.com/r/haskell/comments/hvxqzz/is_it_unfunctional_to_use_direct_access_arrays/</li></ul><h2 id="data-access-patterns">Data Access Patterns</h2><ul><li>http://www.nic.uoregon.edu/~khuck/ts/acumem-report/manual_html/ch05s02.html</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;functional-programming-in-r-focus-on-usability-contains-detailed-description-of-removing-for-loops&quot;&gt;Functional Programming in R (foc</summary>
      
    
    
    
    <category term="Code" scheme="https://abbaswu.github.io/categories/Code/"/>
    
    
  </entry>
  
  <entry>
    <title>Strategies, Tactics, and Mindset Learned from &quot;The Ph.D. Grind&quot;</title>
    <link href="https://abbaswu.github.io/2023/12/31/Strategies-Tactics-and-Mindset-Learned-from-The-Ph-D-Grind/"/>
    <id>https://abbaswu.github.io/2023/12/31/Strategies-Tactics-and-Mindset-Learned-from-The-Ph-D-Grind/</id>
    <published>2023-12-31T08:00:00.000Z</published>
    <updated>2024-01-06T17:30:23.190Z</updated>
    
    <content type="html"><![CDATA[<p>Note: This is a Paper Reading for Philip Guo's famous book "The Ph.D. Grind: A Ph.D. Student Memoir."</p><hr /><h1 id="main-strategies">Main Strategies</h1><ul><li>Be careful when choosing advisors and collaborators.</li><li>Consider the background and incentives of the advisors and collaborators, as evidenced by recent papers, grant applications, and future aspirations.</li><li>Think about what you want to do, what that work is like, and how that aligns with mutual interests.</li><li>A match in research philosophy does not imply feeling comfortable working together.</li><li>Be extremely careful working with people with a grind mindset.</li></ul><blockquote><p>I found a master's thesis advisor, and like any ambitious student, I began proposing my own partially developed research project ideas to him. <strong>My advisor patiently entertained my ideas but ultimately convinced me to focus on more conventional research topics that aligned with both his academic interests and, more importantly, the conditions of his grant funding. Since my master's program tuition was partially covered by a research grant my advisor had obtained from the U.S. government, I was obligated to work on projects within the scope of that grant.</strong> Therefore, I followed his recommendations and dedicated two and a half years to developing prototype tools for analyzing the runtime behavior of computer programs written in the C and C++ languages.</p></blockquote><blockquote><p>When I arrived on campus, Dawson was a recently-tenured professor who had been at Stanford for the past eight years; professors usually earn tenure (a lifetime employment guarantee) if they have published enough notable papers in their first seven years on the job. Dawson's main research interest was in building innovative tools that could automatically find bugs (errors in software code) in complex pieces of real-world software. Over the past decade, Dawson and his students built several tools that were able to find far more bugs than any of their competitors. Their research techniques were so effective that they created a successful startup company to sell software bug-finding services based on those techniques. <strong>Although I somewhat liked Dawson's projects, what appealed more to me was that his research philosophy matched my own: He was an ardent pragmatist who cared more about achieving compelling results than demonstrating theoretical "interestingness" for the sake of appearing scholarly.</strong></p><p>During my first meeting with Dawson, he seemed <strong>vaguely interested</strong> in my broader goals of making computer usage and programming more productive. <strong>However, he made it very clear that he wanted to recruit new students to work on an automatic bug-finding tool called Klee that his grant money was currently funding.</strong> (The tool has had several names, but I will call it "Klee" for simplicity.) From talking with other professors and senior Ph.D. students in my department, I realized it was the norm for new students to join an existing grant-funded research project rather than to try creating their own original project right away. I convinced myself that automatically finding software bugs was an indirect way to make programmers more productive, so I decided to join the Klee project.</p></blockquote><blockquote><p>Even though none of my particular ideas managed to persuade Scott, he was still interested in collaborating with me to develop a project aligned with my broader interests. <strong>During that period, Scott held the position of an assistant professor, aiming to secure tenure at Stanford, and had been there for just three years. Consequently, he was eager to publish more papers as part of his tenure quest.</strong> As I was funded by a fellowship, Scott didn't need to allocate funds from his grants to support me, which made the collaboration appealing to him without any significant downsides.</p></blockquote><blockquote><p>In hindsight, I can now see why this project was likely to face challenges due to <strong>misaligned incentives</strong>, but at the time, I lacked the wisdom to anticipate such issues. I had decided to become a Klee assistant for Cristi and Dawson because I wanted to join an experienced older Ph.D. student and a professor who had a track record of publishing papers in their specific subfield. This approach had worked exceptionally well the previous year when I collaborated with Joel, an older Ph.D. student, and Scott, a professor, on their HCI project, which resulted in a top-tier, award-nominated paper.</p><p>So, what was different in this case? <strong>In short, neither Cristi nor Dawson had a strong urge to publish. They had already authored several Klee papers together, and a cross-checking paper co-authored with me would have been a "nice-to-have" but not an obligatory follow-up publication. Cristi was in the final year of his Ph.D. and didn't require further papers to graduate, while Dawson had already secured tenure and wasn't in a hurry to publish more. In contrast, Joel was a mid-stage Ph.D. student eager to publish the first paper of his dissertation, and Scott was an assistant professor who needed to publish prolifically to earn tenure.</strong> These two contrasting experiences taught me the crucial importance of thoroughly understanding the motivations and incentives of potential collaborators before embarking on a project with them.</p></blockquote><blockquote><p><strong>I believe that Dawson expected Peter and me to achieve publishable results at a faster pace, which may have led him to perceive us as either incompetent or not fully committed to our work. It's a harsh reality that, as a professor at a top-tier university, Dawson's students are likely less accomplished than he was during his own Ph.D. years.</strong> The explanation is quite straightforward: Only about 1 out of every 75 Ph.D. students from a top-tier university typically possesses the qualities necessary to become a professor at an institution like Stanford (or perhaps 1 out of every 200 Ph.D. students from an average university). Predictably, neither Peter nor I met those exceptional standards. If Dawson had partnered with a younger version of himself, progress may have been different.</p></blockquote><blockquote><p>Two years after Peter and I departed from the Klee project, Dawson eventually found a new Ph.D. student who could successfully bring his Klee-UC vision to fruition. In 2011, Dawson and his new student published a significant paper that incorporated both Klee-UC and cross-checking ideas. Ultimately, it took three attempts involving four different Ph.D. students over five years before Dawson's original Klee-UC concept materialized into a published paper. Of those four students, only one persevered— I left the Klee project, and two others decided to exit the Ph.D. program altogether. From an individual student's standpoint, the chances of success appeared rather low.</p><p>From a professor's perspective, however, Klee-UC represented a resounding success. Since Dawson held tenure, his job was never at risk. <strong>In fact, one of the purposes of tenure is to enable professors to take risks by pursuing more ambitious project ideas. However, the downside of this privilege is that professors often assign students to work on these risky projects, which may have lower success rates. Students often find it challenging to decline such assignments, especially if they are financially supported by their advisors' grants.</strong> Fortunately, as I was funded by fellowships, it was considerably easier for me to discontinue my involvement in the Klee project.</p></blockquote><blockquote><p><strong>Tom's extensive experience in publishing and reviewing numerous empirical software measurement papers made him an invaluable "insider" who understood what types of results and paper writing were well-received by reviewers in that specific subfield. When it came time to submit our paper at the end of that summer, Tom skillfully positioned our contributions within the context of related work, presented compelling arguments for the novelty and significance of our findings, and meticulously polished our paper.</strong> Three months later, I was thrilled to receive the news that our paper, which focused on studying the causes of bug fixes, had been accepted at a top-tier conference. This was particularly impressive given that only 14 percent of all papers submitted that year were accepted.</p><p>However, Tom's dedication didn't stop there. <strong>As a newly-hired researcher at MSR, he was motivated to build his reputation by publishing additional papers. In the following years, we leveraged the results from my summer 2009 internship to write two more top-tier conference papers.</strong> One of these papers explored bug report reassignments, while the other delved into bug report reopenings and even earned a Best Paper Award.</p></blockquote><blockquote><p>During my first month in the new phase of my academic journey, I primarily spent my time reconnecting with old college friends, as my alma mater, MIT, was conveniently located near Harvard. Additionally, I had several meetings with Margo to explore potential research ideas. Margo was open to the idea of me pursuing my own project under her loose supervision, granting me a considerable degree of intellectual freedom. <strong>However, I approached my brainstorming process pragmatically because I aimed to generate a project that would genuinely excite her and secure her strong support for its inclusion in my dissertation. To achieve this, I delved into her recent papers and grant applications to gain insight into her research philosophy.</strong> I tailored my ideas to align with her preferences, recognizing the importance of harmonizing with the subjective inclinations of senior collaborators, as well as the expectations of paper reviewers, even within fields that are considered technically objective.</p><p>At that time, reading a grant proposal was an entirely novel experience for me, and it appeared foreign and unfamiliar. However, with time and practice, I have since become accustomed to writing grants, and it has become a routine part of my academic life.</p></blockquote><hr /><ul><li>Apply to fellowships before starting the Ph.D. for better academic freedom and advisor-advisee relationship.</li></ul><blockquote><p>I was also fortunate to receive two prestigious fellowships, <strong>the NSF and NDSEG graduate research fellowships</strong>. These fellowships were granted to only about five percent of all applicants. <strong>They covered the full expenses for five out of the six years of my Ph.D. studies and relieved me from the obligations of working on specific grant-funded projects. This was a significant advantage over students who had to work on such projects throughout their college years.</strong></p></blockquote><blockquote><p><strong>Applying to Ph.D. programs and fellowships during my master's year</strong> gave me a huge advantage over students who applied during senior year of college, since I had an extra year of research experience.</p></blockquote><blockquote><p>In other fields, such as the humanities and social sciences, students typically do not receive direct funding from their advisors. <strong>This distinction significantly changes the dynamics of the advisor-advisee relationship, turning the Ph.D. experience into more of a solitary journey and less of an employer-employee arrangement.</strong></p></blockquote><blockquote><p>However, I soon came to the realization that I wasn't obligated to remain tethered to Klee in any way, given that my funding came from the NDSEG fellowship rather than Dawson's grants. In contrast, all of Dawson's other students had no option but to persist with their work on Klee, as they were supported by his Klee-related grants. Therefore, I retained Dawson as my advisor but departed from the Klee project, embarking on the journey to create my own research project entirely from scratch.</p></blockquote><blockquote><p>From a professor's perspective, however, Klee-UC represented a resounding success. Since Dawson held tenure, his job was never at risk. <strong>In fact, one of the purposes of tenure is to enable professors to take risks by pursuing more ambitious project ideas. However, the downside of this privilege is that professors often assign students to work on these risky projects, which may have lower success rates. Students often find it challenging to decline such assignments, especially if they are financially supported by their advisors' grants.</strong> Fortunately, as I was funded by fellowships, it was considerably easier for me to discontinue my involvement in the Klee project.</p></blockquote><hr /><ul><li>Question each step in the decision-making process.</li><li>Have an outline or draft of the research paper, especially the evaluation section, before starting a research project.</li><li>This brings the obvious benefit of making research contributions.</li><li>It also forces everyone not to push you around, helping you to jump out of the "pecking order" slowly and surely.</li></ul><blockquote><p>Dawson believed that Klee could uncover new bugs that no automated tool or human being had previously discovered within the code of thousands of Linux device drivers. <strong>I recall thinking that while finding new Linux device driver bugs could be interesting to present in a paper, it wasn't entirely clear to me how these results constituted a substantial research contribution.</strong> To my understanding, my role was to use Klee to uncover new bugs, essentially applying existing research, rather than significantly enhancing Klee in an innovative manner. <strong>Moreover, I couldn't envision how my project would seamlessly integrate with the projects of the other five students for a coherent paper submission in March.</strong> Nevertheless, I had faith in Dawson's high-level paper writing strategy. <strong>Since I had recently joined the project, I didn't want to immediately question these decisions typically made by professors. I was assigned a specific task, and I was determined to carry it out to the best of my abilities.</strong></p></blockquote><blockquote><p>My rational understanding acknowledged that experimental research in science and engineering fields often demands an extensive amount of unglamorous and labor-intensive work to produce tangible results. Ph.D. students, particularly those in their first and second years, are typically the ones tasked with undertaking the most tedious tasks—this is essentially what we are compensated for. In a typical research group, the professor and senior Ph.D. students formulate the high-level project plans and then delegate the responsibility of making all the intricate details function in practice to the junior students. First- and second-year students usually have minimal influence on the overall direction of the group's project. <strong>Although I fully embraced my position as the lowest-ranking member of the team, my emotional state still suffered significantly during those initial months because the work was exceptionally challenging and lacked immediate rewards.</strong></p></blockquote><blockquote><p>I met with Dawson to express my frustration regarding the overwhelming task I was currently tackling. It felt ludicrous to spend several days configuring Klee for each new device driver. Not only was it physically exhausting, but it also didn't seem like genuine research. What could we possibly write in our paper? That I had devoted nearly 1,000 hours to manual labor in getting Klee to function with device drivers without gaining any meaningful insights? It didn't feel like a valuable research contribution; it seemed rather futile <strong>Additionally, panic set in as there were only five weeks left until the paper submission deadline, and Dawson had yet to discuss our group's paper writing strategy.</strong> Typically, writing a respectable paper submission takes a minimum of four weeks, especially when coordinating efforts among six students involved in the project.</p></blockquote><blockquote><p>However, a significant problem arose. <strong>When we finally achieved those favorable results, there were only three days left until the paper submission deadline, and not a single word of the paper had been written yet.</strong> In such a short timeframe, it was physically impossible to write, edit, and refine a paper submission that had any chance of being accepted at a top-tier computer science conference. Nevertheless, we decided to give it our best shot.</p><p>During the final 72 hours leading up to the deadline, Dawson and five of us students (one had dropped out of the project by this point) practically lived in the office, pulling two consecutive all-nighters to wrap up the experiments and draft the paper. Deep down, all of us students realized that there was virtually no chance that this paper would be accepted, but we followed Dawson's lead and pressed on.</p><p><strong>The result was a submission that can only be described as a disorganized mess – it contained numerous typos, nonsensical sentence fragments, graphics lacking explanations, and lacked concluding paragraphs.</strong> It was a dismal sight. At that moment, I couldn't fathom how I would ever complete a Ph.D. if it meant working in such a chaotic and haphazard manner. As anticipated, three months later, the reviews for our paper were overwhelmingly negative, filled with harsh comments like, "The program committee believes that this paper is far too sloppily prepared to warrant acceptance; please refrain from submitting papers that are clearly unready for review."</p></blockquote><blockquote><p>My friend Greg, who was one of Rob's Ph.D. students, emphasized the significance of the third point: <strong>thinking about experiments when suggesting research project ideas</strong>. <strong>Professors are often driven by the desire to have their names associated with published papers, and in the field of computer science, conference papers typically require robust experiments to secure acceptance for publication.</strong> Therefore, it's essential to consider experiment design right from the outset when formulating project proposals.</p></blockquote><hr /><ul><li>Target fellow researchers with similar incentives when conducting HCI studies in academia.</li><li>Without exceptionally strong resources, find a novel, meaningful niche for research instead of an overcrowded and highly competitive domain.</li><li>Do not attempt to do in academia what should be done in industry.</li></ul><blockquote><p>In hindsight, I'm not astonished that my efforts to shadow professionals in their workplaces were unsuccessful. I had nothing to contribute to these seasoned programmers; my presence would have likely disrupted their workday. <strong>Fortunately, a few years later, I had the opportunity to observe a different group of programmers—fellow graduate students engaged in programming for scientific research. They were open to my occasional inquiries and more than willing to discuss their working environments.</strong> These interviews would ultimately serve as a direct source of inspiration for my dissertation work.</p></blockquote><blockquote><p>Dawson and I encountered significant challenges in getting our research results published. Over the course of a year, we submitted two papers that were both rejected. It would take another full year before our work was finally published as a shorter-length paper in a second-tier conference, which held minimal prestige and did not count as a contribution to my dissertation. However, by that point, I had already moved on to other projects.</p><p>The primary reason behind our struggles with publication was that we were not considered "insiders" in the empirical software measurement subfield (sometimes referred to as empirical software engineering), to which our project belonged. When Dawson and I embarked on this work, numerous research teams from various universities and corporate research labs were already engaged in similar endeavors. <strong>We were clearly outmatched by the competition, which included professors and research scientists specializing in empirical software measurement, guiding armies of Ph.D. students through the extensive data analysis. These individuals were eager to publish a multitude of papers, especially young professors aspiring to attain tenure. They possessed expertise in statistical methodologies, framing related work, and crafting persuasive narratives needed to secure acceptance for such papers. Most significantly, they frequently served on program committees and acted as external reviewers for relevant conferences, which provided them with in-depth knowledge of the requisites for producing publishable papers in this subfield.</strong></p></blockquote><blockquote><p>One significant advantage of being an intern at MSR was access to a wealth of internal data sets containing information about Microsoft's software bugs and personnel files. These confidential data sets would have been inaccessible to me as an external researcher. <strong>The richness of these Microsoft data sets provided MSR researchers like Tom with a distinct advantage, making it easier to obtain groundbreaking and publishable results compared to competitors who lacked access to such data.</strong></p><p>In contrast, when I worked with Dawson, the Linux data sets I had access to were smaller and of lower quality. Open-source software projects typically do not maintain records as meticulously as one of the world's largest software companies. <strong>This limitation is something that all university researchers face unless they establish partnerships with companies that can provide them with access to relevant data.</strong></p></blockquote><blockquote><p>Upon returning to Stanford in the fall of 2009, inspired by my previous HCI work with Scott and Joel during my second year, I embarked on a project to interview colleagues who used Python for data analysis in their research. <strong>The objective was to identify the programming-related inefficiencies they faced and explore how IncPy could address and eliminate these inefficiencies.</strong> I also leveraged my connections to give presentations about IncPy, even though it was still a half-baked idea at that stage, at various lab group meetings. These early efforts helped generate fresh ideas and refine the project's "marketing pitch." I'm deeply thankful for the friends who supported me in kickstarting my project when I had little more than a few rudimentary PowerPoint slides.</p><p>As I continued my interviews and refined my design plans, I grew increasingly optimistic. I discovered that researchers in various computation-based fields, including machine learning, pharmacology, bioengineering, bioinformatics, neuroscience, and ocean engineering, all faced similar challenges in their data analysis workflows, making them potential beneficiaries of IncPy. <strong>After a few weeks of interviews and subsequent adjustments to my project's direction, I felt confident that I could convincingly pitch the idea in a future paper submission. The core argument I aimed to convey was that many computational researchers across diverse fields grappled with common inefficiencies in their daily programming tasks, and IncPy presented a novel, fully automated solution to these inefficiencies that had not been previously implemented. This initial pitch would ultimately become the central theme of my entire dissertation.</strong></p></blockquote><hr /><ul><li>Climb the shoulders of giants as much as possible and pick the low-hanging fruit from there before you become a giant.</li></ul><blockquote><p>Following the creation of Klee and related projects between 2005 and 2008, a new subfield emerged. <strong>This development led to numerous assistant professors and young research scientists eagerly producing a plethora of papers, each detailing incremental improvements in their quest to secure tenure or job promotions.</strong> It was akin to an academic gold rush, spurred by the early insights of Cristi, Dawson, and a select few pioneers. Since Dawson already possessed tenure and had gained fame for his contributions, he was above the fray and lacked the desire to publish solely for the purpose of bolstering his academic resume.</p><p><strong>In practice, Ph.D. students collaborating with these young researchers found it comparatively easier to publish their work and complete their graduate programs, while Dawson's students faced considerably more challenges.</strong> Over the three years since I departed from the Klee project, research groups worldwide have collectively published hundreds of papers grounded in Klee-like concepts. Remarkably, fifteen of these papers detailed enhancements to Klee itself, as our laboratory released it as open-source software to encourage further research. <strong>In the meantime, five of Dawson's Ph.D. students have made serious efforts to work on Klee; however, only one has managed to publish a single paper on Klee-UC.</strong></p></blockquote><hr /><ul><li>Actively expand your network and seek collaboration opportunities.</li></ul><blockquote><p>Just before commencing my second year of the Ph.D. program in September 2007, I took a one-week vacation to Boston to visit friends from college. While in the area, I reached out to a few MIT professors I knew from my undergraduate years, seeking their guidance. During our meetings, they all conveyed a similar message: <strong>Take the initiative to engage with professors, explore research topics of mutual interest, and above all, avoid isolation.</strong> This straightforward advice, consistently applied over the next five years, ultimately paved the way for a successful completion of my Ph.D. journey.</p><p>I wasted no time in taking this advice to heart while still in Boston. I sent a cold email to an MIT computer science professor named Rob, politely requesting a meeting with him. In this initial email, I briefly introduced myself as a recent MIT graduate and a current Stanford Ph.D. student with a keen interest in developing tools to enhance the productivity of computer programmers. Given that I knew Rob shared an interest in this research area, I hoped my email would pique his interest rather than end up in his spam folder. Fortunately, Rob generously agreed to meet with me for an hour in his office, during which I presented a few project proposals and sought his feedback. He appeared to find merit in my ideas, which bolstered my confidence that they held promise in the eyes of a professor working in this research domain. Regrettably, I couldn't collaborate with Rob as I was no longer an MIT student. <strong>Nonetheless, at the conclusion of our meeting, Rob suggested that I approach a Stanford computer science professor named Scott to see if I could garner his interest in my ideas.</strong></p></blockquote><blockquote><p>The lasting impact of an MSR (Microsoft Research) internship often extends beyond research achievements to the friendships forged during the experience. <strong>During that particular summer, I had the privilege of forming connections with some of the brightest and most inspiring young computer science researchers of my generation.</strong> For example, one of my three office mates was on the verge of beginning her Ph.D. journey at MIT and had already published more top-tier papers during her undergraduate research than most Ph.D. students could ever aspire to. Another office mate was a UC Berkeley Ph.D. student who dedicated his nights and weekends to a separate research project with collaborators from across the country, all while diligently working on his internship project during workdays. These peers are likely to evolve into award-winning professors, research leaders, and high-tech entrepreneurs, and I am genuinely humbled to have had the opportunity to share a summer with them.</p></blockquote><hr /><ul><li>This is how we can evaluate productivity claims.</li></ul><blockquote><p>From the very beginning of my IncPy project, I recognized the challenge of presenting a compelling evaluation. <strong>The core premise, that IncPy could enhance the productivity of computational researchers, was inherently subjective. To address this, after studying similar papers, I developed a two-pronged evaluation approach</strong>:</p><p><strong>Case Studies: I planned to gather a variety of Python programs from computational researchers and simulate the productivity gains they might have achieved using IncPy instead of standard Python.</strong></p><p>Deployment: The goal was to encourage researchers to incorporate IncPy into their regular work, allowing them to <strong>directly experience and report on its impact on their productivity</strong>.</p></blockquote><blockquote><p>In pursuit of this, <strong>I adopted the roles of both salesman and beggar, persistently seeking Python programs from colleagues for my case studies and encouraging them to use IncPy in their research.</strong> Despite mostly receiving negative responses, I continued asking for referrals and volunteered to speak at various lab meetings to generate interest in IncPy. After months of effort, I managed to acquire Python programs from researchers across various fields, sufficient for starting my case studies.</p></blockquote><blockquote><p>As I concluded my fourth Ph.D. year in September 2010, I submitted my IncPy paper to a top-tier conference. The paper included <strong>case studies and a few deployment anecdotes</strong>. Aware of the low acceptance rate and the unconventional nature of IncPy within academic fields, I was prepared for potential rejection but still aimed high, knowing the value of a top-tier publication for my graduation prospects.</p></blockquote><hr /><ul><li>The runtime is a vital (yet often overlooked) aspect to consider in programming language research.</li></ul><blockquote><p>On July 29, 2010, a year after the initial concept of IncPy was born, I was struck by another idea, this time addressing a common issue in computational research. <strong>I noticed that researchers often write their computer programs in an ad-hoc, somewhat careless manner, leading to frequent crashes for trivial reasons. These crashes not only prevent the production of any results but also cause considerable frustration.</strong></p><p><strong>My realization was that by modifying the runtime environment of the Python programming language, specifically the interpreter, I could address this issue.</strong> The idea was to adapt the Python interpreter in a way that would allow these less rigorously written programs to still run and produce partial results, rather than failing completely and producing none. I decided to name this modified version of the Python interpreter "SlopPy," a playful blend of 'Sloppy' and 'Python', emphasizing its tolerance for less meticulous coding practices. This concept aimed to make the process of data analysis more forgiving and efficient for researchers who may not always adhere to stringent coding standards.</p></blockquote><hr /><ul><li>Focus on acknowledging shortcomings and deriving their insights if experimental evaluation results are suboptimal.</li></ul><blockquote><p>While I was interning at Google during the summer of 2011, I received the joyful news that our ProWrangler paper had been accepted with outstanding reviews. The main factor contributing to our success was Jeff's exceptional work in crafting both the introduction of our paper and the interpretation of our evaluation results. <strong>Initially, our user testing had not demonstrated the productivity improvements we had hoped for, which made me concerned that our paper might face rejection. However, Jeff's skill in technical writing and framing our arguments skillfully transformed what seemed like impending failure into a surprising victory. The reviewers appreciated our candid acknowledgment of the shortcomings in our evaluation and the valuable insights we extracted from them.</strong> Undoubtedly, our paper would not have gained acceptance without Jeff's rhetorical expertise. He had accumulated substantial experience in this area, having published 19 papers during his Ph.D. studies, mostly in top-tier conferences, which is five to ten times more than what is typically expected from computer science Ph.D. students. This level of dedication and productivity is often necessary to secure a faculty position at a prestigious university like Stanford.</p></blockquote><hr /><ul><li>In fiercely competitive domains, do not self-consciously try to network and seek opportunities, and never schmooze. Instead, rely on bypasses and side roads. Many things grow in the garden that were never sown there（有心栽花花不开，无心插柳柳成荫）.</li></ul><blockquote><p><strong>When I made the decision to leave academia, one of the immediate impacts was that I no longer felt the need to engage in networking activities at the three academic conferences I attended that summer. These conferences included talks I gave on IncPy, SlopPy, and CDE. Academic conferences are typically filled with senior Ph.D. students, postdocs, and pre-tenure professors who are actively networking to impress their more senior colleagues. For these junior researchers, professional networking at conferences is a crucial and time-consuming task as it greatly influences their budding careers and academic reputations. However, since I had decided to step away from the academic world, I found myself enjoying the conferences without the usual nervousness or strategic calculations.</strong></p><p>At one of these conferences, I had a casual conversation with John, a professor from the University of Utah who was the keynote speaker. Later on, he wrote a generous blog post that significantly increased the popularity of "The Ph.D. Grind" among professors. He also provided me with practical advice regarding the possibility of returning to academia myself. <strong>This unexpected turn of events occurred because I was no longer driven by a desire to stay in academia and simply chatted with people I found interesting at the conference without a specific networking agenda.</strong> It was a reminder of how unpredictable life can be.</p><p>During a break between sessions at another conference, I noticed Margo sitting alone and working on her laptop. I had previously met Margo during my fourth year at a San Jose workshop where I presented my original IncPy paper. Although I had some reservations about approaching her and reintroducing myself, fearing that she might not remember me or that our conversation might lack substance, my impending departure from academia meant I had no real networking agenda to uphold. I decided to take the chance and greeted her. I reminded her of our previous encounter, and she appeared to recall me. We had a brief five-minute conversation about my new CDE project before I had to rush off to give my talk. After returning home, I sent her a courteous follow-up email with a link to the CDE project webpage, in case her students were interested in using it for their research. This was my standard polite approach when introducing CDE to professional colleagues, and I didn't have high expectations for follow-up.</p><p>As it turned out, Margo would later play a pivotal role in helping me secure a professorship. <strong>Ironically, if I had initially desired a professorship, I might have been too self-conscious and hesitant to approach her in the first place, which could have diminished my chances of eventually obtaining the position. In the end, it was because I didn't actively seek a professorship at the time that I ultimately ended up getting one.</strong> Life has its own unique way of working things out!</p></blockquote><blockquote><p><strong>The culmination of my graduate school journey wouldn't have been possible if I hadn't actively seized the opportunities that I was fortunate enough to receive.</strong> If Robert hadn't informed me about the San Jose workshop two years ago, if I hadn't submitted and presented my IncPy paper there, if Margo hadn't taken an interest in my paper and introduced me to Elaine, if I hadn't maintained contact with Elaine, if I hadn't spontaneously approached Margo again at last summer's conference where I presented CDE, if she hadn't sent me a gracious follow-up email, and if I hadn't taken a risk with my unconventional counterproposal to her, then <strong>I would have still been at Stanford struggling to find one last project and thesis committee member</strong>.</p><p><strong>It's important to acknowledge that achieving this outcome required me to try many different approaches like this, and most of them did not yield the desired results.</strong> Success often involves numerous attempts and failures before finding the right path.</p></blockquote><h1 id="supportive-tactics">Supportive Tactics</h1><ul><li>Optimize the process of note-taking for research.</li></ul><blockquote><p>My daily routine primarily revolved around the development of computer programs designed to extract, clean, reformat, and analyze data from the Linux revision control history and the 2,000 bug reports. <strong>In my pursuit of gaining insights, I independently acquired a foundational understanding of quantitative data analysis, statistics, and data visualization techniques. Throughout my work, I meticulously recorded my experimental progress in a research lab notebook, carefully documenting which trials were successful and which were not.</strong></p><p>Every week or so, I would meet with Dawson to present my findings. <strong>Typically, these meetings involved me presenting him with printouts of graphs or data tables generated through my analyses, followed by him offering high-level suggestions, such as, "This part of the graph appears unusual; can you explain why? Try breaking down the data in this manner and delve deeper."</strong> It was only years later that I discovered this working style was relatively common among computational researchers in various academic disciplines. For my dissertation, I went on to develop tools aimed at streamlining the typical inefficiencies in this prevalent workflow. However, at that time, I had no such long-term vision; my primary goal was to make intriguing discoveries and have them published.</p></blockquote><hr /><ul><li>Raise encountered problems and frustrations, no matter how minor, at meetings and complain.</li><li>Actively contact people for feedback, motivation, and emotional support.</li></ul><blockquote><p>I found myself navigating unfamiliar territory, making it significantly more challenging to seek assistance compared to my undergraduate years when solutions were more straightforward. As the sole person working with Klee on device driver code, my colleagues couldn't offer much guidance. <strong>While Dawson occasionally provided high-level strategic advice, as is customary for tenured professors, his role didn't involve being directly involved in the day-to-day challenges we faced. It fell upon us, the students, to decipher all the intricate details necessary to yield results—my task being to uncover new bugs in Linux device drivers that had not been previously identified. Professors often reiterate the mantra, "If it's already been done before, then it wouldn't be research!"</strong> For the first time, I truly grasped the essence of those words.</p></blockquote><blockquote><p>For the following ten weeks, I found myself daydreaming about my own research concepts in complete isolation, without engaging in any conversations with others. Given my negative initial experience working in a research group over the past few months, I craved solitude to think independently. Dawson was supportive of my absence since he wasn't financially supporting me through his grants.</p><p>I lived in complete seclusion, mentally drained but still attempting to make gradual progress. Each day, I dedicated myself to reading numerous computer science research papers and taking notes in the hope of finding inspiration for my own creative ideas. However, lacking proper guidance or context, I often ended up squandering a lot of time without gaining meaningful insights from my readings. I also roamed aimlessly on my bicycle through the neighborhoods around campus, hoping to spark new research ideas to no avail. Most notably, I procrastinated more than I ever had in my life up to that point: I watched countless TV shows, took numerous naps, and wasted countless hours online. Unlike my friends with conventional nine-to-five jobs, there was no supervisor monitoring my daily activities, so I allowed my mind to wander without any structure in my life.</p><p>During those ten solitary weeks, I scarcely spoke to anyone, not even my friends or family. Complaining seemed futile because it felt like nobody could truly grasp what I was going through at the time. My friends who were not pursuing Ph.D. programs believed I was simply "in school" and taking classes like a typical student. Meanwhile, the few friends I had made in my department were grappling with their own first-year Ph.D. struggles, primarily the shock of diving headfirst into complex, open-ended research problems without the ability to influence the overarching direction of their assigned projects. We, as young computer scientists, willingly engaged in tasks that were both exceptionally challenging and seemingly devoid of purpose, all while earning a quarter of the salary of our friends in the corporate world. It was so disheartening that it became almost comically tragic. <strong>However, I didn't believe that group complaining would be productive, so I chose to remain silent. I avoided the Computer Science Department building, fearing encounters with colleagues who might inevitably inquire about my work, and I had no respectable response to offer. Instead, I preferred secluding myself in libraries and coffee shops.</strong></p></blockquote><blockquote><p>In hindsight, going solo so early in my graduate school journey was a regrettable decision. Contrary to romanticized notions of a solitary scholar sitting outdoors, sipping a latte, and scribbling on blank notebook pages, real research is never conducted in isolation. It necessitates a solid foundation in intellectual, historical, and sometimes even physical aspects (such as laboratory equipment) to develop innovative ideas. <strong>A wiser approach during those weeks would have been to communicate with Dawson more frequently and actively seek collaborations with other professors or senior students. However, at that time, I was so burnt out and frustrated with the traditional hierarchy of group-based research – which often involved new Ph.D. students performing the most unglamorous tasks – that I retreated and embarked on my own path.</strong></p></blockquote><blockquote><p>By the middle of my third year, many of my fellow students and I found ourselves in a state of "limbo." <strong>It became increasingly challenging to muster the motivation to come into the office day in and day out. We also grappled with feelings of isolation and loneliness, as we spent our days and nights immersed in tackling obscure, highly specialized problems that few people in our immediate surroundings comprehended or showed interest in.</strong> While our advisors and senior colleagues occasionally offered high-level guidance, they seldom sat down with us to work through the intricate details of our research endeavors.</p></blockquote><hr /><ul><li>If there is work-life balance in a job and stress levels are not too high, i.e., one is not drained or overburned, would it be a viable opportunity to pursue self-improvement and open doors for future pursuits?</li></ul><blockquote><p>Of course, it would be foolish to pursue a Ph.D. solely out of irrational childhood fears. <strong>To get a preview of corporate working life, I did internships at engineering companies every summer during college. Since I happened to work in offices where I was the only intern, I was given the full responsibilities of a junior engineer, which was a rare privilege. Although I learned a lot of technical skills, I found the day-to-day work to be mind-numbingly dull.</strong> My coworkers were also unenthusiastic about their jobs, and there were few appealing prospects for career advancement. Of course, I'm not claiming that all engineering jobs are mind-numbingly dull; it just happened that the companies I worked for were not first-rate. Many of my college friends who interned at first-rate companies such as Microsoft had great experiences. Ironically, my first full-time job after finishing my Ph.D. was at Google. Google loved their experiences and signed on to work at those companies full-time after graduation.</p></blockquote><hr /><ul><li>Research software tends to be rough prototypes. Keep them simple, stupid.</li></ul><blockquote><p><strong>Similar to other sophisticated software tools, Klee featured numerous configurable options. However, since it was a research prototype assembled by students, most of these options lacked clear documentation regarding their behaviors.</strong> Consequently, I spent a significant amount of time grappling with misunderstandings about the subtle interactions between these options as I adjusted them. My research lab notebook became filled with frustrations, including entries like: "OH SH*T, I believe my mistake was failing to realize that there are specific options meant to be passed into Klee (e.g., -emit-all-errors), and others that should be passed into the target program to set up the model environment (e.g., --sym-args). When these get confused, bizarre outcomes occur because Klee ends up executing the target program with argc and argv values that differ from what you'd expect."</p></blockquote><blockquote><p>From a research perspective, my goal was achieved: <strong>I successfully developed an initial prototype of CDE and demonstrated its functionality in a practical use case. In many applied engineering fields, it's widely accepted that research prototypes like CDE primarily serve to prove the feasibility of innovative concepts. The role of a researcher involves creating prototypes, conducting experimental assessments to measure their effectiveness, publishing research papers, and then moving on to the next idea.</strong> It would be unrealistic for a researcher to expect people to use their prototypes as if they were fully-fledged products. <strong>Instead, if the ideas are promising, professional engineers may incorporate them into their company's future products.</strong> At best, some other research groups might utilize your prototypes as a foundation to develop their own prototypes and subsequently reference your work in their papers (for example, more than a dozen university research groups have expanded upon the Klee tool and published papers detailing their enhancements). However, it is exceedingly rare for individuals outside of the research community to employ research prototypes in their daily tasks. In essence, the objective of academic research is to generate validated ideas, not refined products.</p></blockquote><hr /><p>Make full use of workshops to disseminate preliminary results and collect feedback rapidly, even if it involves spending money yourself.</p><blockquote><p>Typically, I wouldn't have paid much attention to such an announcement for two reasons. Firstly, Robert's research area, data provenance, had no direct relevance to IncPy, so his paper submissions didn't affect me. Secondly, in our department, workshop papers don't usually count as substantial contributions toward a dissertation. <strong>Workshops are primarily meant for sharing early-stage ideas and tend to have much higher acceptance rates (60 to 80 percent) compared to conferences (8 to 30 percent). Many professors prefer their students to focus on publishing in conferences since workshops require them to cover travel, hotel, and registration costs, similar to conferences, but without the same level of prestige.</strong> Consequently, top-tier computer science professors often encourage their students to prioritize conference papers over workshop submissions.</p></blockquote><blockquote><p>While presenting my IncPy workshop paper was beneficial for feedback and networking, particularly with Margo, it didn't qualify as an official publication for my dissertation. I was aware that publishing this work at a recognized conference in my department was necessary. <strong>The main difference between a workshop and a conference paper lies in the requirement for a conference paper to have a robust experimental evaluation, demonstrating the effectiveness of the proposed tool or technique.</strong> The evaluation part of a paper can vary, from measuring runtime performance to conducting user behavior studies in a controlled environment. Given the commonality of similar research ideas, reviewers pay close attention to the implementation and experimental analysis of these ideas when deciding on the acceptance or rejection of papers.</p></blockquote><h1 id="mindset">Mindset</h1><ul><li>Find a meaning for your research and build a reputation.</li></ul><blockquote><p>After two months of persistent effort, I began to achieve some modest victories. I managed to get Klee to function well enough to identify my first few bugs in the smallest device drivers. <strong>To ascertain whether these bugs were genuine (as opposed to false positives resulting from Klee's limitations), I sent emails outlining each potential bug to the Linux developers responsible for those drivers. Several driver creators verified that I had indeed discovered real bugs in their code. These email confirmations brought me great excitement, as they represented my initial glimpses of external validation.</strong></p></blockquote><blockquote><p>The skill of crafting concise and impactful professional emails has proven to be highly beneficial for my career.</p></blockquote><hr /><ul><li>Do not bootlick the "establishment" or the "mainstream." Be wise and brave, identify emergent trends, conquer virgin land, and blaze unheard-of new paths.</li></ul><blockquote><p><strong>The second and more significant reason why pursuing a postdoc didn't make sense for me was the nature of the research topics I was truly passionate about. These topics didn't align well with the likelihood of winning grant funding because they weren't widely accepted by the current academic establishment.</strong> Without grants, it would be impossible to fund students to work on these topics, and without motivated students to tackle the challenging manual work involved, it would be difficult to produce reputable publications. Furthermore, without a substantial number of publications each year, achieving tenure would be out of reach. Even if I did manage to secure tenure, I would still require new grants to support new students in implementing my ideas, perpetuating an ongoing funding cycle. Given my research interests, I wasn't emotionally prepared to engage in the uphill battles necessary to have my proposals taken seriously by grant funding agencies. Convincing peer reviewers to accept my papers had already been a challenge, and grant reviewers would likely be even less sympathetic, as they control the distribution of significant financial resources and would prefer to allocate funding to colleagues conducting more mainstream computer science research.</p><p><strong>When I began my faculty career in 2014, this was my greatest fear. Consequently, I deliberately transitioned to a new research area that had greater potential for securing funding.</strong></p><p><strong>Out of the 26 Ph.D. graduates in the Stanford Computer Science Department from my year, I considered myself relatively average from an academic perspective, as most of my papers were second-tier and not well-received by the academic community. My dissertation work straddled multiple computer science subfields, including Programming Languages, Human-Computer Interaction, and Operating Systems, which made it challenging to gain recognition from top experts in any one specific subfield.</strong></p><p><strong>Given that my dissertation topic was far from mainstream, any junior professor or scientist attempting to build their academic career around its concepts would face difficulties gaining the approval of grant funding agencies, crucial for launching new projects, and the support of senior colleagues, essential for publication and tenure. While I am more than willing to support anyone willing to take on this commendable challenge, I wasn't courageous enough to risk my own career on it.</strong> Instead, I have chosen to pursue an entirely different professional passion, which may become the subject of a future book.</p><p><strong>Interestingly, these ideas eventually became more fundable in the year after I graduated, thanks to the emergence of the Big Data and data science movements. However, by that time, I had already shifted my focus to other interests.</strong></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Note: This is a Paper Reading for Philip Guo&#39;s famous book &quot;The Ph.D. Grind: A Ph.D. Student Memoir.&quot;&lt;/p&gt;
&lt;hr /&gt;
&lt;h1 id=&quot;main-strategies&quot;</summary>
      
    
    
    
    <category term="Paper Reading" scheme="https://abbaswu.github.io/categories/Paper-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>Linear Regression, Ridge Regression, Lasso Regression, and Kernel Ridge Regression</title>
    <link href="https://abbaswu.github.io/2023/12/24/Linear-Regression-Ridge-Regression-Lasso-Regression-and-Kernel-Ridge-Regression/"/>
    <id>https://abbaswu.github.io/2023/12/24/Linear-Regression-Ridge-Regression-Lasso-Regression-and-Kernel-Ridge-Regression/</id>
    <published>2023-12-24T08:00:00.000Z</published>
    <updated>2023-12-25T05:01:47.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="linear-regression">Linear Regression</h1><figure><img src="https://miro.medium.com/v2/resize:fit:766/0*qq0yaecNRQiugnif.png" alt="" /><figcaption>Linear Regression</figcaption></figure><p>Linear regression is a fundamental statistical model used in statistics and supervised machine learning. It establishes a linear relationship between a scalar <em>response</em> and one or more <em>explanatory variables</em>. The simplicity and well-established properties of linear regression make it a cornerstone algorithm in machine learning.</p><p>Historically, linear regression was developed by Legendre (1805) and Gauss (1809) for astronomical predictions and later popularized in the social sciences by Quetelet.</p><p>Linear regression is widely used for two primary purposes:</p><ul><li>For predictive modeling, it fits a model to observed data sets, allowing for future predictions when new explanatory variables are available without their corresponding response values.</li><li>For analysis, it helps quantify the relationship between response and explanatory variables, assessing the strength of this relationship and identifying variables with no linear relationship or redundant information.</li></ul><p>In its most general case, a linear regression model can be written in matrix notation as</p><p><span class="math display">\[\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}\]</span></p><p>where</p><ul><li><span class="math inline">\(\mathbf{y} = {\begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix}}\)</span> is a vector of <span class="math inline">\(n\)</span> observed values of the response variable.</li><li><span class="math inline">\(\mathbf{X} = {\begin{bmatrix} \mathbf{x}_{1}^{\mathsf{T}} \\ \mathbf{x}_{2}^{\mathsf{T}} \\ \vdots \\ \mathbf{x}_{n}^{\mathsf{T}} \end{bmatrix}} = {\begin{bmatrix} 1 &amp; x_{1, 1} &amp; \cdots &amp; x_{1, p} \\ 1 &amp; x_{2, 1} &amp; \cdots &amp; x_{2, p}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{n, 1} &amp; \cdots &amp; x_{n, p} \end{bmatrix}}\)</span> is a matrix of <span class="math inline">\(n\)</span> observed <span class="math inline">\((p + 1)\)</span>-dimensional row-vectors of the explanatory variables.</li><li><span class="math inline">\(\boldsymbol{\beta} = {\begin{bmatrix} \beta_{0} \\ \beta_{1} \\ \beta_{2} \\ \vdots \\ \beta_{p} \end{bmatrix}}\)</span> is a <span class="math inline">\((p + 1)\)</span>-dimensional parameter vector, whose elements, multiplied with each dimension of the explanatory variables, are known as effects or regression coefficients.</li><li><span class="math inline">\(\boldsymbol{\varepsilon}={\begin{bmatrix}\varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon _{n} \end{bmatrix}}\)</span> is a vector of <span class="math inline">\(n\)</span> error terms. It captures all other factors that influence <span class="math inline">\(\mathbf{y}\)</span> other than <span class="math inline">\(\mathbf{X}\)</span>.</li></ul><p>Note that the first dimension of the explanatory variables is the constant 1. This is designed such that the corresponding first element of <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\beta_{0}\)</span>, would be the intercept after matrix multiplication. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero.</p><p>Fitting a linear model to a given data set usually requires estimating <span class="math inline">\(\boldsymbol{\beta}\)</span> such that <span class="math inline">\(\boldsymbol{\varepsilon} = \mathbf{y} - \mathbf{X} \boldsymbol{\beta}\)</span> is minimized.</p><p>For example, it is common to use the sum of squared errors (known as <strong>ordinary least squares</strong>) <span class="math inline">\(\|{\boldsymbol {\varepsilon }}\|_{2}^{2} = \|\mathbf{y} -\mathbf{X}{\boldsymbol{\beta}}\|_{2}^{2}\)</span> as a loss function for minimization. This minimization problem has a unique solution, <span class="math inline">\({\hat{\boldsymbol{\beta}}} = (\mathbf{X}^{\operatorname{T}} \mathbf{X})^{-1}\mathbf{X}^{\operatorname{T}} \mathbf{y}\)</span>.</p><p>References:</p><ul><li>https://en.wikipedia.org/wiki/Linear_regression</li><li>https://en.wikipedia.org/wiki/Ordinary_least_squares</li></ul><p>References:</p><ul><li>https://en.wikipedia.org/wiki/Linear_regression</li><li>https://en.wikipedia.org/wiki/Ordinary_least_squares</li></ul><h1 id="ridge-regression">Ridge Regression</h1><p>However, when linear regression models have some multicollinear (highly correlated) dimensions of the explanatory variables, which commonly occurs in models with high-dimensional explanatory variables, <span class="math inline">\(\mathbf{X}^{\operatorname{T}} \mathbf{X}\)</span> approaches a singular matrix and calculating <span class="math inline">\(\left(\mathbf{X}^{\operatorname{T}} \mathbf{X} \right)^{-1}\)</span> becomes numerically unstable (note how the magnitude of <code>np.linalg.inv(X.T @ X)</code> changes as the columns of <code>X</code> become more and more correlated below):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> x_21 <span class="hljs-keyword">in</span> [<span class="hljs-number">2.1</span>, <span class="hljs-number">2.01</span>, <span class="hljs-number">2.001</span>, <span class="hljs-number">2.0001</span>, <span class="hljs-number">2.00001</span>]:<br><span class="hljs-meta">... </span>    X = np.array([<br><span class="hljs-meta">... </span>        [<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],<br><span class="hljs-meta">... </span>        [<span class="hljs-number">1.</span>, x_21]<br><span class="hljs-meta">... </span>    ])<br><span class="hljs-meta">... </span>    <br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;X:&#x27;</span>)<br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(X)<br><span class="hljs-meta">... </span>    <br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;X.T @ X:&#x27;</span>)<br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(X.T @ X)<br><span class="hljs-meta">... </span>    <br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;np.linalg.inv(X.T @ X):&#x27;</span>)<br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(np.linalg.inv(X.T @ X))<br><span class="hljs-meta">... </span><br>X:<br>[[<span class="hljs-number">1.</span>  <span class="hljs-number">2.</span> ]<br> [<span class="hljs-number">1.</span>  <span class="hljs-number">2.1</span>]]<br>X.T @ X:<br>[[<span class="hljs-number">2.</span>   <span class="hljs-number">4.1</span> ]<br> [<span class="hljs-number">4.1</span>  <span class="hljs-number">8.41</span>]]<br>np.linalg.inv(X.T @ X):<br>[[ <span class="hljs-number">841.</span> -<span class="hljs-number">410.</span>]<br> [-<span class="hljs-number">410.</span>  <span class="hljs-number">200.</span>]]<br>X:<br>[[<span class="hljs-number">1.</span>   <span class="hljs-number">2.</span>  ]<br> [<span class="hljs-number">1.</span>   <span class="hljs-number">2.01</span>]]<br>X.T @ X:<br>[[<span class="hljs-number">2.</span>     <span class="hljs-number">4.01</span>  ]<br> [<span class="hljs-number">4.01</span>   <span class="hljs-number">8.0401</span>]]<br>np.linalg.inv(X.T @ X):<br>[[ <span class="hljs-number">80401.00000048</span> -<span class="hljs-number">40100.00000024</span>]<br> [-<span class="hljs-number">40100.00000024</span>  <span class="hljs-number">20000.00000012</span>]]<br>X:<br>[[<span class="hljs-number">1.</span>    <span class="hljs-number">2.</span>   ]<br> [<span class="hljs-number">1.</span>    <span class="hljs-number">2.001</span>]]<br>X.T @ X:<br>[[<span class="hljs-number">2.</span>       <span class="hljs-number">4.001</span>   ]<br> [<span class="hljs-number">4.001</span>    <span class="hljs-number">8.004001</span>]]<br>np.linalg.inv(X.T @ X):<br>[[ <span class="hljs-number">8004000.98507102</span> -<span class="hljs-number">4000999.99253738</span>]<br> [-<span class="hljs-number">4000999.99253738</span>  <span class="hljs-number">1999999.99626962</span>]]<br>X:<br>[[<span class="hljs-number">1.</span>     <span class="hljs-number">2.</span>    ]<br> [<span class="hljs-number">1.</span>     <span class="hljs-number">2.0001</span>]]<br>X.T @ X:<br>[[<span class="hljs-number">2.</span>         <span class="hljs-number">4.0001</span>    ]<br> [<span class="hljs-number">4.0001</span>     <span class="hljs-number">8.00040001</span>]]<br>np.linalg.inv(X.T @ X):<br>[[ <span class="hljs-number">8.00039556e+08</span> -<span class="hljs-number">4.00009777e+08</span>]<br> [-<span class="hljs-number">4.00009777e+08</span>  <span class="hljs-number">1.99999889e+08</span>]]<br>X:<br>[[<span class="hljs-number">1.</span>      <span class="hljs-number">2.</span>     ]<br> [<span class="hljs-number">1.</span>      <span class="hljs-number">2.00001</span>]]<br>X.T @ X:<br>[[<span class="hljs-number">2.</span>      <span class="hljs-number">4.00001</span>]<br> [<span class="hljs-number">4.00001</span> <span class="hljs-number">8.00004</span>]]<br>np.linalg.inv(X.T @ X):<br>[[ <span class="hljs-number">7.99973381e+10</span> -<span class="hljs-number">3.99985690e+10</span>]<br> [-<span class="hljs-number">3.99985690e+10</span>  <span class="hljs-number">1.99992345e+10</span>]]<br></code></pre></td></tr></table></figure><p>This problem can be alleviated by adding positive elements to the diagonals.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-meta">&gt;&gt;&gt; </span>X = np.array([<br><span class="hljs-meta">... </span>    [<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],<br><span class="hljs-meta">... </span>    [<span class="hljs-number">1.</span>, <span class="hljs-number">2.00001</span>]<br><span class="hljs-meta">... </span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> _<span class="hljs-keyword">lambda</span> <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.01</span>, <span class="hljs-number">0.001</span>, <span class="hljs-number">0.0001</span>]:<br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;np.linalg.inv(X.T @ X + <span class="hljs-subst">&#123;_<span class="hljs-keyword">lambda</span>&#125;</span> * np.eye(len(X))):&#x27;</span>)<br><span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(np.linalg.inv(X.T @ X + _<span class="hljs-keyword">lambda</span> * np.eye(<span class="hljs-built_in">len</span>(X))))<br><span class="hljs-meta">... </span><br>np.linalg.inv(X.T @ X + <span class="hljs-number">1</span> * np.eye(<span class="hljs-built_in">len</span>(X))):<br>[[ <span class="hljs-number">0.81818248</span> -<span class="hljs-number">0.36363595</span>]<br> [-<span class="hljs-number">0.36363595</span>  <span class="hljs-number">0.27272628</span>]]<br>np.linalg.inv(X.T @ X + <span class="hljs-number">0.1</span> * np.eye(<span class="hljs-built_in">len</span>(X))):<br>[[ <span class="hljs-number">8.01980982</span> -<span class="hljs-number">3.96039026</span>]<br> [-<span class="hljs-number">3.96039026</span>  <span class="hljs-number">2.07919969</span>]]<br>np.linalg.inv(X.T @ X + <span class="hljs-number">0.01</span> * np.eye(<span class="hljs-built_in">len</span>(X))):<br>[[ <span class="hljs-number">80.02005978</span> -<span class="hljs-number">39.95998014</span>]<br> [-<span class="hljs-number">39.95998014</span>  <span class="hljs-number">20.07983982</span>]]<br>np.linalg.inv(X.T @ X + <span class="hljs-number">0.001</span> * np.eye(<span class="hljs-built_in">len</span>(X))):<br>[[ <span class="hljs-number">800.02078984</span> -<span class="hljs-number">399.95940022</span>]<br> [-<span class="hljs-number">399.95940022</span>  <span class="hljs-number">200.07918976</span>]]<br>np.linalg.inv(X.T @ X + <span class="hljs-number">0.0001</span> * np.eye(<span class="hljs-built_in">len</span>(X))):<br>[[ <span class="hljs-number">8000.02719959</span> -<span class="hljs-number">3999.95360059</span>]<br> [-<span class="hljs-number">3999.95360059</span>  <span class="hljs-number">2000.07179896</span>]]<br></code></pre></td></tr></table></figure><p>By replacing <span class="math inline">\((\mathbf{X}^{\operatorname{T}} \mathbf{X})^{-1}\)</span> with <span class="math inline">\((\mathbf{X} ^{\mathsf{T}} \mathbf{X} +\lambda \mathbf{I} )^{-1}\)</span> in <span class="math inline">\({\hat{\boldsymbol{\beta}}} = (\mathbf{X}^{\operatorname{T}} \mathbf{X})^{-1}\mathbf{X}^{\operatorname{T}} \mathbf{y}\)</span>, we derive the solution to <strong>ridge regression</strong>, <span class="math inline">\({\hat {\beta }}_{R}=(\mathbf{X} ^{\mathsf{T}} \mathbf{X} +\lambda \mathbf{I} )^{-1}\mathbf{X} ^{\mathsf{T}}\mathbf{y}\)</span>.</p><p>Ridge regression (linear regression with L2 regularization), is linear regression using <span class="math inline">\({\mathcal{L}}(\boldsymbol{\beta}, \lambda) = \|\mathbf{y} -\mathbf{X}{\boldsymbol{\beta}}\|_{2}^{2} + \lambda (\|{\boldsymbol{\beta}}\|_{2}^{2} - C)\)</span> as the loss function to minimize.</p><p>This is a Lagrangian function expressing the original ordinary least squares loss function <span class="math inline">\(\|{\boldsymbol {\varepsilon }}\|_{2}^{2} = \|\mathbf{y} -\mathbf{X}{\boldsymbol{\beta}}\|_{2}^{2}\)</span> subject to the constraint <span class="math inline">\(\|{\boldsymbol{\beta}}\|_{2}^{2} \le C\)</span> for some <span class="math inline">\(C &gt; 0\)</span>.</p><p>Note that <strong>by calculating <span class="math inline">\({\hat {\beta }}_{R}=(\mathbf{X} ^{\mathsf{T}} \mathbf{X} +\lambda \mathbf{I} )^{-1}\mathbf{X} ^{\mathsf{T}}\mathbf{y}\)</span> with a given <span class="math inline">\(\lambda\)</span> value, instead of simultaneously solving for <span class="math inline">\(\boldsymbol{\beta}\)</span> and lambda through <span class="math inline">\(\nabla{\mathcal{L}}(\boldsymbol{\beta}, \lambda) = 0\)</span> (the usual practice of using Lagrangian functions for constrained optimization), we do not necessary obtain a <span class="math inline">\(\boldsymbol{\beta}\)</span> that satisfies for a given value of C. However, increasing the given <span class="math inline">\(\lambda\)</span> value monotonically decreases the value of <span class="math inline">\(\|{\boldsymbol{\beta}}\|_{2}^{2}\)</span>, thus making the constraint <span class="math inline">\(\|{\boldsymbol{\beta}}\|_{2}^{2} \le C\)</span> be satisfied for smaller values of <span class="math inline">\(C\)</span></strong>.</p><figure><img src="https://i.stack.imgur.com/YNwf5.png" alt="" /><figcaption>Image from https://math.stackexchange.com/questions/1723201/solution-for-arg-min-xt-x-1-xt-a-x-ct-x-quadratic</figcaption></figure><p>We can also demonstrate this with an example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-meta">&gt;&gt;&gt; </span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">beta_squared</span>(<span class="hljs-params">l, X, y</span>):<br><span class="hljs-meta">... </span>    beta = np.linalg.inv(X.T @ X + l * np.eye(<span class="hljs-built_in">len</span>(X))) @ X.T @ y<br><span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> beta.T @ beta<br><span class="hljs-meta">... </span><br><span class="hljs-meta">&gt;&gt;&gt; </span>np.random.seed(<span class="hljs-number">0</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><br><span class="hljs-meta">&gt;&gt;&gt; </span>X = np.random.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><br><span class="hljs-meta">&gt;&gt;&gt; </span>y = np.random.rand(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><br><span class="hljs-meta">&gt;&gt;&gt; </span>beta_squared(<span class="hljs-number">0.01</span>, X, y)<br>array([[<span class="hljs-number">1.33717503</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span><br><span class="hljs-meta">&gt;&gt;&gt; </span>beta_squared(<span class="hljs-number">0.1</span>, X, y)<br>array([[<span class="hljs-number">0.37141735</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span><br><span class="hljs-meta">&gt;&gt;&gt; </span>beta_squared(<span class="hljs-number">1.0</span>, X, y)<br>array([[<span class="hljs-number">0.13504294</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span><br><span class="hljs-meta">&gt;&gt;&gt; </span>beta_squared(<span class="hljs-number">10.0</span>, X, y)<br>array([[<span class="hljs-number">0.0062103</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span><br><span class="hljs-meta">&gt;&gt;&gt; </span>beta_squared(<span class="hljs-number">100.0</span>, X, y)<br>array([[<span class="hljs-number">7.92298438e-05</span>]])<br></code></pre></td></tr></table></figure><p>Furthermore, as <span class="math inline">\(\|{\boldsymbol{\beta}}\|_{2}^{2} = \beta_{0}^{2} + \beta_{1}^{2} + \cdots + \beta_{p}^{2}\)</span>, increasing the given <span class="math inline">\(\lambda\)</span> value helps to constrain the magnitude of the effects or regression coefficients corresponding to dimensions which are redundant in high-dimensional explanatory variables.</p><p>This is visualized in the right diagram, where the constraint <span class="math inline">\(\|{\boldsymbol{\beta}}\|_{2}^{2} \le C\)</span> in the Lagrangian function (the green circle) tangentially touches a contour of the original ordinary least squares loss function <span class="math inline">\(\|{\boldsymbol {\varepsilon }}\|_{2}^{2} = \|\mathbf{y} -\mathbf{X}{\boldsymbol{\beta}}\|_{2}^{2}\)</span> at a point where one of the effects (or regression coefficients) is close to 0.</p><figure><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*Jd03Hyt2bpEv1r7UijLlpg.png" alt="" /><figcaption>Modified from the plot used in "The Elements of Statistical Learning" by Saptashwa Bhattacharyya</figcaption></figure><p>To further strengthen this effect and completely "zero out" certain effects or regression coefficients, <strong>lasso regression (linear regression with L1 regularization)</strong> can be used in lieu of ridge recursion.</p><p>In this case, the original ordinary least squares loss function <span class="math inline">\(\|{\boldsymbol {\varepsilon }}\|_{2}^{2} = \|\mathbf{y} -\mathbf{X}{\boldsymbol{\beta}}\|_{2}^{2}\)</span> subject to the constraint <span class="math inline">\(\|{\boldsymbol{\beta}}\|_{1} = |\beta_{0}| + |\beta_{1}| + \cdots + |\beta_{p}| \le C\)</span> for some <span class="math inline">\(C &gt; 0\)</span>, as depicted in the left diagram, where the constraint <span class="math inline">\(\|{\boldsymbol{\beta}}\|_{1} \le C\)</span> in the Lagrangian function (the cyan square) tangentially touches a contour of the original ordinary least squares loss function <span class="math inline">\(\|{\boldsymbol {\varepsilon }}\|_{2}^{2} = \|\mathbf{y} -\mathbf{X}{\boldsymbol{\beta}}\|_{2}^{2}\)</span> at a point where one of the effects (or regression coefficients) is 0.</p><p>However, we cannot derive an analytical solution for <span class="math inline">\(\boldsymbol{\beta}\)</span> given the Lagrangian function for lasso regression (a.k.a. the loss function to minimize), <span class="math inline">\({\mathcal{L}}(\boldsymbol{\beta}, \lambda) = \|\mathbf{y} -\mathbf{X}{\boldsymbol{\beta}}\|_{2}^{2} + \lambda (\|{\boldsymbol{\beta}}\|_{1} - C)\)</span>. We can only iteratively solve for <span class="math inline">\(\boldsymbol{\beta}\)</span> in this case.</p><p>References:</p><ul><li>https://en.wikipedia.org/wiki/Lagrange_multiplier</li><li>https://stats.stackexchange.com/questions/401212/showing-the-equivalence-between-the-l-2-norm-regularized-regression-and</li><li>https://math.stackexchange.com/questions/1723201/solution-for-arg-min-xt-x-1-xt-a-x-ct-x-quadratic</li><li>https://arxiv.org/pdf/1509.09169.pdf</li><li>https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b</li><li>https://en.wikipedia.org/wiki/Ridge_regression</li><li>https://online.stat.psu.edu/stat857/node/155/</li><li>https://allmodelsarewrong.github.io/ridge.html</li></ul><h1 id="kernel-ridge-regression">Kernel Ridge Regression</h1><p>Given the solution to ridge recursion above, <span class="math inline">\({\hat {\beta }}_{R}=(\mathbf{X} ^{\mathsf{T}} \mathbf{X} +\lambda \mathbf{I} )^{-1}\mathbf{X} ^{\mathsf{T}}\mathbf{y}\)</span>, we can predict the value of the response variable <span class="math inline">\(y_{n + 1}(\mathbf{x}_{n + 1})\)</span>, given an out-of-dataset vector of explanatory variables <span class="math inline">\(\mathbf{x}_{n + 1} = {\begin{bmatrix} 1 \\ x_{n + 1, 1} \\ \vdots \\ x_{n + 1, p} \end{bmatrix}}\)</span>:</p><p><span class="math display">\[y_{n + 1}(\mathbf{x}_{n + 1}) = \mathbf{x}_{n + 1}^{\mathsf{T}} {\hat {\beta }}_{R} = \mathbf{x}_{n + 1}^{\mathsf{T}} (\mathbf{X} ^{\mathsf{T}} \mathbf{X} +\lambda \mathbf{I} )^{-1}\mathbf{X} ^{\mathsf{T}}\mathbf{y}\]</span></p><p>We can make some changes to <span class="math inline">\(\mathbf{x}_{n + 1}^{\mathsf{T}} (\mathbf{X} ^{\mathsf{T}} \mathbf{X} +\lambda \mathbf{I} )^{-1}\mathbf{X} ^{\mathsf{T}}\mathbf{y}\)</span>.</p><h2 id="push-through-identity">Push-Through Identity</h2><p>Given two matrices <span class="math inline">\(\mathbf{P}, \mathbf{Q}\)</span>, based on <span class="math inline">\(\mathbf{P} (I + \mathbf{Q} \mathbf{P}) = (I + \mathbf{P} \mathbf{Q}) \mathbf{P}\)</span>, we can derive <span class="math inline">\({(I + \mathbf{P} \mathbf{Q})}^{-1} \mathbf{P} = \mathbf{P} {(I + \mathbf{Q} \mathbf{P})}^{-1}\)</span>. This is known as the push-through identity, one of the matrix inversion identities used to derive the Woodbury matrix identity, which allows cheap computation of inverses and solutions to linear equations.</p><p>References:</p><ul><li>http://www0.cs.ucl.ac.uk/staff/g.ridgway/mil/mil.pdf</li><li>https://en.wikipedia.org/wiki/Woodbury_bmatrix_identity</li></ul><p>Based on the push through identity, <span class="math inline">\(\mathbf{x}_{n + 1}^{\mathsf{T}} (\mathbf{X} ^{\mathsf{T}} \mathbf{X} + \lambda \mathbf{I} )^{-1} \mathbf{X}^{\mathsf{T}} \mathbf{y} = \mathbf{x}_{n + 1}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} {(\mathbf{X} \mathbf{X}^{\mathsf{T}} + \lambda \mathbf{I})}^{-1} \mathbf{y}\)</span>.</p><p>As <span class="math inline">\(\mathbf{X} = {\begin{bmatrix} \mathbf{x}_{1}^{\mathsf{T}} \\ \vdots \\ \mathbf{x}_{n}^{\mathsf{T}} \end{bmatrix}}\)</span>, <span class="math inline">\(\mathbf{X}^{\mathsf{T}} = {\begin{bmatrix} \mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{n} \end{bmatrix}}\)</span>, we have:</p><ul><li><span class="math inline">\(\mathbf{x}_{n + 1}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} = {\begin{bmatrix} \mathbf{x}_{n + 1}^{\mathsf{T}} \mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{n + 1}^{\mathsf{T}} \mathbf{x}_{n} \end{bmatrix}}\)</span></li><li><span class="math inline">\(\mathbf{X} \mathbf{X}^{\mathsf{T}} = {\begin{bmatrix} \mathbf{x}_{1}^{\mathsf{T}} \mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{1}^{\mathsf{T}} \mathbf{x}_{n} \\ \vdots &amp; \ddots &amp; \vdots \\ \mathbf{x}_{n}^{\mathsf{T}} \mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{n}^{\mathsf{T}} \mathbf{x}_{n} \end{bmatrix}}\)</span></li></ul><p>Thus:</p><p><span class="math display">\[y_{n + 1}(\mathbf{x}_{n + 1}) = {\begin{bmatrix} \mathbf{x}_{n + 1}^{\mathsf{T}} \mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{n + 1}^{\mathsf{T}} \mathbf{x}_{n} \end{bmatrix}} {({\begin{bmatrix} \mathbf{x}_{1}^{\mathsf{T}} \mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{1}^{\mathsf{T}} \mathbf{x}_{n} \\ \vdots &amp; \ddots &amp; \vdots \\ \mathbf{x}_{n}^{\mathsf{T}} \mathbf{x}_{1} &amp; \cdots &amp; \mathbf{x}_{n}^{\mathsf{T}} \mathbf{x}_{n} \end{bmatrix}} + \lambda \mathbf{I})}^{-1} \mathbf{y}\]</span></p><p>This means that we can calculate <span class="math inline">\(y_{n + 1}(\mathbf{x}_{n + 1})\)</span> directly from the dot products among <span class="math inline">\(\mathbf{x}_{1}, \cdots, \mathbf{x}_{n}\)</span> and the dot products between <span class="math inline">\(\mathbf{x}_{n + 1}\)</span> and <span class="math inline">\(\mathbf{x}_{1}, \cdots, \mathbf{x}_{n}\)</span>, <strong>without having to explicitly know the values of <span class="math inline">\(\mathbf{x}_{1}, \cdots, \mathbf{x}_{n}\)</span> and <span class="math inline">\(\mathbf{x}_{n + 1}\)</span></strong>.</p><p>Moreover, the dot product between two vectors of explanatory variables here can be generalized to <strong>any symmetric similarity function between two vectors of explanatory variables known as kernel functions</strong>.</p><p>Using <span class="math inline">\(k(\mathbf{x}_{i}, \mathbf{x}_{j})\)</span> to denote the similarity between <span class="math inline">\(\mathbf{x}_{i}, \mathbf{x}_{j}\)</span> under the kernel function <span class="math inline">\(k\)</span>, let:</p><ul><li><span class="math inline">\(\mathbf{K} = \mathbf{X} \mathbf{X}^{\mathsf{T}} = {\begin{bmatrix} k(\mathbf{x}_{1}, \mathbf{x}_{1}) &amp; \cdots &amp; k(\mathbf{x}_{1}, \mathbf{x}_{n}) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_{n}, \mathbf{x}_{1}) &amp; \cdots &amp; k(\mathbf{x}_{n}, \mathbf{x}_{n}) \end{bmatrix}}\)</span></li><li><span class="math inline">\(\mathbf{k}(\mathbf{x}_{n + 1}) = \mathbf{x}_{n + 1}^{\mathsf{T}} \mathbf{X}^{\mathsf{T}} = {\begin{bmatrix} k(\mathbf{x}_{n + 1}, \mathbf{x}_{1}) &amp; \cdots &amp; k(\mathbf{x}_{n + 1}, \mathbf{x}_{n}) \end{bmatrix}}\)</span></li></ul><p>We have:</p><p><span class="math display">\[y_{n + 1}(\mathbf{x}_{n + 1}) = \mathbf{k}(\mathbf{x}_{n + 1}) {(\mathbf{K} + \lambda \mathbf{I})}^{-1} \mathbf{y}\]</span></p><p>There are two benefits of kernel ridge regression.</p><ul><li>It allows implicitly performing nonlinear transformations on the vector representations of explanatory variables within similarity calculation, allowing nonlinearity to be introduced. A prominent example is the widespread <a href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">radial basis function kernel</a>, first used in mining engineering ("kriging").</li><li>It allows regressions on explanatory variables that do not have explicit vector representations but have similarity functions. There are "string kernels," "image kernels," "graph kernels," and so on.</li></ul><figure><img src="https://raw.githubusercontent.com/abbaswu/abbaswu.github.io-images/main/Motivation-Finding-Gold.png" alt="" /><figcaption>Kriging (from UBC CPSC 340 slides)</figcaption></figure><figure><img src="https://desktop.arcgis.com/es/arcmap/latest/extensions/geostatistical-analyst/GUID-49DA5B53-6E2F-4A29-BA01-2BF4F0259594-web.png" alt="" /><figcaption>Kriging</figcaption></figure><figure><img src="https://raw.githubusercontent.com/abbaswu/abbaswu.github.io-images/main/Kernel-Trick-for-Non-Vector-Data-1.png" alt="" /><figcaption>Kernel Trick for Non-Vector Data (from UBC CPSC 340 slides)</figcaption></figure><figure><img src="https://raw.githubusercontent.com/abbaswu/abbaswu.github.io-images/main/Kernel-Trick-for-Non-Vector-Data-2.png" alt="" /><figcaption>Kernel Trick for Non-Vector Data (from UBC CPSC 340 slides)</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h1&gt;
&lt;figure&gt;
&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:766/0*qq0yaecNRQiugnif.png&quot; alt=&quot;</summary>
      
    
    
    
    <category term="Mathematics" scheme="https://abbaswu.github.io/categories/Mathematics/"/>
    
    
  </entry>
  
  <entry>
    <title>Sarah Chasins&#39; Works on PL and HCI</title>
    <link href="https://abbaswu.github.io/2023/11/05/Sarah-Chasins-Works-on-PL-and-HCI/"/>
    <id>https://abbaswu.github.io/2023/11/05/Sarah-Chasins-Works-on-PL-and-HCI/</id>
    <published>2023-11-05T07:00:00.000Z</published>
    <updated>2023-11-08T20:50:51.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="co-designing-for-transparency-lessons-from-building-a-document-organization-tool-in-the-criminal-justice-domain"><a href="https://dl.acm.org/doi/10.1145/3593013.3594093">Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain</a></h1><p>Investigative journalists and public defenders are crucial in scrutinizing and litigating significant matters concerning police violence and misconduct. However, they often need help navigating through vast, unordered heaps of data, which adds strain to their resource-constrained teams.</p><p>In partnership with U.S. public defenders and investigative journalists, we developed an AI-enhanced tool through a joint design effort to aid in working with such data. This process offered us valuable insights into the requirements of resource-constrained teams dealing with large data sets, including how some experts became self-taught programmers to streamline their workflows.</p><p>We pinpointed three primary data needs throughout our collaborative design journey and established five design objectives.</p><h2 id="three-primary-data-needs">Three Primary Data Needs</h2><p>Data Cleaning, particularly the process of de-duplication. That involves identifying identical (images of pages are pixel-for-pixel copies of each other) or nearly identical data (images are not pixel-for-pixel identical but capture the same physical document) within a dataset.</p><p>Data Extraction. The professionals also struggled in extracting relevant information such as names, dates, locations, and case numbers from case files due to their disparate formats and layouts, necessitating extensive, hands-on work.</p><p>Data Organization. There was a need to systematically organize PDF documents by specific cases, complicated by the fact that cases may be spread across numerous documents and folders, or conversely, several cases might be compiled into one extensive PDF.</p><h2 id="five-fundamental-design-principles">Five Fundamental Design Principles</h2><p>Human Control and Intervention. The design must prioritize aiding users over complete automation of the process.</p><p>Non-Interference with Existing Practices. The design should integrate seamlessly with existing workflows and practices.</p><p>Adaptability to Data Diversity.</p><p>High-level Abstractions. General-purpose languages like Python or R demand extensive technical expertise. Pre-built software, on the other hand, offers limited flexibility.</p><p>Cost-Sensitive Solutions.</p><h2 id="results">Results</h2><p>Participants in our sessions became adept in all three programming paradigms (visual, PBE, and text-based interfaces).</p><ul><li>This contradicts the common misconception that non-technical experts need formal coding training to handle text-based programming; if the tools are appropriately supportive, they can.</li><li>Rather than creating new code, participants preferred to modify what was already there. Particularly with text-based coding, almost all chose to adapt sample code instead of originating their own, aligning with previous research on the blank-page syndrome.</li></ul><hr /><h1 id="a-need-finding-study-with-users-of-geospatial-data"><a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581370">A Need-Finding Study with Users of Geospatial Data</a></h1><p>Current geospatial analysis and visualization tools present significant learning curves and usability challenges.</p><ul><li>Finding and transforming geospatial data to specific spatiotemporal constraints.</li><li>Grasping the behavior of geospatial operators.</li><li>Tracking the provenance of geospatial data, including cross-system provenance.</li><li>Exploring the cartographic design space.</li></ul><h2 id="grasping-the-behavior-of-geospatial-operators">Grasping the behavior of geospatial operators</h2><p>Users had to run operators and manually check outputs to understand operator semantics.</p><p>Live programming, which offers users immediate visual feedback on program behavior using concrete inputs, could align with users' existing debugging patterns of using small collections of geographic features or pixels as test cases to infer operator behavior.</p><h2 id="tracking-the-provenance-of-geospatial-data-including-cross-system-provenance">Tracking the provenance of geospatial data, including cross-system provenance</h2><p>The GIS tools used by participants did not track the steps leading to final outputs, complicating the replication of previous analyses.</p><p>Modifying maps or adapting them to new datasets often meant laboriously reverse engineering the initial analysis steps.</p><p>Creating repeatable and communicable geospatial workflows was a struggle for GIS users. Limitations in current history features made it difficult to recover information on the current analysis state or revisit past analysis decisions.</p><p>The problem of tracking provenance across different systems was also prominent.</p><p>Users often kept informal records of the steps taken in data acquisition, cleaning, analysis, and visualization, which spanned several applications. For instance, one user used macOS Notes to detail a process involving data transfer between Sentinel Hub, QGIS, Illustrator, and Photoshop, documenting everything from selecting a Sentinel-2 image to reassembling raster segments in Illustrator. This kind of multi-tool orchestration was typical among our subjects, yet none had automated systems to log data lineage across these platforms.</p><h2 id="exploring-the-cartographic-design-space">Exploring the cartographic design space</h2><p>Many participants used direct manipulation tools for geospatial data visualization, which discarded all geographical metadata, posing challenges to revising the analysis after starting the visualization. This uncovers a potential for development in tools that (1) unify geospatial analysis with cartographic design and (2) preserve the geospatial data aspects of visual elements while supporting direct manipulation.</p><p>Existing research suggests that combining scripting with direct manipulation for visually oriented tasks is feasible. The Sketch-n-sketch application is a testament to the successful merger of these methods for SVG graphics.</p><p>Such a combined approach could also remedy the fundamental issue participants faced when using direct manipulation tools for cartography: the need to recreate map designs in code after finishing a design.</p><hr /><h1 id="how-statically-typed-functional-programmers-write-code"><a href="https://dl.acm.org/doi/10.1145/3485532">How Statically-Typed Functional Programmers Write Code</a></h1><p>A deeper comprehension of the coding methods of statically-typed functional programmers could lead to the creation of more practical tools, more user-friendly programming languages, and better gateways into programming communities.</p><p>These programmers utilize their compilers for more than just producing an executable; they also use compilers as corrective and directive aids.</p><ul><li>Compilers as corrective tools. Compiler error messages were useful not just to fix their programs but also to correct their mental models of the problem domain.</li><li>Compilers as directive tools. Many developers treat compiler errors as to-do lists, guiding their subsequent coding actions. A typical process includes beginning a program change with a minor alteration and compiling to receive error-driven sub-tasks - essentially turning error messages into a step-by-step guide for coding.</li></ul><p>It's not uncommon for programmers to compile their code with the expectation of errors, using the compiler to validate the direction of their development.</p><p>Statically-typed functional programmers often seek feedback from automated tools even when their code isn't yet operational, suggesting that such tools should strive to extract as much information as possible from non-compilable code.</p><p>When comparing pattern matching with combinators, statically-typed functional programmers report less cognitive and time pressure with the former. This could be due to pattern matching's explicit textual representation of tasks, explicit handling of recursion, or consistent interface across various data structures. Nonetheless, some programmers prefer to rewrite their code using combinators eventually. Ideally, a tool would assist in this process, starting with a data type and guiding the programmer through case completion, subsequently offering a series of combinators as a refined alternative.</p><p>It is beneficial to recognize which language constructs allow for low-workload or opportunistic construction and how these constructs are valued within the programming community.</p><p>There's a demand for tools that minimize the difficulty of altering types during development. Furthermore, these tools should facilitate the natural cyclic changes of a developer's focus between modifying types and modifying expressions, possibly by employing program repair techniques to predict how changes in one will affect the other.</p><p>Program sketches provide a wealth of information about undefined functions, like inferred types and potential uses. Since statically-typed functional programmers regularly employ this method of drafting and refining code, there's a clear opportunity for tools that could enhance or even automate parts of this practice.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;co-designing-for-transparency-lessons-from-building-a-document-organization-tool-in-the-criminal-justice-domain&quot;&gt;&lt;a href=&quot;https://dl</summary>
      
    
    
    
    <category term="Paper Reading" scheme="https://abbaswu.github.io/categories/Paper-Reading/"/>
    
    <category term="Research Programming" scheme="https://abbaswu.github.io/categories/Paper-Reading/Research-Programming/"/>
    
    
  </entry>
  
  <entry>
    <title>Giving Talks on Research: Whats and Hows</title>
    <link href="https://abbaswu.github.io/2023/10/27/Giving-Talks-on-Research-Whats-and-Hows/"/>
    <id>https://abbaswu.github.io/2023/10/27/Giving-Talks-on-Research-Whats-and-Hows/</id>
    <published>2023-10-27T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.165Z</updated>
    
    <content type="html"><![CDATA[<hr /><p>As researchers, we often need to give talks on our research, either as part of presenting our results, or as a means of pitching our aspirations for the future. In this talk, inspired by ideas presented by Derek Dreyer at CMMRS 2023, Simon Peyton Jones at ICFP PLSE 2023, Finn Hacket and Robert Xiao at "Workshop on Presentation Skills", and my personal experiences, I will present the high-level "whats" an ideal talk should be like, and the low-level "hows" along the path of actually preparing and giving such talks.</p><p>Source code and compiled PDF of the presentation for "Giving Talks on Research: Whats and Hows" presented at the Systopia Reading Group Oct 27, 2023 is available <a href="https://github.com/abbaswu/Giving-Talks-on-Research-Whats-and-Hows/tree/main">in this GitHub repository</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;hr /&gt;
&lt;p&gt;As researchers, we often need to give talks on our research, either as part of presenting our results, or as a means of pitching o</summary>
      
    
    
    
    <category term="Talks" scheme="https://abbaswu.github.io/categories/Talks/"/>
    
    
  </entry>
  
  <entry>
    <title>Conversation with Prof. Robert Xiao</title>
    <link href="https://abbaswu.github.io/2023/10/23/Conversation-with-Prof-Robert-Xiao/"/>
    <id>https://abbaswu.github.io/2023/10/23/Conversation-with-Prof-Robert-Xiao/</id>
    <published>2023-10-23T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>The following is a polished version of a conversation with <a href="https://www.robertxiao.ca/">Prof. Robert Xiao</a> on the confluence of Programming Languages, Software Engineering, and Human-Computer Interaction (HCI) for research programming. The main points mentioned by Prof. Robert Xiao are as follows.</p><ul><li>Determining which aspects of research programming are amenable to systematization is a challenge.</li><li>Advancements in code tracing, like PyTorch 2's bytecode analysis, which, while not addressing verifiability directly, allows for in-depth program behavior analysis.</li><li>Understanding the influence of input data on outputs is central to the challenge of explainable AI (XAI). However, pinpointing specific model features or layers leading to an output is perhaps more accessible and useful for debugging.</li><li>The concept of 'radioactively' tagging data to trace its influence through a model is an intriguing one, akin to tracking the uptake of a tagged substance in a biological system.</li><li>There are many scenarios with extensive object interactions in programming. Game programming provides a more structured context to study these complexities, with ample open-source resources for research. The difficulty in game programming arises from the myriad interactions between diverse object systems - like physics, collision, and interaction logic. Coding these interactions is a lot of work. It would be enlightening to study how game developers handle this complexity and whether there are ways to simplify it. Game studios, being the behemoths they are, would undoubtedly embrace methods to alleviate the strenuous nature of their programming efforts.</li></ul><h1 id="polished-transcript">Polished Transcript</h1><p>Robert Xiao: [00:00] Could you share the focus of your research and how it's pertinent to this project? Also, what do you aim to achieve with it? There seem to be several components you've touched upon, such as visualization, pipeline development, and programming processes. These represent different approaches you could potentially adopt or consider integrating into a comprehensive pipeline. I believe the ultimate goal here is to aid research programmers in accelerating system development while minimizing errors, correct?</p><p>Jifeng Wu: [00:43] Yes, precisely.</p><p>Robert Xiao: [00:45] Let's delve into your research focus. How does your current work align with this?</p><p>Jifeng Wu: [00:52] My ongoing research isn't directly related, as this is a path I'm contemplating for a future Ph.D. project, which I still need to commit to. I'm currently working on my master's thesis titled 'Type Inference for Python.' It aims to infer types in Python code, which often lacks annotations. This lack can lead to IDEs providing less accurate suggestions. With type information, predictions become more reliable, enhancing the coding and code interaction experience.</p><p>Robert Xiao: [01:54] So, to clarify, your project is about developing a system for automatic type inference that assists IDEs, not just creating a type annotation database. Existing tools do offer preliminary type extraction, but I'm interested in the novel contribution your research makes.</p><p>Jifeng Wu: [02:31] Exactly. I'm not just extracting types; I'm inferring them in unannotated code bases to enhance IDE functionality.</p><p>Robert Xiao: [02:45] Understood. There are incremental typing tools available, but we can explore that later. For now, it's great that you're well-versed in Python, especially since it's prevalent in LLM research. An interesting aspect of your direction could be mitigating bugs, which often derail projects. Implementing automated checks could be invaluable. However, the challenge lies in determining which aspects of research programming are amenable to systematization.</p><p>Jifeng Wu: [06:17] My vision is to support researchers engaged in data analysis or custom model design in an environment akin to Jupyter notebooks. And touching on debugging, I see a potential to harness functional programming due to its purity and ease of debugging.</p><p>Robert Xiao: [07:22] The question, however, is the application of functional programming to research code, which often depends on pre-existing libraries. While functional design has its merits, the practicality of integrating it into the current ecosystem is worth discussing.</p><p>Jifeng Wu: [07:55] I concede the point; many codebases are indeed messy. I'm contemplating a clean slate design, potentially developing a new language or library to demonstrate the concept.</p><p>Robert Xiao: [08:21] It's noteworthy that there have been advancements in code tracing, like PyTorch 2's bytecode analysis, which, while not addressing verifiability directly, allows for in-depth program behavior analysis.</p><p>Jifeng Wu: [11:09] Certainly. There are facets of current notebook technologies that pique my interest, primarily due to their inadequate support, with debugging being a prime example. Debugging encompasses two key aspects: the logic of the program, as previously mentioned, and data provenance. Sometimes, despite the sound logic, I need to delve into the origins of an unexpected output data point by tracing the implicit calculations that led to it.</p><p>[11:58] This necessity for data provenance tracking is something I find critically important in my daily research, and I understand it's known as the data provenance problem.</p><p>Robert Xiao: [12:09] Indeed, if you've ever discussed this with Margo, you're likely well-versed in the topic, given her research focuses precisely on provenance. Many of her colleagues are exploring this area, which is complex, particularly in the context of outputs from extensive machine learning models. While it would be beneficial to trace data points back to their origins, integrating such a mechanism into a model is a formidable challenge.</p><p>[12:55] As a developer, I'm keen on understanding the influence of input data on outputs, which is central to the challenge of explainable AI (XAI). Resolving this would mark a significant milestone. However, pinpointing specific model features or layers leading to an output is perhaps more accessible and useful for debugging.</p><p>[13:53] For instance, identifying a misconfigured layer responsible for input-related issues would be invaluable. Although considering the interconnected nature of model layers, this remains a complex task.</p><p>[14:57] The concept of 'radioactively' tagging data to trace its influence through a model is an intriguing one, akin to tracking the uptake of a tagged substance in a biological system. Yet, translating this to a machine learning environment presents a unique set of challenges.</p><p>[16:57] While I'm not deeply familiar with the latest advancements in this field, it's clear that XAI could significantly benefit HCI applications. The goal is to incrementally address these challenges by developing models that acknowledge tagged inputs throughout the data processing pipeline.</p><p>[17:58] These are some thoughts on the subject. I'd like to know which aspects you find most relevant or valuable for your future endeavors.</p><p>Jifeng Wu: [18:15] The examples and pointers you've provided are insightful. As someone with a software engineering background, I believe that adapting certain constructs, like functional programming and traditional program analysis, could offer potential solutions. These are directions I'm considering for my Ph.D. research.</p><p>Robert Xiao: [18:58] Exploring program tracing for optimization could prove fruitful, given the untapped potential in that area. The philosophy I subscribe to favors solutions that minimize user effort, exemplified by the tracing compiler feature in PyTorch 2.0. Unlike TensorFlow model, which requires upfront operation declarations, PyTorch's immediate mode operation presents a more straightforward approach for users, facilitating a clearer understanding of variable flow during execution.</p><p>Jifeng Wu: [ 24:12 ] Incidentally, as a researcher in HCI, have you ever engaged in work that marries aspects of software engineering, specifically functional programming, with HCI?</p><p>Robert Xiao: [ 24:28 ] My experience with functional programming in a research capacity is virtually nonexistent. My computer science education covered the basics of functional programming - I dabbled in Scheme and Racket - but in terms of research, functional programming hasn't been part of my repertoire. Our work typically involves Python, C#, and various visual programming tools, none of which adhere to a functional programming paradigm.</p><p>[ 24:56 ] Game programming, which encompasses many of our VR/AR programming, is about as far removed from functional programming as possible. It's heavily state-driven, with an ever-changing state environment. A functional approach could be applied to VR/AR development. It could offer advantages over current methods. I'm aware of actor model programming being used for VR/AR experiences, though it's not functional programming per se, and I have yet to adopt it in my work personally. Conversely, when it comes to software engineering, we do integrate its methodologies into our software creation process. This includes best practices like code structuring for reusability, modularization, refactoring, and especially source control, which I find is grossly underutilized in research programming, among other things.</p><p>Jifeng Wu: [ 26:33 ] Understood, yes.</p><p>My vision, when I speak of integrating functional programming, isn't confined to conventional languages like Scheme or Racket that you mentioned earlier.</p><p>My thoughts were more aligned with programming paradigms like the actor model you described, as well as visual programming. I'm interested in approaches that are more formalized, easier to reason about, and offer a clearer path to verifying properties and facilitating debugging.</p><p>Robert Xiao: [ 27:18 ] With general-purpose imperative coding, the debugging process is, frankly, a nightmare. Although my personal experience in developing large-scale games is limited, our research typically involves creating specific VR/AR experiences. These are smaller in scale, utilizing existing libraries to build a finite number of interactive elements within a controlled environment. This contrasts with the vast complexity of full-scale game development, where the difficulty arises from the myriad interactions between diverse object systems - like physics, collision, and interaction logic.</p><p>[ 28:55 ] Take collision logic as an instance; the multitude of possible outcomes from a single collision event can be incredibly intricate to code. If we consider bullet dynamics in games, the behavior of these projectiles upon impact with walls, enemies, or objects varies dramatically, leading to a cascade of different effects. Coding these interactions is a lot of work. It would be enlightening to study how game developers handle this complexity and whether there are ways to simplify the process. The Entity Component System (ECS) attempts to mitigate this by adopting an actor-like model, but even then, complexity escalates rapidly as interactions increase.</p><p>[ 30:56 ] Despite efforts to manage these interactions, there comes a point where local decisions require some form of higher-level orchestration. When a bullet is fired, for instance, numerous actions must be coordinated, from ammo count adjustments to triggering animations. All this complexity makes game programming something I would not want to revisit except in the context of my research. However, there lies a vast potential for impactful research in understanding and easing the complexities of game development. Game studios, being the behemoths they are, would undoubtedly embrace methods to alleviate the strenuous nature of their programming efforts. Yes, indeed.</p><p>Jifeng Wu: [32:00] Indeed, that aligns with my central interests. My master's thesis on Python type inference encountered similar challenges to those you've highlighted. In analyzing types, one must grapple with substantial propagation throughout the program. This is precisely the issue at hand, and it serves as a prime motivator in my quest to develop formalizations that simplify the process for programmers, particularly in areas like game development, where complex interactions are commonplace.</p><p>Robert Xiao: [32:45] You've sparked a thought here - this may be a digression - but I'm curious. Has there been research into the amount of typing necessary for untyped code to converge? You made an excellent point about the recursive search required in untyped code to determine types, which resembles an intricate graph search. However, it's more than that because the connections in the 'type graph' are not always apparent. This raises an intriguing theoretical question: At what point in a code base's typing does the cost shift from an exponential to a linear time complexity? It's a highly theoretical question, indeed, and one that surely must have been examined in terms of computational complexity.</p><p>[34:58] It's fascinating because most research focuses on strong typing systems and type inference within defined parameters. But the dynamics change with untyped constructs. Apologies for the tangent - it's just a curious question.</p><p>Jifeng Wu: [35:32] Your point is very interesting, and it is pertinent to my future research endeavors.</p><p>Robert Xiao: [35:39] There ought to be studies on this - how computationally complex is a type system? The PL community has likely delved into this. However, the issue becomes significantly more compelling when considering untyped languages. Sorry for the tangent, but it's a topic worth exploring.</p><p>Jifeng Wu: [36:15] Returning to our discussion, my work in type inference for Python resonates with your mention of interaction-heavy domains like game programming.</p><p>Robert Xiao: [36:27] Certainly. Game programming exemplifies a scenario with extensive object interactions, which is atypical in most systems where such interactions are minimized. For instance, hiring a student triggers a cascade of bureaucratic actions, illustrating how a single decision can activate multiple layers of complexity. Managing these interactions presents a formidable challenge in software engineering - taming the complexity is a substantial part of the job. However, game programming provides a more structured context to study these complexities, with ample open-source resources for research.</p><p>Jifeng Wu: [38:55] Your insights on game programming are quite valuable.</p><p>[38:59] I hadn't realized the prominence of such problems in that field. Your points offer an excellent foundation for my research into these issues.</p><p>Robert Xiao: [39:16] There are indeed intriguing research opportunities at the intersection of AI, software, programming languages, and HCI. Although my HCI pursuits are broad, one key HCI interest is explainable AI. As language models advance, the ability to explain AI operations lags, posing a risk of increasing reliance on inscrutable systems. Advancing explainable AI methodologies will be critical in HCI, particularly in the coming years.</p><p>Jifeng Wu: [42:10] Yes, indeed. What captivates my interest more than the empirical approach to software engineering - like automated bug detection and performance optimization - is the human-computer interaction aspect, particularly making developers' lives easier. This is crucial, especially for rapid prototyping.</p><p>Robert Xiao: [42:42] Right, your IDE example aligns perfectly with that. It's a tool that enhances usability for people - expanding how they interact with typing. It could very well be something for publication. So, you're drawing connections here. When do you expect to graduate?</p><p>Jifeng Wu: [43:05] If all goes according to my advisor's plan, I should graduate in May 2024.</p><p>Robert Xiao: [43:13] Okay. And your advisor is?</p><p>Jifeng Wu: [43:16] Caroline Lemieux from the Software Practices Lab.</p><p>Robert Xiao: [43:20] Oh, yes, she's a recent addition. So, she's guiding you toward a May graduation. And regarding your Ph.D., are you considering continuing in the Software Practices Lab or exploring other areas?</p><p>Jifeng Wu: [43:38] Well, our lab's focus is split between user studies and the technical side, like bug detection or software fuzzing, and, on the other hand, theoretical topics like formal semantics. There isn't much overlap with HCI and usability, so I'm uncertain about where I'll pursue a Ph.D., if at all, but likely not within our Software Practices Lab.</p><p>Robert Xiao: [44:21] That's something to ponder, especially as you approach graduation in May. Are you already applying to Ph.D. programs?</p><p>Jifeng Wu: [44:34] I am still deciding. I'm still considering my options.</p><p>Robert Xiao: [44:38] Sure, it's a big decision. My focus is on VR/AR-related projects, where my funding comes from. It's trickier to shift to developing support tools for developers without an established portfolio. However, I'm open to discussing co-advisory opportunities or committee collaborations if you pursue a Ph.D. here at UBC.</p><p>Jifeng Wu: [45:44] That's encouraging to hear.</p><p>Robert Xiao: [45:45] To clarify, I'm not sure I could supervise a Ph.D., but I'm open to discussing future possibilities.</p><p>Jifeng Wu: [45:59] That aligns with my thoughts as well.</p><p>Robert Xiao: [46:01] Engaging in research discussions can help refine your interests, which is beneficial for crafting a strong research statement for Ph.D. applications. These conversations can guide your initial research direction, even though your focus may evolve.</p><p>Jifeng Wu: [47:10] Thank you for the insights and advice today. They've been very helpful, and I'll reflect on them further.</p><p>Robert Xiao: [47:24] I'm glad to assist. You should have my email address.</p><p>Jifeng Wu: [47:30] Is it listed on your website?</p><p>Robert Xiao: [47:32] It should be - unless my website isn't updated with my current email, which would be a blunder. Let me check.</p><p>Jifeng Wu: [47:36] I'll look it up.</p><p>Robert Xiao: [47:37] If it's on there, then it's correct.</p><p>Jifeng Wu: [47:45] Yes, it's on your site.</p><p>Robert Xiao: [47:47] Great, feel free to reach out anytime.</p><p>Robert Xiao: [47:56] I'm impressed by your work and would be happy to attend your thesis presentation when the time comes.</p><p>Jifeng Wu: [48:15] I appreciate that. Our conversation today has been very enjoyable.</p><p>Robert Xiao: [48:21] It was an enlightening chat, indeed. If you have further questions or topics, don't hesitate to email me.</p><p>Jifeng Wu: [48:34] For now, that's all I have. If something else comes up, I'll send you an email. Thank you.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;The following is a polished version of a conversation with &lt;a href=&quot;https://www.robertxiao.ca/&quot;&gt;Prof. Rob</summary>
      
    
    
    
    <category term="Meeting Minutes" scheme="https://abbaswu.github.io/categories/Meeting-Minutes/"/>
    
    
  </entry>
  
  <entry>
    <title>Nico Ritschel&#39;s Ph.D. Defense Summary</title>
    <link href="https://abbaswu.github.io/2023/10/13/Nico-Ritschel-s-Ph-D-Defense-Summary/"/>
    <id>https://abbaswu.github.io/2023/10/13/Nico-Ritschel-s-Ph-D-Defense-Summary/</id>
    <published>2023-10-13T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.165Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cs.ubc.ca/~ritschel/">Nico Ritschel</a>'s research focuses on refining block-based programming by integrating elements from visual programming to make it more accessible and effective for end-users, especially in the robotics domain.</p><h1 id="problem-statement">Problem Statement</h1><ul><li>Block-based programming is mainly used for computer science education. Can they target other tasks, such as end-user programming?</li><li>The challenge: end-users often need to write larger, real-world programs, contrasting with the simple toy examples students typically handle.</li><li>Traditional block-based programming struggles with scalability, especially in terms of readability.</li><li>While visual end-user programming tools like Excel and Simulink support bigger programs through domain-specific visual abstractions, creating new visual languages is difficult and costly.</li><li>Solution Approach: Merge design features from visual programming into block-based programming languages.</li></ul><h1 id="target-domain-robotics">Target Domain: Robotics</h1><ol type="1"><li><strong>Current Scenario:</strong><ul><li>Professional tools exist, but they're challenging to use.</li><li>There needs to be more effective block-based tools in the domain.</li></ul></li><li><strong>Robot Arms for Factory Floors:</strong><ul><li>Task: Coordinate and synchronize two robot arms.</li><li>Issues: Current block-based languages require complex solutions like nontrivial mutexes.</li><li><strong>Solution &amp; Studies:</strong><ul><li>Proposed two design ideas:<ol type="1"><li>Represent programs for each arm vertically and side-by-side. Synchronized actions appear as shared nodes between the arms.</li><li>A left-to-right flow resembling video editing.</li></ol></li><li>The 'side-by-side' design was selected.</li><li>A study found that end-users using this design outperformed those using a commercial, text-based tool.</li></ul></li></ul></li><li><strong>Mobile Robots for Warehouses &amp; Labs:</strong><ul><li>Task: Handle large tasks across multiple workstations.</li><li>Issues:<ul><li>Difficulty decomposing long programs and locating where to make changes.</li><li></li></ul></li><li><strong>Solutions &amp; Features:</strong><ul><li>Introduced block-based language that supports functional decomposition.</li><li>Provided two separate canvases: one for task composition/movement and the other for low-level task definitions.</li><li>Included triggers as dataflow graphs to improve the visibility of nested expressions and enhance user freedom in structuring programs.</li></ul></li></ul></li></ol><h1 id="questions-addressed-during-the-practice-session">Questions Addressed During the Practice Session</h1><ol type="1"><li><strong>Why focus on the two robotics scenarios?</strong><ul><li>They are important and relevant in the robotics domain.</li><li>These scenarios present challenges for end-users learning to program.</li><li>They represent a complex form of programming that's worth refining.</li></ul></li><li><strong>Would functional programming principles enhance end-user visual programming, given the imperative nature of block-based programming?</strong><ul><li>The inherent complexity in robotics means many elements can't be simplified.</li><li>Introducing functional programming might not necessarily boost user productivity.</li></ul></li><li><strong>What was the environment for user studies?</strong><ul><li>Engaged actual end-users for genuine feedback.</li><li>Also recruited students from non-computer science departments for a broader perspective.</li></ul></li></ol><h1 id="questions-asked-during-the-ph.d.-defense">Questions Asked During the Ph.D. Defense</h1><ul><li>How were the visions and observations formulated?<ul><li>Separate users into traditional versus new environments and then compare.</li><li>Gain knowledge of their needs and patterns.</li><li>Test on a small pool of users to refine the design.</li></ul></li><li>How do you account for the spectrum of end-users regarding programming experience, domain-specific task time, and tool experience?</li><li>Which results were the most and least robust?</li><li>What factors made the tool easy to learn?<ul><li>The "blocks" concept is already well-known.</li><li>The tool matches the users' previous domain-specific knowledge (e.g., separate columns for two arms).</li></ul></li><li>How realistic is the decomposition at scale? Any evidence from related work?<ul><li>More of a "lower bound," limited by the time of the user study.</li></ul></li><li>Why was the comparison made between block-based methods and graph-based methods?<ul><li>Graph-based methods are already used in end-user programming, such as game programming.</li></ul></li><li>What is the importance and implication of the determined p-value?<ul><li>We have a null hypothesis - there is no difference between the performance of the two groups.</li></ul></li><li>What improvements (e.g., 5%) are worthwhile?</li><li>What are the advantages of block-based approaches over dataflow, and how can this be further investigated?<ul><li>Different aspects, e.g., reading vs writing</li><li>Different domains, e.g., robotics vs game</li><li>Different styles of programs</li><li>Different representations of graphs</li></ul></li><li>How are potential accessibility challenges addressed?<ul><li>Already addressed to a degree in the normal block-based domain.</li><li>Domain-specific challenges are directions for future work.</li></ul></li><li>How does the new tool compare with LLMs?<ul><li>Can work together.</li><li>Have advantages in evolution and understanding vs. writing something that would work the first time.<ul><li>Debugging.</li><li>Reliability.</li><li>No training required.</li></ul></li></ul></li><li>What follow-up studies are anticipated for real-world usage? How do you anticipate the tool's usability in practical scenarios? Follow-up studies based on real-world usage in the wild may encounter unanticipated, really specific problems. Is your tool something someone wants to use in practice?</li><li>Would featuring a table of reactive values a la Excel be beneficial?</li><li>How do different domains within computer science influence the tool's design and analysis? What interdisciplinary expertise would be beneficial?<ul><li>Information visualization.</li><li>Designing design drafts with an expert in visualization would be beneficial.</li></ul></li><li>What about your tool's applicability to expert programmers instead of end users?<ul><li>Different design goals.</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.cs.ubc.ca/~ritschel/&quot;&gt;Nico Ritschel&lt;/a&gt;&#39;s research focuses on refining block-based programming by integrating elemen</summary>
      
    
    
    
    <category term="Meeting Minutes" scheme="https://abbaswu.github.io/categories/Meeting-Minutes/"/>
    
    
  </entry>
  
  <entry>
    <title>Conversation with Prof. Margo Seltzer</title>
    <link href="https://abbaswu.github.io/2023/10/10/Conversation-with-Prof-Margo-Seltzer/"/>
    <id>https://abbaswu.github.io/2023/10/10/Conversation-with-Prof-Margo-Seltzer/</id>
    <published>2023-10-10T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction-to-prof.-margo-seltzer-quoted-from-wikipedia">Introduction to Prof. Margo Seltzer (quoted from Wikipedia)</h1><p>"<strong>Margo Ilene Seltzer</strong> is a professor and researcher in computer systems. She is currently the Canada 150 Research Chair in Computer Systems and the Cheriton Family Chair in Computer Science at the <a href="https://en.wikipedia.org/wiki/University_of_British_Columbia" title="University of British Columbia">University of British Columbia</a>. Previously, Seltzer was the Herchel Smith Professor of Computer Science at Harvard University's <a href="https://en.wikipedia.org/wiki/John_A._Paulson_School_of_Engineering_and_Applied_Sciences" title="John A. Paulson School of Engineering and Applied Sciences">John A. Paulson School of Engineering and Applied Sciences</a> and director at the <a href="https://en.wikipedia.org/wiki/Center_for_Research_on_Computation_and_Society" title="Center for Research on Computation and Society">Center for Research on Computation and Society</a>."</p><h1 id="question-how-did-you-conduct-research-across-a-variety-of-domains-from-operating-systems-to-machine-learning-systems">Question: How did you conduct research across a variety of domains, from operating systems to machine learning systems?</h1><p>Prof. Seltzer: I've always been intellectually curious and I find almost all research problems fascinating. Engaging in discussions with diverse people has also fueled my passion. When I was a junior faculty member, I focused on tenure and focused on core systems research, but that was miserable. However, I still explored different areas.</p><p>My deep interest lies in software architecture, even though my Ph.D. was in storage. I was fortunate when another lab decided to support my research. This shift allowed me to progress from storage to core systems.</p><p>I also got interested into data provenance, especially realizing that we could do a lot more at the systems level.</p><p>Transitioning to machine learning was a natural progression, driven mainly by collaborations with graduate students and other partners.</p><h1 id="question-why-did-you-pursue-a-ph.d.-in-storage-if-you-were-more-interested-in-core-systems">Question: Why did you pursue a Ph.D. in storage if you were more interested in core systems?</h1><p>Prof. Seltzer: Before pursuing my Ph.D., I was primarily involved with databases. However, as I delved deeper, my curiosity veered towards system issues.</p><h1 id="question-how-do-you-manage-evolving-interests-during-a-ph.d.">Question: How do you manage evolving interests during a Ph.D.?</h1><p>Prof. Seltzer: It's uncommon for Ph.D. students to plot a lifetime research agenda. Instead, it's about producing one miracle per paper and developing the skills to do research for your whole life. The key is to focus on accomplishing your first piece of independent research during your Ph.D.</p><p>Choosing a supervisor you get along well with is most important. It's essential to be involved in an interesting area and join a lab that aligns with your interests. However, a perfect match isn't always necessary. Looking at co-supervised students can give insights into potential co-supervision opportunities.</p><p>As a Ph.D. student, your primary goal should be to define your research problem. Although you shouldn't jump between entirely different areas, it's crucial to select a project that genuinely interests you in the first year. Other interests can be pursued as side projects.</p><p>To maintain engagement, pick a broad domain that offers a plethora of projects you find captivating.</p><p>Before starting a Ph.D., actively seek out research papers that intrigue you and identify the labs behind them.</p><h1 id="question-whats-your-vision-for-the-future-of-computer-systems">Question: What's your vision for the future of Computer Systems?</h1><p>Prof. Seltzer: A pressing concern is that people are not very good at writing software that works. We need to develop strategies to create software with minimal bugs from the ground up. Embracing modularity can be a solution, and the solution is about software architecture.</p><p>Researchers focus on verifying existing software products because the publication cycle is way too short. Moreover, we lack good metrics for evaluating software architecture, and there is no equivalent of a debugger for software architecture. Software architecture, in its current state, remains an art more than a well-defined discipline. Often, professionals in the field rely heavily on mentors, and they don't see the growth of a new generation of software architects.</p><h1 id="personal-comments-and-recommendations">Personal Comments and Recommendations:</h1><p>I'm pleased to note your inspiration derived from challenges in the 'Type Inference for Python' project, including the importance of formalizations and specifications both for the design goal and for implementation, the tedious and fault-prone task of setting up an evaluation pipeline, etc.</p><p>Deep learning thrives in domains with a clear ground truth. In other scenarios, basic probabilistic methods might offer better results.</p><p>Your task of combining AST traversal with introspection of live objects reminds me of the work I did in "StarFlow: A Script-Centric Data Analysis Environment" and Arpan Gujarati's tracing infrastructure efforts in Python.</p><p>Should you wish to delve deeper into software architecture for data science and machine learning, I recommend focusing on constructing intricate software like operating systems instead of shorter data wrangling scripts. Or you can explore the challenges in experimental frameworks. For insights on this, consider discussing with Joe Wonsil. Additionally, Philip Guo at UCSD has an intriguing Ph.D. thesis about tools for research programmers that might be of interest to you.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;introduction-to-prof.-margo-seltzer-quoted-from-wikipedia&quot;&gt;Introduction to Prof. Margo Seltzer (quoted from Wikipedia)&lt;/h1&gt;
&lt;p&gt;&quot;&lt;str</summary>
      
    
    
    
    <category term="Meeting Minutes" scheme="https://abbaswu.github.io/categories/Meeting-Minutes/"/>
    
    
  </entry>
  
  <entry>
    <title>Pre-MICCAI Workshop@UBC Observations and Gained Insights</title>
    <link href="https://abbaswu.github.io/2023/10/08/Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights/"/>
    <id>https://abbaswu.github.io/2023/10/08/Pre-MICCAI-Workshop-UBC-Observations-and-Gained-Insights/</id>
    <published>2023-10-08T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.169Z</updated>
    
    <content type="html"><![CDATA[<p>From the <a href="https://sites.google.com/view/pre-miccai-ubc/home">Pre-MICCAI Workshop@UBC</a> website:</p><blockquote><p>The Pre-MICCAI Workshop is a dynamic and innovative platform that unites machine learning and medical computer vision. As a prelude to the prestigious MICCAI (Medical Image Computing and Computer-Assisted Intervention) conference, this workshop serves as a vital nexus where experts, researchers, and enthusiasts converge to explore cutting-edge advancements, exchange knowledge, and foster collaborative partnerships in the field of medical image analysis.</p></blockquote><figure><img src="https://raw.githubusercontent.com/abbaswu/abbaswu.github.io-images/main/Pre-MICCAI-Workshop-UBC-Selfie.jpg" alt="" /><figcaption>Selfie</figcaption></figure><h1 id="shaoting-zhang-shanghai-ai-lab---keynote-talk-2---foundation-models-in-medicine-generalist-vs-specialist">Shaoting Zhang (Shanghai AI Lab) - Keynote Talk 2 - Foundation Models in Medicine: Generalist vs Specialist</h1><ul><li>Advantages of Large Models:<ul><li>Emergent abilities.</li><li>Long-tail problems (only a small amount of fine-tuning is required for downstream tasks and does not require a tremendous amount of data collection and labeling).</li><li>Model sharing strengthens data security.</li></ul></li><li>Shanghai AI Lab presents <a href="https://github.com/openmedlab">OpenMEDLab (open-source medical image and language foundation models)</a>.</li><li>Utilizing a single model with varied prompts for diverse tasks.</li><li>Large language model training encompasses:<ul><li>Self-supervised pre-training.</li><li>Instruction tuning.</li><li>RLHF.</li><li>Plugins for accessing updated information without retraining.</li></ul></li><li>Computer vision researchers lean towards generalist models due to the technical challenges.</li><li>Clinicians prefer specialist models to solve day-to-day work.</li></ul><p>Question: Will medical foundation models support more modalities in the future besides vision and language?</p><p>Answer:</p><ul><li>People will still focus on one modality for one model with high accuracy to address practical business demands.</li><li>Multiple models can be used on demand to handle multimodal data.</li></ul><h1 id="briefings">Briefings</h1><h2 id="sana-ayromlou---continual-class-specific-impression-for-data-free-class-incremental-learning">Sana Ayromlou - Continual Class-Specific Impression for Data-free Class Incremental Learning</h2><ul><li>Focuses on training models over newly introduced classes, termed <a href="https://en.wikipedia.org/wiki/Incremental_learning">incremental learning</a>.</li><li>Challenges include the loss of old data, resulting in catastrophic forgetting.</li><li>Proposed Solution: Generate synthetic medical data from prior classes using <a href="https://arxiv.org/abs/2201.10787">model inversion</a> (extracting training data from the model) and employing <a href="https://arxiv.org/abs/2102.09517">cosine-normalized cross-entropy loss</a>.</li></ul><h2 id="hooman-vaseli---protoasnet">Hooman Vaseli - ProtoASNet</h2><ul><li>Emphasizes the importance of interpretability in AI solutions, especially in healthcare.</li><li>Core Technology: <a href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html">Prototypical neural networks</a>, which "learn a metric space in which classification can be performed by computing distances to prototype representations of each class."</li></ul><h1 id="ruogu-fang-university-of-florida---keynote-talk-4---a-tale-of-two-frontiers-when-brain-meets-ai">Ruogu Fang (University of Florida) - Keynote Talk 4 - A Tale of Two Frontiers: When Brain Meets AI</h1><p>Research Vision:</p><ul><li>Integrate domain knowledge over mere data-driven approaches.</li><li>Harness neuroscience principles for next-gen AI designs.</li><li>Leverage AI in testing neural science hypotheses and promoting brain health.</li></ul><p><a href="https://www.sciencedirect.com/science/article/pii/S1361841518307734">Deep Evolutionary Networks with Expedited Genetic Algorithms for Medical Image Denoising</a></p><ul><li>Auto feature extraction and hyperparameter search are major pain points in deep learning research (compared with traditional machine learning research) faced by deep learning researchers.</li><li>Fine gene transfer learning to optimize on a larger dataset - c.f. <a href="https://www.investopedia.com/financial-edge/0412/the-best-portfolio-balance.aspx">portfolio balance</a> in finance</li><li>Question: Is it possible to combine the genetic algorithm that maintains a gene pool of neural networks with ensemble learning?<ul><li>Answer: Different objective.</li></ul></li></ul><p><a href="https://www.biorxiv.org/content/10.1101/2023.04.16.537079v2.abstract">Emergence of Emotion Selectivity in A Deep Neural Network Trained to Recognize Visual Objects</a></p><ul><li>Simple, interpretable neural network architecture based on biology.</li><li>Representation similarity between the DNN model and brain amygdala.</li><li>$1M NSF funding.</li></ul><p><a href="https://www.nature.com/articles/s41598-020-80312-2">Modular machine learning for Alzheimer's disease classification from retinal vasculature</a></p><ul><li>Retina data is easy to collect.</li><li>A lot of information (gender, body mass index) can be seen from the retina.</li><li>The results are interpretable.</li></ul><h1 id="hervé-lombaert-ets-montreal---keynote-talk-3---geometric-deep-learning---examples-on-brain-surfaces">Hervé Lombaert (ETS Montreal) - Keynote Talk 3 - Geometric Deep Learning - Examples on Brain Surfaces</h1><p>Research directions:</p><ul><li>Geometry and Machine Learning.</li><li>Correspondences and variability existent in the brain.</li></ul><p>Motivation:</p><ul><li>Traditional algorithms frequently rely on an image grid (pixels). However, in neuroimaging, data is often on 3D surfaces. Two neighboring points may be neighbors but may lie very far away on such a surface.</li><li>How to learn on such surfaces? How do we transfer convolution and pooling on images to such surfaces?</li></ul><p>Solution:</p><ul><li>Represent surfaces as graphs.</li><li>Project problem into spectral space (<a href="https://en.wikipedia.org/wiki/Spectral_shape_analysis">spectral shape analysis</a>).<ul><li>An object's vibration pattern is governed by shape - spectral space captures a unique intrinsic shape signature.</li><li>Extract spectral signature via spectral decomposition and exploit to find correspondences.</li><li>Enables transforming convolutions on surfaces to convolutions on spectral embeddings, enabling classical architectures on brain surfaces.</li></ul></li></ul><p>Ongoing work:</p><ul><li><a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)">Active learning</a> to reduce annotation effort - focus on sample-level uncertainty and find the most uncertain images.<ul><li>Goals: Informative and diverse samples.</li><li>Works:<ul><li><a href="https://link.springer.com/chapter/10.1007/978-3-031-17027-0_5">TAAL: Test-time augmentation for active learning in medical image segmentation</a></li><li><a href="https://arxiv.org/abs/2301.07670">Active learning for medical image segmentation with stochastic batches</a></li></ul></li></ul></li></ul><h1 id="ali-bashashatiruogu-fangshaoting-zhanghervé-lombaerjun-ma---panel-discussion">Ali Bashashati/Ruogu Fang/Shaoting Zhang/Hervé Lombaer/Jun Ma - Panel Discussion</h1><h2 id="the-influence-of-large-language-models-is-growing-significantly.-what-changes-do-you-think-llms-will-bring-about-in-medical-imaging-from-both-positive-and-negative-sides">The influence of Large Language Models is growing significantly. What changes do you think LLMs will bring about in medical imaging (from both positive and negative sides)?</h2><ul><li>Language contributes to improved performance.</li><li>Still need a diversity of models to investigate different modalities and tasks.</li><li>Large language models help in day-to-day routine tasks. They are a copilot which facilitates the processing of huge amounts of information in pathology and brain research.</li><li>Reduces cost and boosts accessibility for patients.</li><li>Multimodal data integration.</li><li>LLMs face data privacy and trustworthiness.</li><li>When to use LLMs and when to use human abilities requires careful thinking.</li></ul><h2 id="what-other-recent-medical-image-analysis-advancements-excite-you-the-most">What other recent medical image analysis advancements excite you the most?</h2><ul><li>Classic problems like segmentations and how to capture geometry remain unsolved.</li><li>More comprehensive and dynamic brain-inspired, biologically-inspired AI.</li><li>Understanding the biology behind the data will help you design more applicable models. Those models can better make a difference</li><li>Prior knowledge is important in addition to big data. Foundational models will explore all non-synthetic data in the next few years; no new data will exist.</li><li>Montreal is a major hub for neuroscience and AI.</li></ul><h2 id="for-the-many-students-here-what-technical-skills-and-knowledge-should-the-next-generation-of-medical-image-analysis-researchers-prepare-for">For the many students here, what technical skills and knowledge should the next generation of medical image analysis researchers prepare for?</h2><ul><li>Know the neglected basics, e.g., solid mathematical background and proficiency in programming</li><li>Understand the data</li><li>Ability to explain the results and ask the question of why and how</li><li>Visualization is very important for both exploratory data analysis and publishing</li><li>Learning from mistakes - find out why a model doesn't work instead of throwing in different models</li><li>Ask yourself: Who will care about an increase in accuracy? Is it significant? Will it have tradeoffs in robustness, explainability, etc.?</li><li>Quickly take up new skills (mathematics, programming, etc.)</li><li>Research paradigms have changed in the foundation model era - how to leverage foundation models for your field to stand on the shoulders of giants?</li><li>Low-level implementation details such as preprocessing, multiprocessing in coding for large-scale data, model development, multi-node distributed training, efficient fine-tuning, and model deployment on constrained environments are also critical skills.</li><li>Work and have fun at the same time.</li><li>Perseverance in the face of failure is one of the most essential qualities for Ph.D. students.</li></ul><h2 id="question-the-future-of-models-for-specific-tasks-e.g.-segmentation-vs-end-to-end-models.">Question: The future of models for specific tasks (e.g., segmentation) vs end-to-end models.</h2><ul><li>New models for specific tasks make lovely reads.</li><li>Methodology will change, but specific tasks will stay there. However, improving specific tasks will gradually shift towards industry. Universities will focus on publishing the first paper in a domain, while industry will focus on publishing the last paper in a domain.</li><li>In the end, we care about helping patients.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;From the &lt;a href=&quot;https://sites.google.com/view/pre-miccai-ubc/home&quot;&gt;Pre-MICCAI Workshop@UBC&lt;/a&gt; website:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Pre-MIC</summary>
      
    
    
    
    <category term="Conferences" scheme="https://abbaswu.github.io/categories/Conferences/"/>
    
    
  </entry>
  
  <entry>
    <title>Understanding the Name, Structure, and Loss Function of the Variational Autoencoder</title>
    <link href="https://abbaswu.github.io/2023/09/30/Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder/"/>
    <id>https://abbaswu.github.io/2023/09/30/Understanding-the-Name-Structure-and-Loss-Function-of-the-Variational-Autoencoder/</id>
    <published>2023-09-30T07:00:00.000Z</published>
    <updated>2023-12-30T15:38:35.429Z</updated>
    
    <content type="html"><![CDATA[<p>Despite the intuitive appeal of <a href="https://en.wikipedia.org/wiki/Variational_autoencoder">variational autoencoders (VAEs)</a>, their underlying principles can be elusive. After extensive research across papers and online resources, I will summarize the core insights behind the VAE's name, structure, and loss function and try to explain <strong>how the mathematical formulas used to describe the VAE came into being from first principles</strong>, as opposed to simply providing interpretations for them.</p><h2 id="basics-of-vaes">Basics of VAEs</h2><p>VAEs are probabilistic generative models, when trained on a dataset <span class="math inline">\(X\)</span>, allow us to sample from a latent variable <span class="math inline">\(Z\)</span> and generate output resembling samples in <span class="math inline">\(X\)</span> through a trained neural network <span class="math inline">\(f: Z \rightarrow X\)</span>.</p><p>This can be formulated as making the probability of generating <span class="math inline">\(X = x\)</span> as close as possible to the actual <span class="math inline">\(P(X = x)\)</span> (known quality) under the entire generative process.</p><h2 id="ideal-training-goal">Ideal Training Goal</h2><p>In the <strong>ideal situation</strong>, based on the <a href="https://en.wikipedia.org/wiki/Marginal_distribution">marginal distribution formula</a> we have <span class="math inline">\(P(X = x) = \int{P(X = x | Z = z) P(Z = z) dz}\)</span>. Thus, the training goal of variational autoencoders is to <strong>make the actual <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span> as close to <span class="math inline">\(P(X = x)\)</span> as possible</strong>.</p><h2 id="latent-variable-distribution">Latent Variable Distribution</h2><p>VAEs select a <strong>multivariate normal distribution</strong> for the latent variable <span class="math inline">\(Z\)</span> based on the principle that <a href="https://doi.org/10.1145/318242.318443">any distribution in <span class="math inline">\(d\)</span> dimensions can be generated by mapping normally distributed variables through a sufficiently complicated function</a>, which could be approximated using the neural network <span class="math inline">\(f: Z \rightarrow X\)</span> we train.</p><h2 id="approximation-challenge">Approximation Challenge</h2><p>Having reasonably decided <span class="math inline">\(Z \sim N(0, I)\)</span>, we may calculate the actual <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span>. This is straightforward to approximate: we can randomly sample a large number of <span class="math inline">\(Z\)</span> values <span class="math inline">\(\{z_1, \dots, z_n\}\)</span>, and approximate <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span> as <span class="math inline">\(\sum_{j}^{n}{P(X = x | Z = z_j)}\)</span>.</p><p>However, for most <span class="math inline">\(Z\)</span> values, <span class="math inline">\(P(X = x | Z)\)</span> will be nearly zero, contributing almost nothing to our calculation. This is especially the case in high dimensional spaces, for which an extremely large number of samples of <span class="math inline">\(Z\)</span> may be required.</p><p>To address the problem, we can attempt to <strong>sample values of <span class="math inline">\(Z\)</span> that are likely to have produced <span class="math inline">\(X = x\)</span> and compute <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span> just from those</strong>.</p><h2 id="the-variational-aspect">The "Variational" Aspect:</h2><p>To do so, we can <strong>fit another parametrized function</strong> <span class="math inline">\(Q(Z | X = x)\)</span>, which can give us a distribution over <span class="math inline">\(Z\)</span> values that are likely to produce <span class="math inline">\(X = x\)</span> through <span class="math inline">\(f: Z \rightarrow X\)</span> given <span class="math inline">\(X = x\)</span>. This is an example of a <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational Bayesian method</a>, which involves finding an "optimal" function (a task known as <a href="https://en.wikipedia.org/wiki/Calculus_of_variations">variational calculus</a>) and is the source of the word "variational" in variational autoencoders.</p><h2 id="minimizing-divergence">Minimizing Divergence</h2><p>Theoretically, the values of <span class="math inline">\(Z\)</span> that are likely to have produced <span class="math inline">\(X = x\)</span> follow the conditional distribution <span class="math inline">\(P(Z | X = x)\)</span>. <strong>Thus, our original goal of making the actual <span class="math inline">\(\int{P(X = x | Z = z) P(Z = z) dz}\)</span> as close to <span class="math inline">\(P(X = x)\)</span> as possible can be transformed to minimizing the Kullback-Leibler divergence between <span class="math inline">\(P(Z | X = x)\)</span> and <span class="math inline">\(Q(Z | X = x)\)</span></strong>:</p><p><span class="math display">\[KL(Q(Z | X = x) || P(Z | X = x)) = \int{Q(Z = z | X = x) \log{\frac{Q(Z = z | X = x)}{P(Z = z | X = x)}} dz}\]</span></p><p>According to Bayes' Law,</p><p><span class="math display">\[P(Z = z | X = x) = \frac{P(X = x | Z = z) P(Z = z)}{P(X = x)}\]</span></p><p>Thus, we have:</p><p><span class="math display">\[\int{Q(Z = z | X = x) \log{\frac{Q(Z = z | X = x) P(X = x)}{P(X = x | Z = z) P(Z = z)}} dz}\]</span></p><p><span class="math display">\[= \int{Q(Z = z | X = x) (\log{\frac{Q(Z = z | X = x)}{P(Z = z)}} + \log{P(X = x)} - \log{P(X = x | Z = z)}) dz}\]</span></p><p><span class="math display">\[= \int{Q(Z = z | X = x) \log{\frac{Q(Z = z | X = x)}{P(Z = z)}} dz} + \int{Q(Z = z | X = x) \log{P(X = x)} dz} - \int{Q(Z = z | X = x) \log{P(X = x | Z = z)} dz}\]</span></p><p>Note that:</p><p><span class="math display">\[\int{Q(Z = z | X = x) \log{\frac{Q(Z = z | X = x)}{P(Z = z)}} dz} = KL(Q(Z | X = x) || P(Z))\]</span></p><p><span class="math display">\[\int{Q(Z = z | X = x) \log{P(X = x)} dz} = \log{P(X = x)} \int{Q(Z = z | X = x)} dz = \log{P(X = x)}\]</span></p><p>Thus, we have:</p><p><span class="math display">\[KL(Q(Z | X = x) || P(Z | X = x)) = KL(Q(Z | X = x) || P(Z)) + \log{P(X = x)} - \int{Q(Z = z | X = x) \log{P(X = x | Z = z)} dz}\]</span></p><p>As <span class="math inline">\(\log{P(X = x)}\)</span> is constant, if we were to minimize <span class="math inline">\(KL(Q(Z | X = x) || P(Z | X = x))\)</span>, we should minimize:</p><p><span class="math display">\[KL(Q(Z | X = x) || P(Z)) - \int{Q(Z = z | X = x) \log{P(X = x | Z = z)} dz}\]</span></p><p>To further transfer that into a calculatable function, we need to be more specific about the form that <span class="math inline">\(Q(Z | X)\)</span> will take. The usual choice is to say that <span class="math inline">\(Q(Z | X = x) = N(Z | \mu(X = x), \Sigma(X = x))\)</span>, i.e., <span class="math inline">\(Q(Z | X = x)\)</span> follows a Gaussian distribution where the mean and covariance matrix are calculated by <strong>parameterized functions (trained neural networks)</strong> given <span class="math inline">\(X = x\)</span>. In this case, <strong>fitting <span class="math inline">\(Q(Z | X = x)\)</span> involves training these neural networks</strong>.</p><figure><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*kXiln_TbF15oVg7AjcUEkQ.png" alt="" /><figcaption><span class="math inline">\(Q(Z | X = x) = N(Z | \mu(X = x), \Sigma(X = x))\)</span></figcaption></figure><p>The advantages of this choice are <em>computational</em>, as <span class="math inline">\(KL(Q(Z | X = x) || P(Z)) + \log{P(X = x)}\)</span> is now <strong>a KL-divergence between two multivariate Gaussian distributions</strong>, which can be computed in <strong>closed form</strong>.</p><p>As for <span class="math inline">\(\int{Q(Z = z | X = x) \log{P(X = x | Z = z)} dz}\)</span>, it depicts the expected log-likelihood of generating <span class="math inline">\(X = x\)</span> as the VAE's output through <span class="math inline">\(f(Z)\)</span> when sampling from <span class="math inline">\(Q(Z = z | X = x)\)</span> given <span class="math inline">\(X = x\)</span>. Thus, it can be treated as the "reconstruction loss" of the VAE, and different closed-form indices, such as mean square error, may be used as proxies of it depending on the project domain.</p><h2 id="why-autoencoders">Why "Autoencoders"?</h2><p>Despite the mathematical basis of VAEs being quite different from classical autoencoders, they are named "autoencoders" due to their final training objective involving an encoder (the neural networks <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> determining mean and covariance) and a decoder (the neural network <span class="math inline">\(f\)</span>), which resembles a traditional autoencoder in structure.</p><h2 id="references">References</h2><ul><li>https://arxiv.org/abs/1606.05908</li><li>https://agustinus.kristia.de/techblog/2016/12/10/variational-autoencoder/</li><li>https://arxiv.org/abs/1312.6114</li><li>https://arxiv.org/abs/1907.08956</li><li></li><li>https://stats.stackexchange.com/questions/485488/should-reconstruction-loss-be-computed-as-sum-or-average-over-input-for-variatio</li><li>https://stats.stackexchange.com/questions/540092/how-do-we-get-to-the-mse-in-the-loss-function-for-a-variational-autoencoder</li><li>https://stats.stackexchange.com/questions/464875/mean-square-error-as-reconstruction-loss-in-vae</li><li>https://stats.stackexchange.com/questions/323568/help-understanding-reconstruction-loss-in-variational-autoencoder</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Despite the intuitive appeal of &lt;a href=&quot;https://en.wikipedia.org/wiki/Variational_autoencoder&quot;&gt;variational autoencoders (VAEs)&lt;/a&gt;, thei</summary>
      
    
    
    
    <category term="Mathematics" scheme="https://abbaswu.github.io/categories/Mathematics/"/>
    
    
  </entry>
  
  <entry>
    <title>My Software Engineering Philosophy</title>
    <link href="https://abbaswu.github.io/2023/09/24/My-Software-Engineering-Philosophy/"/>
    <id>https://abbaswu.github.io/2023/09/24/My-Software-Engineering-Philosophy/</id>
    <published>2023-09-24T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.165Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>If you deprive yourself of outsourcing and your competitors do not, you're putting yourself out of business. Lee Kuan Yew</p></blockquote><ul><li>Do the high-level, high-value <a href="https://en.wikipedia.org/wiki/Requirements_engineering"><strong>requirements</strong></a>, <a href="https://en.wikipedia.org/wiki/Requirements_analysis"><strong>analysis</strong></a>, and <a href="https://en.wikipedia.org/wiki/Software_design"><strong>design</strong></a> work in an <em>incremental</em> fashion while not sacrificing <em>rigor</em>.</li><li>Maximize the utilization of <em>tools</em> that make coding, testing, and operations as <em>cheap, trivial, straightforward, and error-free</em> as possible, minimizing <em>technical debt</em>, including:<ul><li>Generative AI tools like ChatGPT.</li><li>Functional Programming.</li></ul></li><li>A critique on both <a href="https://en.wikipedia.org/wiki/Waterfall_model">the waterfall model</a> and <a href="https://en.wikipedia.org/wiki/Agile_software_development">the agile model</a>.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;If you deprive yourself of outsourcing and your competitors do not, you&#39;re putting yourself out of business. Lee Kuan Yew&lt;/p</summary>
      
    
    
    
    <category term="Reflections" scheme="https://abbaswu.github.io/categories/Reflections/"/>
    
    
  </entry>
  
  <entry>
    <title>The Cornell, Maryland, Max Planck Pre-doctoral Research School 2023 Observations and Gained Insights</title>
    <link href="https://abbaswu.github.io/2023/09/08/The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023-Observations-and-Gained-Insights/"/>
    <id>https://abbaswu.github.io/2023/09/08/The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023-Observations-and-Gained-Insights/</id>
    <published>2023-09-08T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.169Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://raw.githubusercontent.com/abbaswu/abbaswu.github.io-images/main/The-Cornell-Maryland-Max-Planck-Pre-doctoral-Research-School-2023.jpeg" alt="" /><figcaption>Group Photo</figcaption></figure><h2 id="panel-session-2-research-in-industry-vs.-academia">Panel Session 2: "Research in industry vs. academia"</h2><h3 id="problem-focus-recognition">Problem Focus &amp; Recognition</h3><ul><li>Industry tends to focus on concrete problems.</li><li>In academia, broader issues are often addressed.</li><li>Authorship and credit in academia is complex. It's not a zero-sum game. It's not just about who is first or second author; giving credit to students doesn't mean professors won't get any.</li></ul><h3 id="publication-quality">Publication &amp; Quality</h3><ul><li>The emphasis is on publishing fewer papers but ensuring they are of high quality. It's not about the quantity but the impact and quality of the papers.</li><li>The first, last, or best paper on a topic are the most influential.</li></ul><h3 id="career-path">Career Path</h3><ul><li>Before securing a tenure professor position, many go through multiple postdocs and even stints as industrial research scientists.</li><li>Only about 10-20% of PhDs eventually become faculty.</li><li>Some research scientists find academic-like environments within the right industry groups.</li></ul><h3 id="factors-differentiating-academia-and-industry">Factors Differentiating Academia and Industry:</h3><ul><li>Industrial research must eventually have some commercial value.</li><li>In academia, there are constraints like obtaining funding, student recruitment, and equipment acquisition.</li><li>Academics have better job security and can rebound from mistakes.</li><li>Industry doesn't need to chase grants or funding in the same way academia does.</li></ul><h3 id="skills-transitions">Skills &amp; Transitions</h3><ul><li>Transferring skills between departments or companies is straightforward.</li><li>Transitioning between academia and industry is often a one-way street. It's challenging to return to academia from industry unless one maintains a consistent publishing record and works on research-valued projects.</li></ul><h3 id="geographical-and-topic-mobility">Geographical and Topic Mobility</h3><p>Researchers are encouraged to be flexible, moving across countries and topics.</p><h3 id="work-life-balance">Work-Life Balance</h3><ul><li>Systems vary across locations.</li><li>Enforce personal boundaries and learn to say no.</li><li>A balance doesn't mean absence of stress. In the industry, even if the work-life balance is okay, stress may arise from working on undesired projects or facing peer pressure.</li><li>Find people who become friends with you.</li></ul><h3 id="two-body-problem">Two-Body Problem</h3><p>It's more of an issue in academia than in industry since it's easier to change companies than academic institutions.</p><h3 id="personality-and-approach">Personality and Approach</h3><ul><li>Industry caters to hackers and those interested in tooling.</li><li>Academics focus on research and higher purposes and see coding as a tool. Effective communication, including selling your idea in proposals and talks, is vital.</li></ul><h3 id="startups-vs.-phd-journey">Startups vs. PhD Journey</h3><ul><li>Both require a significant commitment, typically around 6-8 years to IPO.</li><li>Startups demand full devotion, often with little to no work-life balance.</li></ul><h3 id="funding-tenure">Funding &amp; Tenure</h3><ul><li>If a grant from a company fails, there will be no direct legal consequences, but the likelihood of getting another might be reduced.</li><li>Tenure provides a basic salary and job security, but researchers still need to raise funds for their research.</li><li>Doing a job aligned research can be beneficial for dissertation and future career opportunities.</li></ul><h2 id="laxman-dhulipala-2nd-lecture"><a href="https://cloud.mpi-sws.org/index.php/s/n97WnKRxgYoLcCL?dir=undefined&amp;openfile=84146671">Laxman Dhulipala (2nd Lecture)</a></h2><ul><li>Graphs are ubiquitous structures. Implementing high-performance graph algorithms speeds up scientific discovery.</li><li>I don't work on dense graphs. Real-world graphs are sparse, and I haven't seen a dense graph in practice in 10 years.</li><li>I focus on shared-memory algorithms and don't recommend programming supercomputers until you have to.</li><li>Recommended reading: <a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf"><strong>Scalability! But at what COST?</strong></a></li><li>Should batch updates to dynamic graphs<ul><li>More parallelism</li><li>Reduces the cost of each update</li></ul></li><li>Representing adjacency information using purely functional trees are safe for concurrency.</li></ul><h2 id="guest-lecture-yiting-xia"><a href="https://cloud.mpi-sws.org/index.php/s/n97WnKRxgYoLcCL?dir=undefined&amp;openfile=84146671">Guest Lecture: Yiting Xia</a></h2><ul><li>There are different available connections at different time slices.</li><li>Precomputing routes and handling link failure is still work in progress.</li></ul><h2 id="group-mentoring-session">Group-Mentoring Session</h2><h3 id="peter-druschel-and-bobby-bhattacharjee"><a href="https://people.mpi-sws.org/~druschel/">Peter Druschel</a> and <a href="https://www.cs.umd.edu/people/bobby">Bobby Bhattacharjee</a></h3><h4 id="key-skills-and-knowledge">Key Skills and Knowledge</h4><ul><li>Emphasized the importance of academic aptitude and the ability to work in unstructured environments.</li></ul><h4 id="problem-solving-approach">Problem-solving Approach</h4><ul><li>Seek problems that are significant, solvable, and align with your skill set.</li><li>Recognize that one may not always approach the right problem from the best angle.</li><li>Handling setbacks is crucial. Time spent on tackling a problem is never lost.</li><li>Resilience, dedication, and discipline are essential traits for success.</li><li>Read many things that are loosely related to solve a problem, as they might offer insights.</li></ul><h4 id="application-strategy">Application Strategy</h4><ul><li>Apply to a minimum of 5-10 institutions.</li><li>Do the homework for providing a strong application, especially given low acceptance rates, like 10%.</li></ul><h4 id="interests-and-graduate-programs">Interests and Graduate Programs</h4><ul><li>Have a broad range of interests when considering a graduate program.</li><li>Opt for programs that offer a wide variety of choices.</li><li>Expressing diverse interests in applications can improve acceptance chances.</li><li>It's advisable not to close one's doors apriori.</li></ul><h4 id="monitoring-progress-in-grad-programs">Monitoring Progress in Grad Programs</h4><ul><li>A competent group advisor is crucial, as they will guide and look out for students challenges like selecting an excessively challenging problem, lacking motivation, or poor time management.</li><li>Set achievable milestones that lead to publications, helping to build a solid publication record. ### <a href="https://mtoneva.com/">Mariya Toneva</a></li></ul><h4 id="changing-discipline-during-ph.d.">Changing Discipline during Ph.D.</h4><ul><li>Evaluate if the institution has the necessary resources to support this transition.</li></ul><h4 id="traits-of-an-ideal-ph.d.-student">Traits of an Ideal Ph.D. Student</h4><ul><li>Effective communication skills.</li><li>Strong critical thinking abilities.</li><li>A robust computational background.</li><li>Prior research experience.</li></ul><h4 id="linguistics">Linguistics</h4><ul><li>Noted a resurgence in the domain of linguistics as opposed to pure data-driven techniques.</li></ul><h4 id="mpi-sws">MPI-SWS</h4><ul><li>MPI-SWS is highly recommended for programming languages, especially when collaborating with diverse groups of people.</li></ul><h4 id="diving-into-nlp-natural-language-processing---hop-on-now">Diving into NLP (Natural Language Processing) - Hop On Now?</h4><ul><li>When considering venturing into NLP, focus on:<ul><li>Experts who have a distinct vision in a less-saturated niche.</li><li>Those with substantial experience in related fields, such as the intersection of NLP and robotics.</li></ul></li></ul><h4 id="distinguishing-yourself-in-applications">Distinguishing Yourself in Applications</h4><ul><li>To stand out:<ul><li>Foster qualities like initiative, drive, and ambition.</li><li>Accumulate experiences that align with and support your academic and research interests.</li><li>Obtain references that can vouch for your character and work ethic.</li></ul></li><li>It's also essential to explore and consider multiple options or paths.</li></ul><h3 id="lorenzo-alvisi"><a href="https://www.engineering.cornell.edu/faculty-directory/lorenzo-alvisi">Lorenzo Alvisi</a></h3><h4 id="cultivating-an-academic-sense">Cultivating an Academic Sense</h4><ul><li>To nurture an academic mindset, one should assess how an individual performs when faced with a problem.</li><li>He mentioned the "Dijkstra club" at UT Austin as an example.</li><li>Emphasized the significance of "beautiful work" and that it's crucial for individuals to produce work of beauty and quality.</li><li>Observing and learning from the endeavors of others is beneficial.</li></ul><h4 id="lifes-blueprint">Life's Blueprint</h4><ul><li>Life does not come with a set map but rather a compass for direction.</li><li>Professor Alvisi never limited his imagination about his capabilities.</li><li>Guiding principles in life:<ul><li>Seeking personal happiness.</li><li>Maintaining healthy relationships.</li><li>Pursuing a fulfilling job that combines happiness with challenges.</li></ul></li><li>Acceptance of uncertain outcomes: One might not always know if they will succeed or fail.</li><li>The importance of personal growth: Find joy in self-improvement.</li><li>Shared personal experience of pursuing two Ph.D. degrees, the first of which was at an institution he didn't particularly favor. Highlighted that struggles are often hidden from view.</li></ul><h4 id="career-perspectives">Career Perspectives</h4><ul><li>One's career doesn't necessarily peak at a fixed point; there's always potential for growth, including entering academia.</li><li>Career choices are not always black and white; it depends on personal preferences and aspirations, such as seeking excellent opportunities close to home.</li><li>Consider the duration of your investments in particular career choices. Not every commitment needs to be long-term.</li></ul><h4 id="balancing-hobbies-and-work">Balancing Hobbies and Work</h4><ul><li>Prof. Alvisi shared advice from his mentor's mentor about integrating hobbies into professional life.</li><li>While he had diverse interests, he made sacrifices to focus on computer science due to his intellectual capacities. Some hobbies were too time-consuming.</li><li>Emphasized the importance of hobbies as they provide a necessary balance and maintain mental well-being.</li></ul><h4 id="addressing-the-two-body-problem">Addressing the Two-Body Problem</h4><ul><li>Universities recognize the challenge when both partners in a relationship are professionals.</li><li>If partners excel in different domains, there's potential for both to be hired with attractive incentives.</li><li>Solutions include proactive planning, alternating priorities between partners over the years, and considering remote work opportunities.</li></ul><h4 id="other-insights">Other Insights</h4><ul><li>Mentioned the Sloan Fellowship as a notable achievement before tenure.</li><li>Advised young professionals to delay specialization as long as possible. Explore various options.</li><li>Encouraged students to seek advice from multiple professors to gain a diverse range of opinions and insights.</li></ul><h2 id="tapomayukh-bhattacharjee-2nd-lecture"><a href="https://cloud.mpi-sws.org/index.php/s/n97WnKRxgYoLcCL?dir=undefined&amp;openfile=84146671">Tapomayukh Bhattacharjee (2nd Lecture)</a></h2><ul><li>There are six activities of daily living (ADLs) defined in literature: personal hygiene or grooming, dressing, toileting, transferring or ambulating, and eating</li><li>Anomaly detection is used in processing sensor data.</li><li>A* is widely used in motion planning due to its efficiency and optimality (it never overestimates the cost).</li><li>Motion planning time = search time + collision checking time (~90%). Therefore, the author proposed lazy A* (which finds an optimal path in an unconstrained situation, goes over collision checking while on the path, and re-searches a path if a collision is encountered).</li><li>Collect a dataset before embarking on research.</li><li>To understand how to manipulate different kinds of foods, the author created a <em>food manipulation taxonomy</em>.</li><li>Choose hardware components for real-world deployability.</li><li>Use deformation of points on a gel coupled with computer vision algorithms to measure shear force</li><li>Add <em>structure</em> to machine learning algorithms to overcome a lack of data.</li><li>If integrating multimodal data sources, think of <em>where</em> to integrate as the size or magnitude of different data may be inconsistent.</li><li>A "bandit" algorithm is an RL algorithm where we utilize <em>partial feedback</em> of <em>one step</em> in the decision-making process, unlike conventional RL algorithms with "episodes" spanning multiple steps.</li></ul><h3 id="audience-question-how-to-stay-up-to-date-with-the-state-of-the-art-especially-in-the-fast-changing-landscape-of-machine-learning">Audience question: How to stay up-to-date with the state-of-the-art (especially in the fast-changing landscape of machine learning)?</h3><ul><li>One of the main tasks of faculty life</li><li>Look at titles and abstracts of publications in all well-known conferences.</li><li>Organize reading groups and reading sessions.</li><li>Interact with known other research groups.</li></ul><h2 id="derek-dreyer-how-to-write-papers-and-give-talks-that-people-can-follow"><a href="https://cloud.mpi-sws.org/index.php/s/n97WnKRxgYoLcCL?dir=undefined&amp;openfile=84146671">Derek Dreyer: How to write papers and give talks that people can follow</a></h2><ul><li><p>Many papers suffer from the TMI (too much information) problem.</p></li><li><p>Aim at giving <em>constructive principles</em> that are easy to check and fix.</p></li><li><p>A paper is different from a textbook - people <em>aren't as committed to reading a paper as they are to reading a textbook</em>.</p></li><li><p>A <em>good but not interesting</em> paper tends to get a "B" or a "weak accept."</p></li><li><p>Putting the Related Work section at the front (as opposed to in the back before the Conclusion) may hinder unfamiliar authors from understanding your work.</p></li><li><p>Most people <em>don't listen to talks to determine whether they should read a paper</em>. Instead, they listen to talks to discuss with others. The main goal of a talk is to <em>give people positive feelings about your work</em>.</p></li><li><p>A talk should only cover the <em>intro</em> and <em>key ideas</em> sections of the corresponding paper.</p></li><li><p>The <em>key ideas</em> should be the high point in your talk before presenting the takeaway messages.</p></li><li><p>Add visual elements to emphasize <em>one point</em> per slide.</p></li><li><p>Use smooth animations to help the listener follow.</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/abbaswu/abbaswu.github.io-images/main/The-Cornell-Maryland-Max-Planck-Pre-doctoral-Rese</summary>
      
    
    
    
    <category term="Conferences" scheme="https://abbaswu.github.io/categories/Conferences/"/>
    
    
  </entry>
  
  <entry>
    <title>From the Fourier Series to the Fourier Transform to the Discrete-time Fourier Transform: Demystifying the Formulas</title>
    <link href="https://abbaswu.github.io/2023/09/04/From-the-Fourier-Series-to-the-Fourier-Transform-to-the-Discrete-time-Fourier-Transform-Demystifying-the-Formulas/"/>
    <id>https://abbaswu.github.io/2023/09/04/From-the-Fourier-Series-to-the-Fourier-Transform-to-the-Discrete-time-Fourier-Transform-Demystifying-the-Formulas/</id>
    <published>2023-09-04T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.165Z</updated>
    
    <content type="html"><![CDATA[<p>In realms as broad as electrical engineering, acoustics, optics, signal processing, quantum mechanics, and econometrics, the Fourier Series, Fourier Transform, and Discrete-time Fourier Transform play a pivotal role in analyzing signals by allowing us to decompose them into simpler components. Many articles present their formulas or dive into their intuition and applications. However, what seems to be missing is a blog post that explains the derivation of their formulas in a way that is both clear and accessible, requiring no more than a rudimentary understanding of calculus.</p><h1 id="fourier-series">Fourier Series</h1><h2 id="standard-form-of-the-fourier-series">Standard Form of the Fourier Series</h2><p>Our journey begins with the <a href="https://en.wikipedia.org/wiki/Fourier_series">Fourier Series</a> - a method to represent periodic functions as a sum of sine and cosine waves.</p><p>Let <span class="math inline">\(x(t)\)</span> be a periodic function with period <span class="math inline">\(T\)</span>. The <em>standard form of the Fourier series</em> for <span class="math inline">\(x(t)\)</span> is given by:</p><p><span class="math display">\[x(t) = \frac{a_0}{2} + a_1 \cos{\frac{2\pi}{T} t} + b_1 \sin{\frac{2\pi}{T} t} + a_2 \cos{\frac{4\pi}{T} t} + b_2 \sin{\frac{4\pi}{T} t} + \dots \]</span></p><p>To solve for <span class="math inline">\(a_0, a_1, b_1, \dots\)</span>, we first observe the .</p><p>Thus, we can multiply both sides of the equation by <span class="math inline">\(cos{\frac{2k\pi}{T} t}\)</span> or <span class="math inline">\(\sin{\frac{2k\pi}{T} t}\)</span> <span class="math inline">\((k \in \{0, 1, 2, \dots, n\})\)</span>, and then integrate over one period <span class="math inline">\([-\frac{T}{2}, \frac{T}{2})\)</span> to obtain:</p><p><span class="math display">\[a_k = \frac{2}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) \cos{\frac{2k\pi}{T} t} dt}\]</span></p><p><span class="math display">\[b_k = \frac{2}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) \sin{\frac{2k\pi}{T} t} dt}\]</span></p><h2 id="exponential-form-of-the-fourier-series">Exponential Form of the Fourier Series</h2><p>Using Euler's formula <span class="math inline">\(e^{ix} = \cos{x} + i \sin{x}\)</span>, we can derive:</p><p><span class="math display">\[\cos{x} = \frac{e^{ix} + e^{-ix}}{2}\]</span></p><p><span class="math display">\[\sin{x} = -i \frac{e^{ix} - e^{-ix}}{2}\]</span></p><p>Substituting these representations of <span class="math inline">\(\cos{x}\)</span> and <span class="math inline">\(\sin{x}\)</span> into <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>, we get:</p><p><span class="math display">\[a_k = \frac{2}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) \frac{e^{i \frac{2k\pi}{T} t} + e^{-i \frac{2k\pi}{T} t}}{2} dt}\]</span></p><p><span class="math display">\[b_k = \frac{2}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{-i x(t) \frac{e^{i \frac{2k\pi}{T} t} - e^{-i \frac{2k\pi}{T} t}}{2} dt}\]</span></p><p>And:</p><p><span class="math display">\[a_k \cos{\frac{2k\pi}{T} t} + b_k \sin{\frac{2k\pi}{T} t} = a_k \frac{e^{i \frac{2k\pi}{T} t} + e^{-i \frac{2k\pi}{T} t}}{2} - i b_k \frac{e^{i \frac{2k\pi}{T} t} - e^{-i \frac{2k\pi}{T} t}}{2} = \frac{a_k - i b_k}{2} e^{i \frac{2k\pi}{T} t} + \frac{a_k + i b_k}{2} e^{-i \frac{2k\pi}{T} t}\]</span></p><p>And:</p><p><span class="math display">\[\frac{a_k - i b_k}{2} = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) e^{-i \frac{2k\pi}{T} t} dt}\]</span></p><p><span class="math display">\[\frac{a_k + i b_k}{2} = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) e^{i \frac{2k\pi}{T} t} dt}\]</span></p><p>Furthermore, if we let:</p><p><span class="math display">\[c_k = \frac{a_k - i b_k}{2} = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) e^{-i \frac{2k\pi}{T} t} dt}\]</span></p><p>Substituting <span class="math inline">\(k \leftarrow -k\)</span> into the expression for <span class="math inline">\(c_k\)</span>, we will obtain:</p><p><span class="math display">\[c_{-k} = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) e^{i \frac{2k\pi}{T} t} dt} = \frac{a_k + i b_k}{2}\]</span></p><p>Thus:</p><p><span class="math display">\[a_k \cos{\frac{2k\pi}{T} t} + b_k \sin{\frac{2k\pi}{T} t}  = \frac{a_k - i b_k}{2} e^{i \frac{2k\pi}{T} t} + \frac{a_k + i b_k}{2} e^{-i \frac{2k\pi}{T} t}= c_k e^{i \frac{2k\pi}{T} t} + c_{-k} e^{-i \frac{2k\pi}{T} t}\]</span></p><p>And by substituting <span class="math inline">\(k \leftarrow 0\)</span> into the expression for <span class="math inline">\(c_k\)</span>, we get:</p><p><span class="math display">\[c_0 = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) dt} = \frac{a_0}{2}\]</span></p><p>Therefore:</p><p><span class="math display">\[x(t) = \frac{a_0}{2} + \sum_{k=1}^{n}{(a_k \cos{\frac{2k\pi}{T} t} + b_k \sin{\frac{2k\pi}{T} t})}  = c_0 + \sum_{k=1}^{n}{(c_k e^{i \frac{2k\pi}{T} t} + c_{-k} e^{-i \frac{2k\pi}{T} t})} = \sum_{k=-n}^{n}{c_k e^{i \frac{2k\pi}{T} t}}\]</span></p><p>Where:</p><p><span class="math display">\[c_k= \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) e^{-i \frac{2k\pi}{T} t} dt}\]</span></p><p>This is the <em>exponential form of the Fourier series</em>. It is more concise than the standard form of the Fourier series and is used more often in practice.</p><h1 id="fourier-transform">Fourier Transform</h1><p>The Fourier transform is a generalization of the Fourier series, which can analyze the effect of a frequency in <em>any function</em> (which may not necessarily be a periodic function). In this section, we will present how it can be derived from the exponential form of the Fourier series.</p><p>Given a periodic function <span class="math inline">\(x(t)\)</span> with period <span class="math inline">\(T\)</span>, the exponential form of the Fourier series of <span class="math inline">\(x(t)\)</span> is as follows:</p><p><span class="math display">\[x(t) = \sum_{k=-n}^{n}{c_k e^{i \frac{2k\pi}{T} t}}\]</span></p><p>Where:</p><p><span class="math display">\[c_k= \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) e^{-i \frac{2k\pi}{T} t} dt}\]</span></p><p>Let's say that the period <span class="math inline">\(T\)</span> is associated with a frequency known as the <a href="https://en.wikipedia.org/wiki/Fundamental_frequency"><em>fundamental frequency</em></a> <span class="math inline">\(f_0 = \frac{1}{T}\)</span>. Given <span class="math inline">\(f_0\)</span>, we can rewrite the previous Fourier series as:</p><p><span class="math display">\[x(t) = \sum_{k=-n}^{n}{c_k e^{i 2\pi k f_0 t}}\]</span></p><p>Where:</p><p><span class="math display">\[c_k= \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}}{x(t) e^{-i 2\pi k f_0 t} dt}\]</span></p><p>For a non-periodic function, we can consider it as a periodic function with <span class="math inline">\(T \rightarrow +\infty\)</span>. In this case, the fundamental frequency <span class="math inline">\(f_0\)</span> is an infinitesimal quantity; therefore, <em>we can consider that any frequency <span class="math inline">\(f\)</span> can be expressed as an integer multiple of the fundamental frequency, and the difference between two neighboring frequencies is the fundamental frequency <span class="math inline">\(f_0\)</span></em>. In this case, the fundamental frequency <span class="math inline">\(f_0\)</span> can be expressed as a differential of the frequency <span class="math inline">\(f\)</span>, i.e., <span class="math inline">\(df\)</span>.</p><p>In this case, for a possibly non-periodic function <span class="math inline">\(x(t)\)</span>:</p><p><span class="math display">\[x(t) = \sum_{k=-\infty}^{\infty}{c_k e^{i 2\pi k (df)t}}\]</span></p><p><span class="math display">\[c_k= (df) \int_{-\infty}^{\infty}{x(t) e^{-i 2\pi k (df) t} dt}\]</span></p><p>Thus, <span class="math inline">\(x(t)\)</span> can be represented as:</p><p><span class="math display">\[x(t) = \sum_{k=-\infty}^{\infty}{[(df) \cdot \int_{-\infty}^{\infty}{x(t) e^{-i 2\pi k (df) t} dt} \cdot e^{i 2\pi k (df) t}]}\]</span></p><p>By considering <span class="math inline">\(f \leftarrow k (df)\)</span>, <a href="https://maninbocss.medium.com/summation-and-the-definite-integral-235663ef5ec3">we can transform the summation into a definite integral</a>:</p><p><span class="math display">\[x(t)= \int_{-\infty}^{\infty}{ [(\int_{-\infty}^{\infty}{x(t) e^{-i 2\pi f t} dt}) e^{i 2\pi f t}] df}\]</span></p><p>Let:</p><p><span class="math display">\[X(f) = \int_{-\infty}^{\infty}{x(t) e^{-i 2\pi f t} dt}\]</span></p><p>Then <span class="math inline">\(x(t)\)</span> can be represented as:</p><p><span class="math display">\[x(t) = \int_{-\infty}^{\infty}{ X(f) e^{i 2\pi f t} df}\]</span></p><p>These two equations are very important.</p><ul><li>If we know <span class="math inline">\(x(t)\)</span> (i.e., the value of <span class="math inline">\(x(t)\)</span> at any time <span class="math inline">\(t\)</span>), through <span class="math inline">\(X(f) = \int_{-\infty}^{\infty}{x(t) e^{-i 2\pi f t} dt}\)</span>, we can compute <em>the relative magnitude of any frequency <span class="math inline">\(f\)</span> over the whole time period</em>.</li><li>At the same time, if we know <span class="math inline">\(X(f)\)</span> (i.e., the relative magnitude of any frequency <span class="math inline">\(f\)</span> over the whole time period), by means of <span class="math inline">\(x(t) = \int_{-\infty}^{\infty}{ X(f) e^{i 2\pi f t} df}\)</span>, we can calculate <em>the value of <span class="math inline">\(x(t)\)</span> at any time <span class="math inline">\(t\)</span></em>.</li></ul><p>We refer to <span class="math inline">\(X(f)\)</span> as the <em>Fourier transform</em> of <span class="math inline">\(x(t)\)</span>, also known as the <em>spectrum</em> of <span class="math inline">\(x(t)\)</span>, and to <span class="math inline">\(x(t)\)</span> as the <em>inverse Fourier transform</em> of <span class="math inline">\(X(f)\)</span>.</p><h1 id="discrete-time-fourier-transform">Discrete-time Fourier Transform</h1><p>When we process signals with computers, as computers cannot store a continuous infinite function, we usually take <span class="math inline">\(N\)</span> samples of the original signal <span class="math inline">\(x(t)\)</span> at a certain time interval <span class="math inline">\(\Delta t\)</span>, obtaining an array <span class="math inline">\(x[0:N-1]\)</span>.</p><p>Using <span class="math inline">\(x[0:N-1]\)</span> to estimate the Fourier transform <span class="math inline">\(X(f)\)</span> of the sampled function <span class="math inline">\(x(t)\)</span>, we get:</p><p><span class="math display">\[X(f) = \int_{-\infty}^{\infty}{x(t) e^{-i 2\pi f t} dt} \approx \int_{0}^{N \Delta t}{x(t) e^{-i 2\pi f t} dt} \approx \sum_{m=0}^{N - 1}{x(m \Delta t) e^{-i 2\pi f m \Delta t}}\]</span></p><p>If we <em>assume these samples have spanned a period of the original signal, e.g. <span class="math inline">\(T = N \Delta t\)</span>, and that we only consider frequencies satisfying <span class="math inline">\(f = k \frac{1}{N \Delta t} (k \in \{0, 1, \dots, N - 1\})\)</span></em>, we get:</p><p><span class="math display">\[X(k \frac{1}{N \Delta t}) \approx \sum_{n=0}^{N - 1}{x(n \Delta t) e^{-i 2\pi k \frac{1}{N \Delta t} n \Delta t}} = \sum_{n=0}^{N - 1}{x(n \Delta t) e^{-i 2\pi \frac{k}{N} n}} = \sum_{n=0}^{N - 1}{x[n] e^{-i 2\pi \frac{k}{N} n}}\]</span></p><p>Let:</p><p><span class="math display">\[X[k] = \sum_{n=0}^{N - 1}{x[n] e^{-i 2\pi \frac{k}{N} n}} (k \in \{0, 1, \dots, N - 1\})\]</span></p><p>We call such an array of <span class="math inline">\(N\)</span> discrete numbers <span class="math inline">\(X[0:N-1]\)</span> the <em>discrete-time Fourier transform</em> of <span class="math inline">\(x[0:N-1]\)</span>, which is a <em>discrete</em> frequency domain representation of <span class="math inline">\(x[0:N-1]\)</span>.</p><p>Using <span class="math inline">\(X[0:N-1]\)</span>, we can restore <span class="math inline">\(x[0:N-1]\)</span>:</p><p><span class="math display">\[x[n] = \frac{1}{N} \sum_{k=0}^{N - 1}{X[k] e^{i 2\pi \frac{k}{N} n}} (n \in \{0, 1, \dots, N - 1\})\]</span></p><p>We call <span class="math inline">\(x[0:N-1]\)</span> the <em>inverse discrete-time Fourier transform</em> of <span class="math inline">\(X[0:N-1]\)</span>. This is analogous to <span class="math inline">\(X(f)\)</span> being the Fourier transform of <span class="math inline">\(x(t)\)</span> and <span class="math inline">\(x(t)\)</span> being the inverse Fourier transform of <span class="math inline">\(X(f)\)</span> in the continuous case.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In realms as broad as electrical engineering, acoustics, optics, signal processing, quantum mechanics, and econometrics, the Fourier Seri</summary>
      
    
    
    
    <category term="Mathematics" scheme="https://abbaswu.github.io/categories/Mathematics/"/>
    
    
  </entry>
  
  <entry>
    <title>On Convolutional Neural Networks and Photographic Lenses</title>
    <link href="https://abbaswu.github.io/2023/08/24/On-Convolutional-Neural-Networks-and-Photographic-Lenses/"/>
    <id>https://abbaswu.github.io/2023/08/24/On-Convolutional-Neural-Networks-and-Photographic-Lenses/</id>
    <published>2023-08-24T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.165Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional neural networks</a> are <a href="https://en.wikipedia.org/wiki/Camera_lens">camera lenses</a> to a computer.</p><figure><img src="https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png" alt="" /><figcaption>A convolutional neural network</figcaption></figure><figure><img src="https://upload.wikimedia.org/wikipedia/commons/7/7a/Tessar.png" alt="" /><figcaption>A camera lens</figcaption></figure><p>The analogy does not stop at the point that both compress visual information:</p><ul><li>The <a href="https://www.jeremyjordan.me/convnet-architectures/">evolution of convolutional neural network architectures</a> resembles the <a href="https://en.wikipedia.org/wiki/History_of_photographic_lens_design">evolution of camera lenses</a>.</li><li>The <a href="https://analyticsindiamag.com/complete-guide-to-understanding-precision-and-recall-curves/">P-R curve</a> showing the performance of a convolutional neural network is strikingly similar to the <a href="https://www.dearsusan.net/how-to-read-mtf-curves-like-an-artist/">MTF curve</a> evaluating lens performance.</li></ul><figure><img src="https://machinelearningmastery.com/wp-content/uploads/2020/01/Precision-Recall-Curve-of-a-Logistic-Regression-Model-and-a-No-Skill-Classifier2.png" alt="" /><figcaption>A P-R curve</figcaption></figure><figure><img src="https://photographylife.com/wp-content/uploads/2013/01/How-to-Read-MTF-Charts.png" alt="" /><figcaption>An MTF curve</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;Convolutional neural networks&lt;/a&gt; are &lt;a href=&quot;https://en.wikipedia.</summary>
      
    
    
    
    <category term="Reflections" scheme="https://abbaswu.github.io/categories/Reflections/"/>
    
    
  </entry>
  
  <entry>
    <title>Our Motivation for Maintaining Our Blog</title>
    <link href="https://abbaswu.github.io/2023/08/16/Our-Motivation-for-Maintaining-Our-Blog/"/>
    <id>https://abbaswu.github.io/2023/08/16/Our-Motivation-for-Maintaining-Our-Blog/</id>
    <published>2023-08-16T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.165Z</updated>
    
    <content type="html"><![CDATA[<p>今之博客，乃昔（初高中）之错题本之翻版也。昔之背景知识、解题思路、高效算法，今之认识、洞见，皆为辛苦求索所得，故笔录之，以期日积月累，唯“应试”“科研”之直接目的异也。</p><p>This blog is a replica of our previous "problem books" used for junior and senior high school. In the past, we would record background knowledge, problem solving ideas, and efficient algorithms. Today, we would note down understandings and insights. These are all the result of the hard work of searching and exploring, and we record them down in order to gradually accumulate our knowledge and understanding. Only the direct purpose has been changed from "preparing for a test" to "doing scientific research".</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今之博客，乃昔（初高中）之错题本之翻版也。昔之背景知识、解题思路、高效算法，今之认识、洞见，皆为辛苦求索所得，故笔录之，以期日积月累，唯“应试”“科研”之直接目的异也。&lt;/p&gt;
&lt;p&gt;This blog is a replica of our previous &quot;probl</summary>
      
    
    
    
    <category term="Reflections" scheme="https://abbaswu.github.io/categories/Reflections/"/>
    
    
  </entry>
  
  <entry>
    <title>Timetable of Well-known Conferences in Different Subdomains of Computer Science</title>
    <link href="https://abbaswu.github.io/2023/08/16/Timetable-of-Well-known-Conferences-in-Different-Subdomains-of-Computer-Science/"/>
    <id>https://abbaswu.github.io/2023/08/16/Timetable-of-Well-known-Conferences-in-Different-Subdomains-of-Computer-Science/</id>
    <published>2023-08-16T07:00:00.000Z</published>
    <updated>2023-11-06T07:31:36.169Z</updated>
    
    <content type="html"><![CDATA[<p>We have compiled a timetable of well-known conferences in different subdomains of computer science based on the <em>Class A</em> and <em>Class B</em> conferences in <a href="https://www.ccf.org.cn/Academic_Evaluation/By_category/">"Directory of International Academic Conferences and Journals Recommended by the Chinese Computer Society"</a>. Although the precise start dates of each conference vary year by year, the provided start dates provide a general guideline on the <em>relative order</em> of the conferences throughout each year.</p><table><thead><tr class="header"><th>Name</th><th>Start Date</th><th>Subdomain</th></tr></thead><tbody><tr class="odd"><td>CIDR</td><td>01/08/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>GROUP</td><td>01/08/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="odd"><td>POPL</td><td>01/15/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>VMCAI</td><td>01/15/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>HiPEAC</td><td>01/16/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>SODA</td><td>01/22/23</td><td>Theoretical Computer Science</td></tr><tr class="odd"><td>PPoPP</td><td>02/05/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>AAAI</td><td>02/07/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>FPGA</td><td>02/12/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>FAST</td><td>02/21/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>HPCA</td><td>02/25/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>CGO</td><td>02/25/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>NDSS</td><td>02/27/23</td><td>Security</td></tr><tr class="even"><td>WSDM</td><td>02/27/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="odd"><td>FM</td><td>03/07/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>PERCOM</td><td>03/13/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="odd"><td>FSE</td><td>03/20/23</td><td>Security</td></tr><tr class="even"><td>SANER</td><td>03/21/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>DCC</td><td>03/21/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>ASPLOS</td><td>03/25/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>VR</td><td>03/25/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>IUI</td><td>03/27/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="odd"><td>ICDT</td><td>03/28/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>EDBT</td><td>03/28/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="odd"><td>ICDE</td><td>04/03/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>RECOMB</td><td>04/16/23</td><td>Interdisciplinary/Emerging</td></tr><tr class="odd"><td>DATE</td><td>04/17/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>NSDI</td><td>04/17/23</td><td>Computer Networks</td></tr><tr class="odd"><td>DASFAA</td><td>04/17/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>ETAPS</td><td>04/22/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>EUROCRYPT</td><td>04/23/23</td><td>Security</td></tr><tr class="even"><td>CHI</td><td>04/23/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="odd"><td>SDM</td><td>04/27/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>WWW</td><td>04/30/23</td><td>Interdisciplinary/Emerging</td></tr><tr class="odd"><td>I3D</td><td>05/03/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>PKC</td><td>05/07/23</td><td>Security</td></tr><tr class="odd"><td>EG</td><td>05/08/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>RTAS</td><td>05/09/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>IPSN</td><td>05/09/23</td><td>Computer Networks</td></tr><tr class="even"><td>HSCC</td><td>05/09/23</td><td>Theoretical Computer Science</td></tr><tr class="odd"><td>EuroSys</td><td>05/09/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>ICSE</td><td>05/14/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>IPDPS</td><td>05/15/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>ICPC</td><td>05/15/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>INFOCOM</td><td>05/17/23</td><td>Computer Networks</td></tr><tr class="even"><td>MSST</td><td>05/22/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>S&amp;P</td><td>05/22/23</td><td>Security</td></tr><tr class="even"><td>ICRA</td><td>05/29/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>AAMAS</td><td>05/29/23</td><td>Artificial Intelligence</td></tr><tr class="even"><td>ICASSP</td><td>06/04/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="odd"><td>ECSCW</td><td>06/05/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="even"><td>NOSSDAV</td><td>06/10/23</td><td>Computer Networks</td></tr><tr class="odd"><td>CAiSE</td><td>06/12/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>SoCG</td><td>06/12/23</td><td>Theoretical Computer Science</td></tr><tr class="odd"><td>ICMR</td><td>06/12/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>EuroVis</td><td>06/12/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="odd"><td>SPAA</td><td>06/16/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>ISCA</td><td>06/17/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>PLDI</td><td>06/17/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>LCTES</td><td>06/17/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>MobiSys</td><td>06/18/23</td><td>Computer Networks</td></tr><tr class="even"><td>SIGMOD</td><td>06/18/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="odd"><td>PODS</td><td>06/18/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>CVPR</td><td>06/18/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>SIGMETRICS</td><td>06/19/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>IWQoS</td><td>06/19/23</td><td>Computer Networks</td></tr><tr class="odd"><td>PODC</td><td>06/19/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>HPDC</td><td>06/20/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>STOC</td><td>06/20/23</td><td>Theoretical Computer Science</td></tr><tr class="even"><td>ICS</td><td>06/21/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>HotOS</td><td>06/22/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>LICS</td><td>06/26/23</td><td>Theoretical Computer Science</td></tr><tr class="odd"><td>DSN</td><td>06/27/23</td><td>Security</td></tr><tr class="even"><td>EGSR</td><td>06/28/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="odd"><td>CADE/IJCAR</td><td>07/01/23</td><td>Theoretical Computer Science</td></tr><tr class="even"><td>ICWS</td><td>07/02/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>SGP</td><td>07/03/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>SAT</td><td>07/04/23</td><td>Theoretical Computer Science</td></tr><tr class="odd"><td>SPM</td><td>07/05/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>ICAPS</td><td>07/08/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>DAC</td><td>07/09/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>CSFW</td><td>07/09/23</td><td>Security</td></tr><tr class="odd"><td>ACL</td><td>07/09/23</td><td>Artificial Intelligence</td></tr><tr class="even"><td>USENIX ATC</td><td>07/10/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>OSDI</td><td>07/10/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>ICALP</td><td>07/10/23</td><td>Theoretical Computer Science</td></tr><tr class="odd"><td>ICME</td><td>07/10/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>COLT</td><td>07/12/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>ISSTA</td><td>07/17/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>ECOOP</td><td>07/17/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>CAV</td><td>07/17/23</td><td>Theoretical Computer Science</td></tr><tr class="even"><td>CCC</td><td>07/17/23</td><td>Theoretical Computer Science</td></tr><tr class="odd"><td>ICCBR</td><td>07/17/23</td><td>Artificial Intelligence</td></tr><tr class="even"><td>ICDCS</td><td>07/18/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>SIGIR</td><td>07/23/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>ICML</td><td>07/23/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>ISMB</td><td>07/23/23</td><td>Interdisciplinary/Emerging</td></tr><tr class="even"><td>CogSci</td><td>07/26/23</td><td>Interdisciplinary/Emerging</td></tr><tr class="odd"><td>UAI</td><td>07/31/23</td><td>Artificial Intelligence</td></tr><tr class="even"><td>SCA</td><td>08/04/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="odd"><td>SIGKDD</td><td>08/06/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>SIGGRAPH</td><td>08/06/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="odd"><td>ICPP</td><td>08/07/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>USENIX Security</td><td>08/09/23</td><td>Security</td></tr><tr class="odd"><td>CRYPTO</td><td>08/19/23</td><td>Security</td></tr><tr class="even"><td>IJCAI</td><td>08/19/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>HOT CHIPS</td><td>08/27/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>CP</td><td>08/27/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>VLDB</td><td>08/28/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>KR</td><td>09/02/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>RE</td><td>09/04/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>ICFP</td><td>09/04/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>ESA</td><td>09/04/23</td><td>Theoretical Computer Science</td></tr><tr class="even"><td>SIGCOMM</td><td>09/10/23</td><td>Computer Networks</td></tr><tr class="odd"><td>CHES</td><td>09/10/23</td><td>Security</td></tr><tr class="even"><td>SECON</td><td>09/11/23</td><td>Computer Networks</td></tr><tr class="odd"><td>ASE</td><td>09/11/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>CODES+ISSS</td><td>09/17/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>EMSOFT</td><td>09/17/23</td><td>Interdisciplinary/Emerging</td></tr><tr class="even"><td>ECML-PKDD</td><td>09/18/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="odd"><td>CONCUR</td><td>09/19/23</td><td>Theoretical Computer Science</td></tr><tr class="even"><td>ESORICS</td><td>09/25/23</td><td>Security</td></tr><tr class="odd"><td>SRDS</td><td>09/25/23</td><td>Security</td></tr><tr class="even"><td>MobileHCI</td><td>09/26/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="odd"><td>ECAI</td><td>09/30/23</td><td>Artificial Intelligence</td></tr><tr class="even"><td>MoDELS</td><td>10/01/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>ICSME</td><td>10/01/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>MobiCom</td><td>10/02/23</td><td>Computer Networks</td></tr><tr class="odd"><td>ICCV</td><td>10/02/23</td><td>Artificial Intelligence</td></tr><tr class="even"><td>ECCV</td><td>10/02/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>ITC</td><td>10/08/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>UbiComp</td><td>10/08/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="odd"><td>ESEM</td><td>10/09/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>ISSRE</td><td>10/09/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>ICNP</td><td>10/10/23</td><td>Computer Networks</td></tr><tr class="even"><td>PG</td><td>10/10/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="odd"><td>CSCW</td><td>10/14/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="even"><td>RAID</td><td>10/16/23</td><td>Security</td></tr><tr class="odd"><td>ISMAR</td><td>10/16/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="even"><td>PACT</td><td>10/21/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>CIKM</td><td>10/21/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="even"><td>OOPSLA</td><td>10/22/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>SAS</td><td>10/22/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>IEEE VIS</td><td>10/22/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="odd"><td>MobiHoc</td><td>10/23/23</td><td>Computer Networks</td></tr><tr class="even"><td>SOSP</td><td>10/23/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>IMC</td><td>10/24/23</td><td>Computer Networks</td></tr><tr class="even"><td>MICRO</td><td>10/28/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>ICCAD</td><td>10/29/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>ACM MM</td><td>10/29/23</td><td>Computer Graphics/Multimedia</td></tr><tr class="odd"><td>UIST</td><td>10/29/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="even"><td>SoCC</td><td>10/30/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>CLUSTER</td><td>10/31/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>ISS</td><td>11/05/23</td><td>Human Computer Interaction and Ubiquitous Computing</td></tr><tr class="odd"><td>ICCD</td><td>11/06/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="even"><td>ISWC</td><td>11/06/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="odd"><td>FOCS</td><td>11/06/23</td><td>Theoretical Computer Science</td></tr><tr class="even"><td>SC</td><td>11/12/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>SenSys</td><td>11/12/23</td><td>Computer Networks</td></tr><tr class="even"><td>Performance</td><td>11/14/23</td><td>Computer Architecture/Parallel and Distributed Computing/Storage Systems</td></tr><tr class="odd"><td>CCS</td><td>11/26/23</td><td>Security</td></tr><tr class="even"><td>ICSOC</td><td>11/28/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="odd"><td>TCC</td><td>11/29/23</td><td>Security</td></tr><tr class="even"><td>ICDM</td><td>12/01/23</td><td>Databases/Data Mining/Information Retrieval</td></tr><tr class="odd"><td>FSE/ESEC</td><td>12/03/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr><tr class="even"><td>ACSAC</td><td>12/04/23</td><td>Security</td></tr><tr class="odd"><td>ASIACRYPT</td><td>12/04/23</td><td>Security</td></tr><tr class="even"><td>CoNEXT</td><td>12/05/23</td><td>Computer Networks</td></tr><tr class="odd"><td>RTSS</td><td>12/05/23</td><td>Interdisciplinary/Emerging</td></tr><tr class="even"><td>BIBM</td><td>12/05/23</td><td>Interdisciplinary/Emerging</td></tr><tr class="odd"><td>EMNLP</td><td>12/06/23</td><td>Artificial Intelligence</td></tr><tr class="even"><td>NeurIPS</td><td>12/10/23</td><td>Artificial Intelligence</td></tr><tr class="odd"><td>Middleware</td><td>12/11/23</td><td>Software Engineering/Systems Software/Programming Languages</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;We have compiled a timetable of well-known conferences in different subdomains of computer science based on the &lt;em&gt;Class A&lt;/em&gt; and &lt;em&gt;</summary>
      
    
    
    
    <category term="Reference" scheme="https://abbaswu.github.io/categories/Reference/"/>
    
    
  </entry>
  
</feed>
